================================================================================
XGBOOST R² VARIANCE - FINAL ACTIONABLE SUMMARY
================================================================================

USER'S OBSERVATION:
  "XGBoost R² now drops only 0.01-0.04 (improved from 0.05!)"
  
ANALYSIS RESULT:
  This is NOT acceptable CV variance - it's parameter drift caused by incomplete
  hyperparameter storage. ElasticNet achieves ±0.002 because it stores ALL grid
  parameters. XGBoost only stores 3 of 6+ important parameters.

THE PROBLEM:
  1. XGBoost grid search uses 3 parameters: n_estimators, learning_rate, max_depth
  2. Grid search results store ONLY these 3 in the Params column
  3. Missing regularization params: subsample, colsample_bytree, reg_alpha
  4. When refined models load, only 3 params are applied via model.set_params()
  5. Missing params revert to XGBoost defaults (subsample=1.0, colsample_bytree=1.0, reg_alpha=0)
  6. This causes 0.01-0.04 R² drop between search and refinement

PROOF:
  - ElasticNet stores both grid params (alpha, l1_ratio) → ±0.002 variance
  - XGBoost stores 3 params, missing 3 critical ones → ±0.01-0.04 variance
  - Code verified: GUI loads params from results exactly as stored (line 4628)
  - Missing params traced to grid creation (lines 450-474 in models.py)

THE FIX (5 changes, ~30 minutes):

1. UPDATE model_config.py (lines 105-124)
   Add to each XGBoost tier:
   'subsample': [0.8, 1.0],
   'colsample_bytree': [0.8, 1.0],
   'reg_alpha': [0, 0.1],
   
2. UPDATE models.py - XGBoost regression (lines 450-474)
   Add 3 nested loops and parameters to grid creation
   
3. UPDATE models.py - XGBoost classification (lines 609-633)
   Add identical 3 nested loops and parameters
   
4. UPDATE models.py - Default XGBoost regression (lines 106-114)
   Add subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1
   
5. UPDATE models.py - Default XGBoost classification (lines 166-174)
   Add identical default parameters

EXPECTED RESULT:
  - R² variance drops from ±0.01-0.04 to ±0.005-0.01 (like ElasticNet)
  - Params column now stores 6 parameters instead of 3
  - Refined models perfectly reproduce search results
  - Grid size increases from 8 to 64 for standard tier (30 min → 1-2 hours)

IS THIS FIXABLE?
  YES. 100% fixable. This is a straightforward hyperparameter tracking bug,
  not a fundamental algorithm issue.

CONFIDENCE LEVEL:
  95%+ (root cause clearly identified, code paths verified, fix is surgical)

FILES TO MODIFY:
  1. /home/user/dasp/src/spectral_predict/model_config.py (1 location)
  2. /home/user/dasp/src/spectral_predict/models.py (4 locations)

DETAILED PATCHES:
  See /tmp/xgboost_fix_patches.txt

QUICK VALIDATION AFTER FIX:
  1. Run standard tier search on small dataset
  2. In Results tab, click any XGBoost row → Params cell should show 6 params
  3. Go to Model Development, click "Run Refined Model"
  4. Check console output: should apply 6 params, not 3
  5. Refined R² should match search R² within ±0.005

TIME ESTIMATE:
  - Understanding this report: 15 minutes
  - Applying 5 patches: 15 minutes  
  - Testing: 30 minutes
  - TOTAL: ~1 hour

NEXT STEP:
  Apply the patches from /tmp/xgboost_fix_patches.txt to the 5 code locations

================================================================================
