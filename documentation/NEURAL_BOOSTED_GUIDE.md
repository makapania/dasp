# Neural Boosted Regression - User Guide

**Version:** 1.0
**Date:** October 27, 2025
**Status:** Production Ready

---

## Overview

**Neural Boosted Regression** is an ensemble machine learning method that combines small neural networks using gradient boosting. It provides an excellent balance between:

- **Accuracy**: Captures nonlinear relationships better than PLS
- **Interpretability**: Provides feature importances (unlike black-box deep learning)
- **Robustness**: Handles noisy data and outliers
- **Speed**: Faster than training large neural networks

---

## When to Use Neural Boosted Regression

### ‚úÖ Best For:

| Your Situation | Why Neural Boosted is Good |
|----------------|----------------------------|
| **Nonlinear relationships** | Captures curves and interactions that PLS misses |
| **Need interpretability** | Provides wavelength importances (unlike deep MLP) |
| **Noisy spectral data** | Robust to measurement noise and outliers (with Huber loss) |
| **Moderate dataset size** | Works well with 50-500 samples |
| **Want better than PLS** | Typically +5-15% R¬≤ improvement on nonlinear problems |

### ‚ö†Ô∏è Consider Alternatives:

| Your Situation | Use Instead |
|----------------|-------------|
| **Purely linear relationship** | PLS (faster, simpler) |
| **Need maximum speed** | Random Forest (parallel processing) |
| **Very large dataset (>5000 samples)** | Random Forest (scales better) |
| **Very small dataset (<30 samples)** | PLS (less prone to overfitting) |
| **Very high-dimensional with few samples** | Use wavelength subsets (top250, top500) first |

### üí° Handling High-Dimensional Data:

If you have **very high-dimensional data** (1000+ wavelengths) with **few samples** (<100):

1. **Use wavelength subset selection**: The system automatically tests `top250` and `top500` subsets
2. **These often perform BETTER** than full spectrum due to reduced noise
3. **Check results CSV**: Look for rows with `SubsetTag = "top250"` or `"top500"`
4. **Neural Boosted works best** with subset models in high-dimensional settings

**Example:**
- Full spectrum (2000 vars, 50 samples): R¬≤ may be poor
- Top 250 variables (250 vars, 50 samples): R¬≤ significantly better
- The system handles this automatically!

---

## Quick Start

### Using the GUI

1. Launch the Spectral Predict GUI
2. Load your spectral data (ASD files or CSV)
3. The system automatically tests **all models** including Neural Boosted
4. Check the results CSV - look for rows with `Model = NeuralBoosted`
5. Examine the `top_vars` column to see which wavelengths are most important

**That's it!** Neural Boosted runs automatically with optimized settings.

### Using Python Code

```python
from spectral_predict.neural_boosted import NeuralBoostedRegressor
import numpy as np

# Load your data
X = ... # Shape: (n_samples, n_wavelengths)
y = ... # Shape: (n_samples,)

# Create model
model = NeuralBoostedRegressor(
    n_estimators=100,        # Max boosting rounds
    learning_rate=0.1,       # Conservative updates
    hidden_layer_size=5,     # Small network (weak learner)
    early_stopping=True,     # Stop when validation plateaus
    random_state=42
)

# Fit model
model.fit(X, y)

# Make predictions
predictions = model.predict(X)

# Get feature importances
importances = model.get_feature_importances()
top_10_wavelengths = np.argsort(importances)[-10:][::-1]
print(f"Top 10 wavelengths: {top_10_wavelengths}")
```

---

## Understanding the Results

### Results CSV Output

When you run an analysis, the results CSV will include Neural Boosted models:

```csv
Model,Preprocess,n_vars,LVs,RMSE,R2,SubsetTag,top_vars,CompositeScore,Rank
NeuralBoosted,snv,2151,nan,0.068,0.95,full,"1450.0,2250.0,1455.0,...",0.234,1
NeuralBoosted,snv,250,nan,0.072,0.94,top250,"1450.0,2250.0,950.0,...",0.198,3
PLS,snv,2151,12,0.095,0.88,full,"1450.0,1455.0,1460.0,...",0.412,8
```

**Key Columns:**

- **Model**: "NeuralBoosted" indicates this row
- **RMSE**: Lower is better (prediction error)
- **R2**: Higher is better (0-1 scale, 1 = perfect)
- **n_vars**: Number of wavelengths used
- **SubsetTag**: "full" = all wavelengths, "top250" = top 250 important wavelengths
- **top_vars**: Top 30 most important wavelengths (comma-separated)
- **CompositeScore**: Combined performance + complexity score (lower = better)
- **Rank**: Overall ranking (1 = best model)

### Interpreting Feature Importances

The `top_vars` column shows wavelengths in **order of importance**:

```
"1450.0,2250.0,1455.0,2255.0,950.0,..."
   ^      ^
 Most   2nd most
important
```

**What this means:**
- **1450 nm**: Most important wavelength for predictions
- **2250 nm**: Second most important
- These wavelengths have strong relationships with your target variable

**Use this to:**
1. **Validate results**: Do important wavelengths make chemical sense?
   - 1450 nm ‚Üí O-H stretch (water, hydroxyl)
   - 2250 nm ‚Üí C-H combinations
   - 1730 nm ‚Üí C-H first overtone
2. **Design targeted instruments**: Build sensors focusing on key wavelengths
3. **Understand your samples**: Which chemical features drive the property?

---

## How Neural Boosted Works

### Algorithm Overview

```
Step 1: Start with F(x) = 0 (no prediction)

Step 2: For each boosting round:
    a. Compute residuals = y - F(x)  [what we got wrong]
    b. Fit small neural network to residuals
    c. Update: F(x) = F(x) + learning_rate √ó network(x)

Step 3: Stop when validation stops improving (early stopping)

Final prediction = Sum of all weak learners
```

### Why It Works

**Gradient Boosting:** Each new network corrects mistakes of previous networks
**Small Networks:** 3-5 nodes prevents overfitting (weak learner property)
**Learning Rate:** Small steps (0.05-0.2) improve generalization
**Early Stopping:** Stops before overfitting to validation data

### Hyperparameters Tested

The system automatically tests **24 configurations** per preprocessing method:

| Parameter | Values Tested | What It Does |
|-----------|---------------|--------------|
| **n_estimators** | 50, 100 | Maximum boosting rounds (early stopping usually triggers first) |
| **learning_rate** | 0.05, 0.1, 0.2 | Step size for updates (lower = more conservative) |
| **hidden_layer_size** | 3, 5 | Nodes in hidden layer (small = weak learner) |
| **activation** | tanh, identity | Activation function (tanh = smooth nonlinear, identity = linear) |

**Total:** 2 √ó 3 √ó 2 √ó 2 = 24 configurations

Each configuration is tested with:
- Multiple preprocessing methods (raw, SNV, 1st derivative, 2nd derivative)
- Full spectrum
- Multiple variable subsets (top 10, 20, 50, 100, 250, 500, 1000)

**Result:** Hundreds of Neural Boosted models tested automatically!

---

##Advanced Features

### Outlier Robust Regression (Huber Loss)

If your data has outliers (measurement errors, contaminated samples), use Huber loss:

```python
model = NeuralBoostedRegressor(
    loss='huber',           # Robust to outliers
    huber_delta=1.35,       # Threshold for outlier detection
    n_estimators=100,
    learning_rate=0.1,
    hidden_layer_size=5
)
```

**How Huber works:**
- Small errors (< delta): Use squared error (standard)
- Large errors (> delta): Use linear penalty (less sensitive)

**When to use:**
- Known outliers in your dataset
- Measurement noise
- Contaminated samples

### Custom Hyperparameters

If you want more control, modify the model grid in `src/spectral_predict/models.py`:

```python
# Around line 130
learning_rates = [0.03, 0.1, 0.3]  # Add more aggressive rate
hidden_sizes = [3, 5, 7]            # Test larger networks
activations = ['tanh', 'relu']      # Try ReLU
```

---

## Comparison with Other Models

### Neural Boosted vs. PLS

| Feature | PLS | Neural Boosted |
|---------|-----|----------------|
| **Speed** | Very fast (seconds) | Moderate (minutes) |
| **Accuracy (linear)** | Excellent | Good |
| **Accuracy (nonlinear)** | Poor | Excellent |
| **Interpretability** | VIP scores | Aggregated importances |
| **Overfitting risk** | Low | Low (with early stopping) |
| **Best for** | Linear relationships | Nonlinear relationships |

**Example results:**
- **Linear data**: PLS R¬≤ = 0.92, Neural Boosted R¬≤ = 0.93 (similar)
- **Nonlinear data**: PLS R¬≤ = 0.78, Neural Boosted R¬≤ = 0.92 (+14% improvement!)

### Neural Boosted vs. Random Forest

| Feature | Random Forest | Neural Boosted |
|---------|---------------|----------------|
| **Speed** | Fast (parallel) | Moderate |
| **Accuracy** | Very good | Very good |
| **Feature importances** | Gini importance | Aggregated weights |
| **Hyperparameter tuning** | Many parameters | Fewer parameters |
| **Best for** | Large datasets, high dimensions | Medium datasets, interpretability |

**Both are excellent choices!** Try both and compare results.

### Neural Boosted vs. MLP (Deep Learning)

| Feature | MLP | Neural Boosted |
|---------|-----|----------------|
| **Speed** | Slow | Moderate |
| **Accuracy** | Excellent (large data) | Excellent (medium data) |
| **Interpretability** | Poor (black box) | Good (feature importances) |
| **Overfitting risk** | High | Low (boosting + early stop) |
| **Best for** | Large datasets, complex patterns | Interpretable nonlinear models |

**Neural Boosted is like "interpretable deep learning"** - you get nonlinearity with feature importances.

---

## Performance Tips

### 1. Preprocessing Matters

Neural Boosted works with all preprocessing methods:

- **SNV (Standard Normal Variate)**: Good starting point
- **1st Derivative**: Removes baseline, emphasizes peaks
- **2nd Derivative**: Further sharpens features
- **Raw**: No preprocessing (test this too!)

**The system tests all methods automatically.** Check which works best in your results.

### 2. Early Stopping is Your Friend

Early stopping typically triggers at **20-40 estimators**, saving 60-80% of computation time:

```
n_estimators=100 ‚Üí Actually uses 25 estimators (early stop)
Saves: 75% computation time
Performance: Often better (no overfitting)
```

**Keep early stopping enabled** (default: `True`).

### 3. Variable Subset Selection

The system tests:
- Full spectrum (all wavelengths)
- Top N important wavelengths (10, 20, 50, 100, 250, 500, 1000)

**Sometimes fewer variables perform better!**

Example:
```csv
Model,n_vars,RMSE,R2,SubsetTag
NeuralBoosted,2151,0.085,0.92,full           ‚Üê All wavelengths
NeuralBoosted,250,0.072,0.95,top250          ‚Üê BETTER with fewer!
```

**Why?** Removing noisy wavelengths can improve generalization.

### 4. Check Training Time

Typical training times (100 samples √ó 2000 wavelengths):

| Configuration | Time per Config | Total Time (24 configs) |
|---------------|-----------------|-------------------------|
| Full spectrum | ~45 seconds | ~18 minutes |
| Top 250 vars | ~15 seconds | ~6 minutes |
| Top 50 vars | ~5 seconds | ~2 minutes |

**With early stopping:** Usually 40-60% faster.

---

## Troubleshooting

### Problem: R¬≤ is Low (<0.5)

**Possible causes:**
1. **Data is too noisy**: Try Huber loss
2. **Not enough samples**: Need at least 50+ samples
3. **Truly linear relationship**: Use PLS instead
4. **Wrong preprocessing**: Try different methods (SNV, derivatives)

**Solutions:**
```python
# Try Huber loss
model = NeuralBoostedRegressor(loss='huber', huber_delta=1.35)

# Increase model complexity
model = NeuralBoostedRegressor(hidden_layer_size=7, n_estimators=150)

# Check if PLS works better (might be linear)
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
pls.fit(X, y)
```

### Problem: Training Takes Too Long

**Solutions:**

1. **Reduce variable subset**:
   - Use top 250 or 500 wavelengths instead of full spectrum
   - Check `top250` or `top500` results in CSV

2. **Reduce grid size** (modify `models.py`):
   ```python
   learning_rates = [0.1]          # Just one
   n_estimators_list = [100]        # Just one
   hidden_sizes = [5]               # Just one
   # Total: 2 activations = 2 configs instead of 24
   ```

3. **Use fewer preprocessing methods**:
   - Modify CLI/GUI to test only SNV and 1st derivative

### Problem: Convergence Warnings

You might see warnings like:
```
ConvergenceWarning: Maximum iterations (500) reached
```

**This is usually OK!** The boosting algorithm is robust to imperfect weak learners.

**If you want to fix it:**
```python
model = NeuralBoostedRegressor(
    max_iter=1000,  # Increase iterations
    alpha=0.001     # Stronger regularization
)
```

### Problem: Model Ranks Low (Not Best)

Neural Boosted might not always rank #1. **This is OK!**

**Check:**
1. What model ranked #1? (PLS? RandomForest?)
2. How much better? (ŒîR¬≤ < 0.02 is negligible)
3. Do you need interpretability? (Neural Boosted importances > RF)

**Remember:** The goal is finding the best model for *your* data, not forcing Neural Boosted to win.

---

## Technical Details

### Feature Importance Calculation

```python
# For each weak learner:
weights = learner.coefs_[0]  # Shape: (n_features, n_hidden)

# Average absolute weight across hidden nodes
importance_per_learner = mean(|weights|, axis=1)

# Aggregate across all learners
total_importance = sum(importance_per_learner) / n_learners
```

**Result:** Single importance score per wavelength.

**Properties:**
- Non-negative (uses absolute values)
- Higher = more important
- Normalized across ensemble

### Early Stopping Logic

```python
1. Reserve 15% of training data for validation
2. After each boosting round:
   - Compute validation loss
   - If loss improved: reset counter
   - If loss didn't improve: increment counter
3. Stop if counter >= n_iter_no_change (default: 10)
```

**Benefits:**
- Prevents overfitting
- Saves computation
- Often improves test performance

### Memory Usage

Approximate memory for one Neural Boosted model:

```
Memory per weak learner = n_features √ó hidden_size √ó 8 bytes

Example: 2000 wavelengths √ó 5 hidden nodes √ó 8 bytes = 80 KB
For 30 weak learners: 30 √ó 80 KB = 2.4 MB

Full grid (24 configs): ~60 MB
```

**Conclusion:** Memory is not a concern for typical spectral datasets.

---

## Frequently Asked Questions

### Q: How many samples do I need?

**A:** Minimum 50, recommended 100+.

- **< 50 samples**: PLS is safer (less overfitting)
- **50-200 samples**: Neural Boosted works well
- **200-1000 samples**: Excellent performance
- **> 1000 samples**: Random Forest might be faster

### Q: Can I use it for classification?

**A:** Yes! As of v2.0, `NeuralBoostedClassifier` supports both binary and multiclass classification.

**Features:**
- Binary classification using log-loss gradient boosting
- Multiclass via one-vs-rest strategy
- User-selectable early stopping metric (accuracy or log-loss)
- Class weighting for imbalanced datasets ('balanced' or custom)
- Full sklearn API: `predict()`, `predict_proba()`, `predict_log_proba()`

**Example - Binary Classification:**
```python
from spectral_predict.neural_boosted import NeuralBoostedClassifier

model = NeuralBoostedClassifier(
    n_estimators=50,
    learning_rate=0.1,
    hidden_layer_size=5,
    early_stopping_metric='accuracy',  # or 'log_loss'
    class_weight='balanced',  # for imbalanced data
    random_state=42
)

model.fit(X_train, y_train)
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)
```

**Example - Multiclass Classification:**
```python
# Same API, automatically uses one-vs-rest for >2 classes
model = NeuralBoostedClassifier(n_estimators=50, random_state=42)
model.fit(X_train, y_train)  # y_train has 3+ classes

# Get probabilities for all classes
proba = model.predict_proba(X_test)  # Shape: (n_samples, n_classes)
```

**When to use:**
- High-dimensional spectral classification (many wavelengths, few samples)
- When you need probability estimates with good calibration
- Imbalanced datasets (use `class_weight='balanced'`)
- When Random Forest/XGBoost overfit your small dataset

### Q: How do I cite this in a paper?

**Methodological references:**

For gradient boosting theory:
> Friedman, J. H. (2001). "Greedy Function Approximation: A Gradient Boosting Machine." *Annals of Statistics* 29(5): 1189-1232.

For Neural Boosted specifically (JMP implementation):
> SAS Institute Inc. (2021). JMP¬Æ 16 Fitting Linear Models. Cary, NC: SAS Institute Inc.

For your methods section:
> "We used Neural Boosted Regression, an ensemble method that combines small neural networks (3-5 nodes) via gradient boosting with early stopping to prevent overfitting. Hyperparameters (learning rate: 0.05-0.2, number of estimators: up to 100) were optimized via cross-validation."

### Q: Is it better than XGBoost or LightGBM?

**A:** XGBoost/LightGBM use decision trees as weak learners. Neural Boosted uses neural networks.

**Differences:**
- **Trees (XGBoost)**: Better for categorical features, missing data
- **Neural networks (Neural Boosted)**: Better for continuous features (like spectra!)

For spectral data, Neural Boosted and Random Forest often perform similarly.

### Q: Can I extract partial dependence plots?

**A:** Not built-in for v1.0, but you can use sklearn's tools:

```python
from sklearn.inspection import partial_dependence, PartialDependenceDisplay

# Fit model
model.fit(X, y)

# Compute partial dependence for wavelength 100
pd_result = partial_dependence(model, X, features=[100])

# Plot
PartialDependenceDisplay.from_estimator(model, X, features=[100, 200, 300])
```

---

## Changelog

### Version 1.0 (October 27, 2025)
- Initial release
- Regression support only
- MSE and Huber loss
- Feature importance extraction
- Integration with spectral prediction pipeline
- Early stopping
- Automatic hyperparameter grid search

### Planned for v2.0
- Classification support (NeuralBoostedClassifier)
- Gaussian activation function
- Adaptive learning rate
- Feature subsampling
- SHAP value integration
- Partial dependence plots

---

## Support & Feedback

**Questions?** Check:
1. This guide (NEURAL_BOOSTED_GUIDE.md)
2. Implementation plan (NEURAL_BOOSTED_IMPLEMENTATION_PLAN.md)
3. Wavelength selection docs (WAVELENGTH_SUBSET_SELECTION.md)

**Issues?** Report at: https://github.com/anthropics/claude-code/issues

**Want to contribute?** See CONTRIBUTING.md

---

## License

This implementation is part of the Spectral Predict package.

**References:**
- Gradient Boosting: Friedman (2001)
- Neural Boosted Methodology: JMP¬Æ Statistical Software
- Spectral Analysis: Workman & Weyer (2012)

---

*Happy Spectral Analysis!* üéâ
