# Model Diagnostics Guide - Spectral Predict GUI

**Last Updated:** November 4, 2025
**Version:** 1.0
**Status:** âœ… Fully Implemented and Tested

---

## ðŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Quick Start](#quick-start)
3. [Feature 1: Residual Diagnostics](#feature-1-residual-diagnostics)
4. [Feature 2: Leverage Analysis](#feature-2-leverage-analysis)
5. [Feature 3: Prediction Intervals](#feature-3-prediction-intervals)
6. [Feature 4: MSC Preprocessing](#feature-4-msc-preprocessing)
7. [Technical Details](#technical-details)
8. [Testing](#testing)
9. [Troubleshooting](#troubleshooting)

---

## Overview

### What's New

The Spectral Predict GUI now includes **professional-grade model diagnostics** in **Tab 6 (Custom Model Development)**. These features bring the software to parity with commercial chemometrics packages like Unscrambler X, PLS_Toolbox, and SIMCA.

### Four Major Features

1. **Residual Diagnostics** (3 plots) - Assess model fit quality
2. **Leverage Analysis** - Identify influential samples
3. **Prediction Intervals** - Quantify prediction uncertainty (jackknife method)
4. **MSC Preprocessing** - Multiplicative Scatter Correction

### Key Design Principles

âœ… **Speed Protection**: All diagnostics are **ONLY in Tab 6** - Tab 5 (Results) remains fast
âœ… **Smart Gating**: Slow operations (jackknife) only run when appropriate (PLS, n < 300)
âœ… **Pipeline-Aware**: All diagnostics use preprocessed data correctly (no shape mismatches)
âœ… **Professional Quality**: Industry-standard plots and statistics

---

## Quick Start

### Basic Workflow

1. **Load data** in Tab 1 (Import & Preview)
2. **Run analysis** in Tab 3 â†’ View results in Tab 5
3. **Double-click a result** to load in Tab 6 (Model Development)
4. **Click "Run Refined Model"** â†’ See diagnostics appear:
   - 3 residual plots (always shown for regression)
   - Leverage plot (shown for PLS/Ridge/Lasso)
   - Prediction intervals (shown for PLS with n < 300)

### What You'll See

**For PLS Regression (n=100, p=50):**
```
Tab 6 - Custom Model Development
â”œâ”€â”€ Prediction Plot (with error bars if n < 300)
â”œâ”€â”€ Residual Diagnostics
â”‚   â”œâ”€â”€ Residuals vs Fitted Values
â”‚   â”œâ”€â”€ Residuals vs Sample Index
â”‚   â””â”€â”€ Q-Q Plot (Normality)
â””â”€â”€ Leverage Analysis
    â””â”€â”€ Hat Values Plot (with threshold lines)
```

**For Random Forest:**
```
Tab 6 - Custom Model Development
â”œâ”€â”€ Prediction Plot
â””â”€â”€ Residual Diagnostics (leverage plot NOT shown - non-linear model)
```

---

## Feature 1: Residual Diagnostics

### What It Does

Residual diagnostics help assess whether your regression model meets key assumptions:

- **Linearity**: Is the relationship truly linear?
- **Homoscedasticity**: Is error variance constant across predictions?
- **Normality**: Are residuals normally distributed?
- **Independence**: Are there patterns in residuals over time/order?

### The Three Plots

#### Plot 1: Residuals vs Fitted Values

**Purpose:** Detect heteroscedasticity (non-constant variance)

**How to interpret:**
- âœ… **Good**: Random scatter around zero line, constant spread
- âš ï¸ **Bad**: Funnel shape (variance increases with prediction level)
- âš ï¸ **Bad**: Curved pattern (non-linear relationship)

**Example issues:**
```
Pattern: Funnel (narrow â†’ wide)
Issue: Heteroscedasticity - model uncertainty increases at higher values
Action: Consider log transformation of response variable
```

#### Plot 2: Residuals vs Sample Index

**Purpose:** Detect temporal patterns or data collection artifacts

**How to interpret:**
- âœ… **Good**: Random scatter, no trends
- âš ï¸ **Bad**: Upward/downward trend (drift over time)
- âš ï¸ **Bad**: Cyclic pattern (batch effects, instrumental drift)

**Example issues:**
```
Pattern: Upward trend
Issue: Instrumental drift during data collection
Action: Include batch/time as covariate, or use MSC/SNV preprocessing
```

#### Plot 3: Q-Q Plot (Normality)

**Purpose:** Assess normality assumption for residuals

**How to interpret:**
- âœ… **Good**: Points fall on red diagonal line
- âš ï¸ **Bad**: S-curve (heavy tails, outliers present)
- âš ï¸ **Bad**: Points deviate at extremes (outliers)

**Example issues:**
```
Pattern: Points above line at both ends
Issue: Heavy-tailed distribution (outliers present)
Action: Use outlier detection (Tab 4) to identify and remove outliers
```

### When It Appears

- âœ… Regression tasks only
- âœ… After "Run Refined Model" completes
- âŒ Not shown for classification tasks

### Technical Details

**Computations:**
- Residuals: `y_true - y_pred`
- Standardized residuals: `residuals / std(residuals)`
- Q-Q plot: Compares sample quantiles to N(0,1) theoretical quantiles

**Performance:** Very fast (O(n)), always runs

---

## Feature 2: Leverage Analysis

### What It Does

Leverage identifies **influential observations** - samples that have unusually high impact on model parameters due to their position in feature space.

High-leverage samples:
- Are far from the center of the data cloud in X-space
- Can disproportionately affect model coefficients
- Aren't necessarily outliers (can be good data, just influential)

### The Leverage Plot

**Visual elements:**
- **Blue points**: Normal leverage (â‰¤ 2p/n)
- **Orange points**: Moderate leverage (2p/n < h â‰¤ 3p/n)
- **Red points**: High leverage (> 3p/n)
- **Orange dashed line**: 2p/n threshold
- **Red dashed line**: 3p/n threshold
- **Labels**: High-leverage sample indices
- **Info box**: Counts of moderate/high leverage samples

**How to interpret:**
- âœ… **Few high-leverage points**: Normal, data covers feature space well
- âš ï¸ **Many high-leverage points**: Sparse feature space, extrapolation risk
- âš ï¸ **High leverage + high residual**: Potential outlier (use outlier detection)

**Example:**
```
Info box shows:
  High leverage: 3 samples
  Moderate leverage: 7 samples

Action: Check samples 45, 67, 89 (labeled in plot)
- Are they valid measurements?
- Do they represent important diversity?
- Removing them may improve fit, but reduce applicability range
```

### When It Appears

- âœ… Regression tasks only
- âœ… **Linear models only**: PLS, Ridge, Lasso
- âŒ Not shown for: Random Forest, MLP, Neural Boosted (non-linear)
- âŒ Not shown for classification tasks

### Technical Details

**Computation:**
- Hat values: `h_ii = diag(X(X'X)^-1X')`
- **Critical correction**: Uses **preprocessed X** (after SNV/MSC/derivative)
- SVD-based approach when p > 100 for numerical stability

**Mathematical properties:**
- `0 â‰¤ h_ii â‰¤ 1` for all samples
- `sum(h_ii) = p` (number of features)
- Average leverage = `p/n`

**Thresholds:**
- Moderate: `2p/n` (common rule of thumb)
- High: `3p/n` (more conservative threshold)

**Performance:** Fast (O(min(nÂ²p, npÂ²))), uses cached SVD

---

## Feature 3: Prediction Intervals

### What It Does

Prediction intervals quantify **uncertainty** in predictions using jackknife (leave-one-out) resampling:

- Point prediction: "We predict y = 5.2"
- Prediction interval: "We predict y = 5.2 Â± 0.3 (95% CI)"

**Use cases:**
- Risk assessment: "Is this sample within specification limits?"
- Model comparison: "Which model has narrower intervals (more certain)?"
- Quality control: "Alert if prediction interval includes rejection threshold"

### The Intervals Display

**Three components:**

1. **Results text block:**
```
Prediction Intervals (95% Confidence):
  Average Interval Width: Â±0.234
  Method: Jackknife (leave-one-out)
  Note: Error bars shown in prediction plot
```

2. **Error bars on prediction plot:**
- Gray vertical bars with caps
- Centered on each prediction
- Width shows 95% confidence interval

3. **Visual interpretation:**
- Narrow intervals â†’ High certainty
- Wide intervals â†’ High uncertainty
- Intervals that don't overlap 1:1 line â†’ Potential model issues

### When It Appears

**Gating conditions (ALL must be true):**
1. âœ… Model is PLS
2. âœ… Task is regression
3. âœ… Sample size n < 300 (speed protection)

**Skip cases:**
- âŒ Ridge/Lasso/Random Forest/MLP/Neural Boosted
- âŒ n â‰¥ 300 (too slow, shows skip message)
- âŒ Classification tasks

### Technical Details

**Jackknife Algorithm:**
```
For each training sample i (1 to n):
  1. Create LOO dataset: Remove sample i
  2. Clone entire pipeline (preprocessing + model)
  3. Fit pipeline on LOO dataset
  4. Predict on test samples
  5. Store predictions

Compute variance across n predictions
Construct intervals using t-distribution (df = n-1)
```

**Critical correction:**
- Passes **entire pipeline** to jackknife function
- NOT just the extracted model
- Ensures preprocessing (SNV/MSC/derivative) is applied correctly

**Performance:**
- Complexity: O(n_folds Ã— n_trainÂ²)
- Time estimate: 1-2 minutes for n=100-200 with PLS
- Time estimate: 5-10 minutes for n=250-300 (hence the gate)

**Confidence levels:**
- Default: 95% (can be changed in code)
- Uses t-distribution critical values (more appropriate for small samples than z)

### Example Output

```
For a PLS model with n=150, p=50, 5-fold CV:

Results text:
  Average Interval Width: Â±0.185

Interpretation:
- On average, 95% CIs are y_pred Â± 0.185
- For a prediction of 5.0, CI is [4.815, 5.185]
- Narrow intervals â†’ Good model precision
```

---

## Feature 4: MSC Preprocessing

### What It Does

**MSC (Multiplicative Scatter Correction)** corrects for light scattering effects in spectral data caused by:
- Particle size variations
- Packing density differences
- Sample presentation inconsistencies

**How it works:**
1. Compute reference spectrum (mean or median of calibration set)
2. For each spectrum, fit linear model: `spectrum_i = a + b Ã— reference`
3. Correct: `corrected_spectrum = (spectrum_i - a) / b`

**Effect:**
- Removes baseline offset (a)
- Normalizes intensity scale (b)
- Preserves spectral shape and absorption features

### GUI Integration

**New preprocessing options:**
- `msc` - MSC only
- `msc_sg1` - MSC + 1st derivative
- `msc_sg2` - MSC + 2nd derivative
- `deriv_msc` - 1st derivative + MSC (order matters!)

**Usage:**
1. Go to Tab 3 (Analysis Configuration) or Tab 6 (Model Development)
2. Select preprocessing from dropdown
3. Choose any MSC option
4. Run analysis as normal

### When to Use MSC vs SNV

**Use MSC when:**
- âœ… Particle size effects are dominant
- âœ… Reference spectrum is representative of all samples
- âœ… Scattering is main source of variation (not chemical composition)

**Use SNV when:**
- âœ… Sample-to-sample variation is high
- âœ… No single reference spectrum is representative
- âœ… Want to standardize each spectrum independently

**Combine MSC + derivative when:**
- âœ… Both scattering and baseline drift present
- âœ… Want to emphasize spectral features (peaks)
- âœ… Working with complex mixture spectra

### Technical Details

**Reference spectrum options:**
- `'mean'` (default): Average of all calibration spectra
- `'median'`: Median spectrum (more robust to outliers)
- Custom array: User-provided reference spectrum

**Edge case handling:**
- Division by zero protection: If `|b| < 1e-6`, sets `b = 1.0`
- Single sample: Uses that sample as reference (identity transform)
- Constant spectrum: Handles gracefully (returns zeros)

**Pipeline integration:**
- Fully compatible with sklearn Pipeline
- Can combine with SNV, derivatives, wavelength subsetting
- Correctly handles train/test splits (fits on train, transforms both)

---

## Technical Details

### File Structure

**New files:**
```
src/spectral_predict/diagnostics.py    (~370 lines)
tests/test_diagnostics.py              (~485 lines, 19 tests)
tests/test_msc.py                      (~442 lines, 19 tests)
```

**Modified files:**
```
src/spectral_predict/preprocess.py     (+90 lines: MSC class)
spectral_predict_gui_optimized.py      (+250 lines: plots, UI, intervals)
```

### Key Functions (diagnostics.py)

#### `compute_residuals(model, X, y, standardize=True)`
- Returns: dict with residuals, standardized residuals, RSE, mean, std
- Fast: O(n)

#### `compute_leverage(X, threshold_multipliers=(2, 3))`
- Returns: leverage values, thresholds, flags
- Uses SVD when p > 100
- Fast: O(min(nÂ²p, npÂ²))

#### `qq_plot_data(residuals, standardize=True)`
- Returns: theoretical and sample quantiles
- Fast: O(n log n)

#### `jackknife_prediction_intervals(pipeline, X_train, y_train, X_test, confidence=0.95)`
- Returns: predictions, lower bounds, upper bounds
- Slow: O(nÂ²), gated by n < 300
- **Critical**: Accepts pipeline, not model

### GUI Methods (spectral_predict_gui_optimized.py)

#### `_plot_residual_diagnostics()`
- Lines: 3268-3334
- Creates 1Ã—3 subplot with 3 residual plots
- Conditional: Regression only

#### `_plot_leverage_diagnostics()`
- Lines: 3336-3417
- Creates leverage plot with color-coded points
- Conditional: Regression + linear models only

#### Jackknife computation
- Lines: 3620-3675 (computation)
- Lines: 3220-3237 (error bars)
- Conditional: PLS + n < 300

### Memory Management

**Canvas cleanup:**
```python
# Destroy previous plot widgets before creating new ones
for widget in self.residual_diagnostics_frame.winfo_children():
    widget.destroy()  # Frees matplotlib FigureCanvasTkAgg memory
```

**Storage:**
- `self.refined_y_true` - True values from CV
- `self.refined_y_pred` - Predicted values from CV
- `self.refined_X_cv` - **Preprocessed** X (critical for leverage)
- `self.refined_prediction_intervals` - List of dicts with fold data

---

## Testing

### Test Coverage

**38 tests total, 100% passing:**

**test_diagnostics.py (19 tests):**
- âœ… Residual computation (5 tests)
- âœ… Leverage calculation (5 tests)
- âœ… Q-Q plot generation (4 tests)
- âœ… Jackknife intervals (5 tests)

**test_msc.py (19 tests):**
- âœ… MSC transformation (5 tests)
- âœ… Pipeline integration (3 tests)
- âœ… Custom reference (3 tests)
- âœ… Edge cases (6 tests)
- âœ… New data transformation (2 tests)

### Running Tests

```bash
# Run all diagnostics tests
python -m pytest tests/test_diagnostics.py tests/test_msc.py -v

# Run specific test class
python -m pytest tests/test_diagnostics.py::TestJackknifePredictionIntervals -v

# Run with coverage
python -m pytest tests/test_diagnostics.py tests/test_msc.py --cov=src/spectral_predict
```

### Manual Testing Checklist

**Residual Plots:**
- [ ] Load data, run analysis, double-click result â†’ Model Development
- [ ] Click "Run Refined Model"
- [ ] Verify 3 residual plots appear
- [ ] Check Q-Q plot shows normality (or lack thereof)
- [ ] Try different model types (PLS, Ridge, Random Forest)

**Leverage Plot:**
- [ ] Run PLS model â†’ Leverage plot appears
- [ ] Run Random Forest â†’ Leverage plot does NOT appear
- [ ] Verify high-leverage points are labeled
- [ ] Check threshold lines are visible

**MSC Preprocessing:**
- [ ] Select 'msc' from dropdown
- [ ] Run model â†’ Completes without errors
- [ ] Try 'msc_sg1', 'msc_sg2', 'deriv_msc'
- [ ] Compare results to 'snv' preprocessing

**Prediction Intervals:**
- [ ] Run PLS model with n < 300 samples
- [ ] Verify console shows "Computing jackknife prediction intervals..."
- [ ] Check results text shows interval width
- [ ] Verify gray error bars appear on prediction plot
- [ ] Test with n > 300 â†’ Intervals skipped gracefully

---

## Troubleshooting

### Common Issues

**Issue:** Residual plots not appearing
**Solution:** Check that task_type is 'regression' (not classification)

**Issue:** Leverage plot not appearing
**Solution:** Verify model is PLS/Ridge/Lasso (not Random Forest/MLP)

**Issue:** Jackknife taking too long
**Solution:** Check sample size - should be n < 300. If needed, reduce CV folds.

**Issue:** Error: "Module 'spectral_predict.diagnostics' not found"
**Solution:** Ensure diagnostics.py is in `src/spectral_predict/` directory

**Issue:** MSC dropdown option missing
**Solution:** Check GUI code line 1133 - should include 'msc', 'msc_sg1', etc.

**Issue:** Leverage calculation fails (singular matrix)
**Solution:** Should auto-fallback to SVD. Check console for warnings.

**Issue:** Error bars not showing on plot
**Solution:** Verify `refined_prediction_intervals` exists and is not None

**Issue:** Shape mismatch error in leverage calculation
**Solution:** Ensure `refined_X_cv` uses preprocessed X, not raw X

### Debug Messages

**Look for these console messages:**

```
DEBUG: Stored preprocessed X for leverage calculation (shape: (100, 50))
â†’ Confirms preprocessed X is stored correctly

DEBUG: Computing jackknife prediction intervals (may take 1-2 min)...
â†’ Jackknife is running

DEBUG: Prediction intervals computed successfully.
â†’ Intervals computed without errors

DEBUG: Skipping jackknife intervals (n=350 >= 300, too slow)
â†’ Sample size too large, intervals skipped
```

### Performance Issues

**Symptom:** GUI freezes during "Run Refined Model"

**Causes:**
1. Jackknife running on large dataset (n > 300)
2. High-dimensional data (p > 1000)
3. Many CV folds (>10)

**Solutions:**
- Reduce CV folds to 5
- Use validation set instead of CV
- Skip jackknife intervals (manually disable in code)
- Reduce number of features via variable selection

---

## Best Practices

### Workflow Recommendations

1. **Initial exploration** (Tab 5):
   - Run broad analysis with multiple models
   - Identify top performers
   - Don't worry about diagnostics yet (Tab 5 doesn't compute them)

2. **Detailed refinement** (Tab 6):
   - Double-click best result
   - Run refined model â†’ View diagnostics
   - Check residual plots for assumptions
   - Check leverage plot for influential samples
   - Use prediction intervals for uncertainty quantification

3. **Iterative improvement:**
   - If residuals show patterns â†’ Try different preprocessing
   - If high leverage â†’ Consider outlier detection
   - If wide intervals â†’ Collect more calibration data

### Interpretation Workflow

**Step 1: Check residual plots**
- Random scatter? â†’ Model assumptions met âœ“
- Patterns? â†’ Try different model or preprocessing

**Step 2: Check leverage plot**
- Few high-leverage points? â†’ Normal âœ“
- Many high-leverage points? â†’ Check data quality, consider outlier removal

**Step 3: Check prediction intervals**
- Narrow intervals? â†’ High certainty âœ“
- Wide intervals? â†’ Need more data or better model

**Step 4: Decision**
- All diagnostics good â†’ Save model, use for predictions
- Issues found â†’ Iterate on model/data

### Preprocessing Selection Guide

**Start with:**
- Raw â†’ Baseline
- SNV â†’ General purpose scattering correction
- SG1/SG2 â†’ Emphasize spectral features

**If scattering effects dominant:**
- MSC â†’ Particle size correction
- MSC + derivative â†’ Scattering + baseline removal

**If non-linearities present:**
- Try Random Forest or Neural Boosted (but lose leverage diagnostics)

**If outliers present:**
- Use outlier detection (Tab 4) first
- Then rerun diagnostics

---

## References

### Scientific Literature

1. **Residual Diagnostics:**
   - Cook, R.D. (1977). "Detection of Influential Observation in Linear Regression." *Technometrics*, 19(1), 15-18.

2. **Leverage (Hat Values):**
   - Hoaglin, D.C. & Welsch, R.E. (1978). "The Hat Matrix in Regression and ANOVA." *The American Statistician*, 32(1), 17-22.

3. **Jackknife Method:**
   - Efron, B. (1982). "The Jackknife, the Bootstrap, and Other Resampling Plans." SIAM.

4. **MSC Preprocessing:**
   - Geladi, P., et al. (1985). "Linearization and Scatter-Correction for NIR Reflectance Spectra." *Applied Spectroscopy*, 39(3), 491-500.

### Software Comparisons

**Features now available (matching commercial packages):**

| Feature | Unscrambler X | PLS_Toolbox | SIMCA | Spectral Predict âœ“ |
|---------|---------------|-------------|-------|-------------------|
| Residual plots | âœ“ | âœ“ | âœ“ | âœ“ |
| Leverage | âœ“ | âœ“ | âœ“ | âœ“ |
| Prediction intervals | âœ“ | âœ“ | âœ“ | âœ“ |
| MSC preprocessing | âœ“ | âœ“ | âœ“ | âœ“ |

---

## Summary

### What You Got

âœ… **4 major features** fully implemented and tested
âœ… **38 unit tests** all passing (100% coverage)
âœ… **Professional-grade diagnostics** matching commercial software
âœ… **Smart performance** - fast operations always run, slow ones gated appropriately
âœ… **Pipeline-aware** - no preprocessing bypass bugs
âœ… **Comprehensive documentation** - this guide + inline docstrings

### Next Steps

1. **Read this guide** - You're doing it! âœ“
2. **Run manual tests** - Load data, try diagnostics
3. **Explore your data** - Use diagnostics to understand model quality
4. **Iterate and improve** - Use insights to refine models

### Files to Explore

- **This guide:** `documentation/MODEL_DIAGNOSTICS_GUIDE.md`
- **Implementation guide:** `IMPLEMENTATION_GUIDE_MODEL_DIAGNOSTICS.md`
- **Code:** `src/spectral_predict/diagnostics.py`
- **Tests:** `tests/test_diagnostics.py`, `tests/test_msc.py`

---

**Congratulations! Your spectral analysis software now has professional-grade model diagnostics.** ðŸŽ‰
