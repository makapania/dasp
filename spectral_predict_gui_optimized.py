"""
Spectral Predict - Redesigned 7-Tab GUI Application (OPTIMIZED)

Tab 1: Import & Preview - Data loading + spectral plots
Tab 2: Data Quality Check - Outlier detection and exclusion
Tab 3: Analysis Configuration - All analysis settings
Tab 4: Analysis Progress - Live progress monitor
Tab 5: Results - Analysis results table (clickable to refine)
Tab 6: Model Development - Interactive model refinement
Tab 7: Model Prediction - Load saved models and predict on new data

OPTIMIZED VERSION:
- Neural Boosted max_iter reduced from 500 to 100 (Phase A optimization)
- Implements evidence-based optimizations for 2-3x speedup
- Phase 3: Integrated outlier detection system with unified exclusion
- Phase 3+: Model prediction tab for applying saved .dasp models
"""

import sys
import os
import ast
from pathlib import Path
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import threading
from datetime import datetime
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Sound notification for analysis completion
try:
    import winsound
    HAS_WINSOUND = True
except ImportError:
    HAS_WINSOUND = False

# Image loading for logos
try:
    from PIL import Image, ImageTk
    HAS_PIL = True
except ImportError:
    HAS_PIL = False

# Check for matplotlib
try:
    import matplotlib
    matplotlib.use('TkAgg')
    import matplotlib.pyplot as plt
    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk
    from matplotlib.figure import Figure
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

# Add src to path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

# Import spectral_predict modules
try:
    from spectral_predict.preprocess import SavgolDerivative
    from spectral_predict.model_registry import get_supported_models, is_valid_model
    from spectral_predict.model_config import get_tier_models, CLASSIFICATION_TIERS, MODEL_TIERS
    HAS_DERIVATIVES = True
except ImportError:
    HAS_DERIVATIVES = False
    SavgolDerivative = None
    get_supported_models = None
    is_valid_model = None
    get_tier_models = None
    CLASSIFICATION_TIERS = None
    MODEL_TIERS = None

try:
    from spectral_predict.outlier_detection import generate_outlier_report
    HAS_OUTLIER_DETECTION = True
except ImportError:
    HAS_OUTLIER_DETECTION = False
    generate_outlier_report = None

# Check for CatBoost availability
try:
    from catboost import CatBoostRegressor
    HAS_CATBOOST = True
except ImportError:
    HAS_CATBOOST = False
    CatBoostRegressor = None

# Import data management module
try:
    from spectral_predict.data_management import (
        DataSource,
        MergedDataset,
        DataSourceManager,
    )
    HAS_DATA_MANAGEMENT = True
except ImportError:
    HAS_DATA_MANAGEMENT = False
    DataSource = None
    MergedDataset = None
    DataSourceManager = None

# Import calibration transfer and instrument profile modules
try:
    from spectral_predict.instrument_profiles import (
        characterize_instrument,
        save_instrument_profiles,
        load_instrument_profiles,
        rank_instruments_by_detail,
        estimate_smoothing_between_instruments,
    )
    from spectral_predict.calibration_transfer import (
        resample_to_grid,
        estimate_ds,
        estimate_pds,
        apply_ds,
        apply_pds,
        save_transfer_model,
        load_transfer_model,
    )
    from spectral_predict.equalization import (
        choose_common_grid,
        equalize_dataset,
    )
    HAS_CALIBRATION_TRANSFER = True
except ImportError:
    HAS_CALIBRATION_TRANSFER = False


# ===== NATIVE TKINTER TOOLTIP CLASS =====
# No external dependencies required - uses pure tkinter
class CreateToolTip(object):
    """
    Create a tooltip for a given widget.
    Uses native tkinter - no external packages required.
    """
    def __init__(self, widget, text='widget info', delay=500):
        self.waittime = delay  # milliseconds
        self.wraplength = 400  # pixels
        self.widget = widget
        self.text = text
        self.widget.bind("<Enter>", self.enter)
        self.widget.bind("<Leave>", self.leave)
        self.widget.bind("<ButtonPress>", self.leave)
        self.id = None
        self.tw = None

    def enter(self, event=None):
        self.schedule()

    def leave(self, event=None):
        self.unschedule()
        self.hidetip()

    def schedule(self):
        self.unschedule()
        self.id = self.widget.after(self.waittime, self.showtip)

    def unschedule(self):
        id = self.id
        self.id = None
        if id:
            self.widget.after_cancel(id)

    def showtip(self, event=None):
        x = y = 0
        x, y, cx, cy = self.widget.bbox("insert")
        x += self.widget.winfo_rootx() + 25
        y += self.widget.winfo_rooty() + 20
        # creates a toplevel window
        self.tw = tk.Toplevel(self.widget)
        # Leaves only the label and removes the app window
        self.tw.wm_overrideredirect(True)
        self.tw.wm_geometry("+%d+%d" % (x, y))
        label = tk.Label(self.tw, text=self.text, justify='left',
                       background="#ffffe0", relief='solid', borderwidth=1,
                       wraplength=self.wraplength, font=("tahoma", "9", "normal"))
        label.pack(ipadx=1)

    def hidetip(self):
        tw = self.tw
        self.tw = None
        if tw:
            tw.destroy()


# ===== TOOLTIP CONTENT DICTIONARY =====
# Centralized repository of tooltip descriptions for models and hyperparameters
TOOLTIP_CONTENT = {
    # ===== MODEL DESCRIPTIONS =====
    'models': {
        'PLS': (
            "Partial Least Squares (PLS) is a linear regression method that finds latent variables "
            "maximizing covariance between spectral data and target values. Excellent for high-dimensional "
            "data with multicollinearity (correlated wavelengths). Fast, interpretable, and works well "
            "with more variables than samples. Ideal for quantitative spectroscopy."
        ),
        'PLS-DA': (
            "PLS Discriminant Analysis extends PLS for classification tasks by treating categorical "
            "labels as continuous variables. Uses latent variables to maximize class separation. "
            "Effective for spectral classification with multiple correlated features. Returns probability "
            "scores for each class."
        ),
        'Ridge': (
            "Ridge Regression is linear regression with L2 regularization that shrinks coefficient "
            "magnitudes toward zero without eliminating any. Prevents overfitting by penalizing large "
            "coefficients. Good for spectral data with multicollinearity. Controlled by alpha parameter - "
            "higher alpha = more regularization."
        ),
        'Lasso': (
            "Lasso Regression uses L1 regularization that can shrink coefficients exactly to zero, "
            "performing automatic feature selection. Creates sparse models by identifying the most "
            "important wavelengths. Useful when you want to know which specific wavelengths matter most. "
            "Higher alpha = more sparsity."
        ),
        'ElasticNet': (
            "ElasticNet combines Ridge (L2) and Lasso (L1) regularization, getting benefits of both. "
            "Can select groups of correlated features (like Lasso) while maintaining stability (like Ridge). "
            "Controlled by alpha (regularization strength) and l1_ratio (0=Ridge only, 0.5=balanced, 1=Lasso only). "
            "Excellent for spectral data with highly correlated wavelengths."
        ),
        'RandomForest': (
            "Random Forest builds many decision trees on random subsets of data and features, then "
            "averages their predictions. Nonlinear, handles outliers well, and resistant to overfitting. "
            "No assumptions about data distribution. Can capture complex spectral patterns. "
            "Slower than linear models but very robust and versatile."
        ),
        'MLP': (
            "Multi-Layer Perceptron is a feedforward neural network with hidden layers that can learn "
            "complex nonlinear relationships. Uses backpropagation for training. Can approximate any "
            "continuous function given enough neurons. Good for spectral data with nonlinear relationships "
            "but requires more samples and careful tuning to avoid overfitting."
        ),
        'SVR': (
            "Support Vector Regression finds a hyperplane that best fits the data while allowing "
            "a margin of tolerance (epsilon). Can use kernels (linear, RBF, poly) to capture nonlinear "
            "patterns. Effective for small to medium datasets. RBF kernel good for complex spectral patterns. "
            "Memory-intensive for large datasets."
        ),
        'XGBoost': (
            "XGBoost is gradient boosting that builds trees sequentially, each correcting errors of "
            "previous trees. Industry-leading performance with built-in regularization (L1/L2). "
            "Handles missing data, reduces overfitting, and excellent for spectral data. Fast parallel "
            "processing. Highly tunable with many hyperparameters for optimal performance."
        ),
        'LightGBM': (
            "LightGBM is Microsoft's gradient boosting framework optimized for speed and memory efficiency. "
            "Uses leaf-wise tree growth (vs level-wise) for faster training. Excellent for large datasets "
            "with many features (like spectral data). Faster than XGBoost with similar performance. "
            "Great for high-dimensional spectroscopy applications."
        ),
        'CatBoost': (
            "CatBoost is Yandex's gradient boosting that handles categorical features automatically "
            "and reduces overfitting with ordered boosting. Very robust with minimal tuning required - "
            "good default hyperparameters. Slower than LightGBM but often achieves better accuracy out-of-box. "
            "Note: Requires Visual Studio 2022 Build Tools on Windows."
        ),
        'NeuralBoosted': (
            "Neural Boosted combines gradient boosting with neural networks as base learners instead of "
            "decision trees. Each boosting round adds a small neural network. Can capture very complex "
            "nonlinear patterns in spectral data. More powerful than standard tree-based boosting but "
            "slower to train. Best for complex spectroscopy problems with sufficient data."
        ),
    },

    # ===== PREPROCESSING METHOD DESCRIPTIONS =====
    'preprocessing': {
        'Raw': (
            "Raw (no preprocessing) uses the original, unprocessed spectral data exactly as measured. "
            "No transformations applied. Useful as a baseline comparison to see if preprocessing helps. "
            "Works well when spectra are already clean and consistent, or when you want to preserve "
            "absolute intensity information. Start here to establish baseline performance."
        ),
        'SNV': (
            "Standard Normal Variate (SNV) corrects for scatter effects and baseline variations by "
            "normalizing each spectrum to zero mean and unit variance. Removes multiplicative and "
            "additive effects from particle size, path length differences, and light scattering. "
            "Very effective for solid samples and diffuse reflectance spectroscopy. "
            "One of the most commonly used preprocessing methods in NIR spectroscopy."
        ),
        'SG1': (
            "Savitzky-Golay 1st Derivative calculates the first derivative of spectra using polynomial "
            "smoothing. Removes baseline offset and linear trends while preserving peak shapes. "
            "Highlights regions of spectral change and reduces baseline drift effects. Enhances "
            "differences between spectra. Derivative window size controls smoothing - larger windows = "
            "more smoothing but may miss fine details."
        ),
        'SG2': (
            "Savitzky-Golay 2nd Derivative calculates the second derivative of spectra. "
            "Removes both baseline offset and linear drift, and enhances peak shapes by showing "
            "inflection points and hidden shoulders. Very sensitive to subtle spectral features. "
            "Provides excellent peak resolution but amplifies noise more than 1st derivative. "
            "Best for overlapping peaks and complex spectral patterns."
        ),
        'deriv_snv': (
            "Derivative then SNV applies Savitzky-Golay derivative first, followed by SNV normalization. "
            "This advanced combination removes baseline effects (derivative) then normalizes the "
            "derivative spectra (SNV). Less common than SNV alone or derivative alone. "
            "Can be useful for very noisy data or when both scatter correction and baseline removal "
            "are needed, but may over-process in some cases."
        ),
        'window_size': (
            "Derivative window size controls the Savitzky-Golay smoothing window (must be odd number). "
            "Smaller windows (7-11) = less smoothing, preserves fine details, more noise. "
            "Medium windows (13-17) = balanced smoothing, good for most applications (default: 17). "
            "Larger windows (19-25) = more smoothing, removes noise, may lose sharp features. "
            "Choose based on your spectral resolution and noise level. Can test multiple values."
        ),
    },

    # ===== HYPERPARAMETER DESCRIPTIONS =====
    'hyperparameters': {
        # Neural Boosted
        'neuralboosted_n_estimators': (
            "Number of boosting rounds (sequential neural networks to train). Each round adds a small "
            "neural network that corrects errors from previous rounds. More estimators = better fit but "
            "slower training and risk of overfitting. Typical range: 50-200 for spectral data."
        ),
        'neuralboosted_learning_rate': (
            "Controls step size when updating the model (how much each boosting round contributes). "
            "Lower values (0.05-0.1) = more conservative, less overfitting, but needs more estimators. "
            "Higher values (0.2-0.3) = faster learning but risk overfitting. "
            "Range 0.1-0.3 works well for most spectroscopy applications."
        ),
        'neuralboosted_hidden_layer_size': (
            "Number of neurons in the hidden layer of each weak learner neural network. "
            "Keep small (3-5) to maintain weak learner properties and ensemble diversity. "
            "Larger values (7-10) create stronger learners but may reduce benefit of boosting. "
            "Default: 3 and 5 (grid search explores both options)."
        ),
        'neuralboosted_activation': (
            "Activation function for the hidden layer in weak learner neural networks. "
            "tanh = Smooth sigmoid-like nonlinear function (default, good for spectral data). "
            "identity = Linear activation (fast, simple baseline). "
            "relu = Rectified Linear Unit (standard deep learning activation). "
            "logistic = Sigmoid function (0 to 1 output range)."
        ),
        'neuralboosted_max_iter': (
            "Maximum iterations for each neural base learner during training. Controls how long each "
            "small neural network trains. Higher = better fit per network but slower overall. "
            "100-200 typically sufficient for spectral features. Reduced from default 500 for speed optimization."
        ),

        # Random Forest (these already have tooltips, but including here for completeness)
        'rf_n_estimators': (
            "Number of decision trees in the forest. More trees = better performance and stability but "
            "slower training and prediction. Returns typically diminish after 100-200 trees. "
            "Typical range: 100-500 for spectroscopy. Can use 1000+ if you have time."
        ),
        'rf_max_depth': (
            "Maximum depth each tree can grow. None = unlimited (trees grow until leaves are pure or "
            "reach min_samples_split). Lower values (10-30) prevent overfitting by limiting tree complexity. "
            "Higher values or None allow capturing complex patterns but risk overfitting."
        ),

        # Ridge
        'ridge_alpha': (
            "Regularization strength (penalty for large coefficients). Controls the bias-variance tradeoff. "
            "Lower values (0.001-0.1) = less regularization, closer to ordinary least squares. "
            "Higher values (1-10+) = more shrinkage, simpler model. Optimal value depends on data complexity "
            "and noise level. Cross-validation finds the best alpha."
        ),

        # Lasso
        'lasso_alpha': (
            "Regularization strength that controls sparsity (how many coefficients become zero). "
            "Lower values (0.001-0.01) = keep more wavelengths, less feature selection. "
            "Higher values (0.1-1+) = aggressive feature selection, very sparse model. "
            "Useful for identifying the most important wavelengths in your spectra."
        ),

        # ElasticNet
        'elasticnet_alpha': (
            "Overall regularization strength combining L1 and L2 penalties. Similar to Ridge/Lasso alpha. "
            "Lower = less regularization. Higher = more regularization. Works with l1_ratio to determine "
            "the mix of L1 (sparsity) vs L2 (coefficient shrinkage). Typical range: 0.01-1.0."
        ),
        'elasticnet_l1_ratio': (
            "Controls the mix of L1 (Lasso) vs L2 (Ridge) regularization. "
            "0.0 = Ridge only (no feature selection, all wavelengths kept). "
            "0.5 = balanced mix of L1 and L2. "
            "1.0 = Lasso only (aggressive feature selection). "
            "Values 0.3-0.7 often work well for spectral data with correlated features."
        ),

        # PLS
        'pls_max_n_components': (
            "Maximum number of latent variables (components) to evaluate during model optimization. "
            "PLS extracts these components from spectral data to predict target values. More components "
            "can capture more variance but risk overfitting. Cross-validation automatically finds the "
            "optimal number ≤ this maximum. Typical range: 5-20 for spectroscopy."
        ),
        'pls_max_iter': (
            "Maximum iterations for the PLS algorithm to converge when extracting each component. "
            "PLS uses an iterative algorithm (NIPALS) to find latent variables. Most spectral data "
            "converges quickly (<100 iterations). 500-1000 is safe for all cases. Higher values "
            "rarely needed but won't hurt (just ensures convergence)."
        ),
        'pls_tol': (
            "Convergence tolerance for the PLS iterative algorithm. Smaller = more precision but "
            "longer computation. Algorithm stops when change between iterations < tolerance. "
            "1e-6 (0.000001) is standard and works well for spectral data. "
            "1e-7 = higher precision (rarely needed). 1e-5 = faster but less precise."
        ),

        # XGBoost
        'xgb_n_estimators': (
            "Number of boosting rounds (trees) to build sequentially. Each tree corrects errors from "
            "previous trees. More estimators = better fit but slower and risk overfitting (use with "
            "lower learning_rate). Typical range: 100-200 for spectral data. "
            "Can use 500+ with low learning_rate for best performance."
        ),
        'xgb_learning_rate': (
            "Step size shrinkage to prevent overfitting (also called eta). Lower values make model more "
            "conservative by reducing the contribution of each tree. "
            "0.01-0.05 = very conservative (use with many trees). "
            "0.1 = balanced (standard default). "
            "0.2-0.3 = aggressive (faster but risk overfitting). For spectral data, 0.05-0.1 often optimal."
        ),
        'xgb_max_depth': (
            "Maximum depth for each decision tree. Controls model complexity and overfitting. "
            "Shallow trees (3-6) = simple patterns, less overfitting, faster. "
            "Deep trees (9-12) = complex patterns, more overfitting risk. "
            "For spectroscopy with many features, 3-6 usually optimal. Deeper rarely helps."
        ),
        'xgb_subsample': (
            "Fraction of training samples used for each tree (row sampling). Adds randomness to prevent "
            "overfitting. 1.0 = use all samples. 0.8 = use 80% (recommended for robustness). "
            "0.5-0.7 = more aggressive sampling (helps with very noisy data). "
            "Lower values increase diversity but may need more trees."
        ),
        'xgb_colsample_bytree': (
            "Fraction of features (wavelengths) used for each tree. Critical for high-dimensional spectral "
            "data (1000+ wavelengths). 1.0 = use all wavelengths. 0.8 = use 80% (recommended). "
            "0.5-0.7 = aggressive feature sampling (increases tree diversity). "
            "Helps prevent overfitting when you have thousands of correlated wavelengths."
        ),
        'xgb_reg_alpha': (
            "L1 regularization (Lasso-style) on tree leaf weights. Encourages sparsity in leaf values. "
            "0 = no L1 penalty (default). 0.1-0.5 = light regularization (recommended for high-dim data). "
            "1-5 = strong regularization (very sparse model). Helps with feature selection and "
            "prevents overfitting in spectroscopy with many wavelengths."
        ),
        'xgb_reg_lambda': (
            "L2 regularization (Ridge-style) on tree leaf weights. Smooths leaf values without forcing "
            "sparsity. 1.0 = default (light regularization). 5-10 = moderate (comprehensive tier). "
            "Higher values make model more conservative. Combine with reg_alpha for ElasticNet-style "
            "regularization. Good for noisy spectral data."
        ),
        'xgb_min_child_weight': (
            "Minimum sum of instance weight (hessian) needed in a child node. Controls overfitting by "
            "requiring minimum data in each leaf. 1 = allow small leaves (risk overfitting). "
            "3-5 = moderate constraint (balanced). 7-10 = conservative (prevents very specific patterns). "
            "Larger values good for noisy data or small datasets."
        ),
        'xgb_gamma': (
            "Minimum loss reduction required to make a split. Regularization parameter that makes the "
            "algorithm more conservative. 0 = split freely (default). 0.1-0.5 = light regularization. "
            "1-5 = strong regularization (very conservative splitting). "
            "Higher values prevent overfitting by rejecting weak splits."
        ),

        # LightGBM
        'lightgbm_max_depth': (
            "Maximum tree depth. -1 = no limit (controlled by num_leaves and min_data_in_leaf instead). "
            "5-10 = shallow to moderate trees. 20-50 = deep trees (may overfit). "
            "LightGBM uses leaf-wise growth (vs level-wise), so depth matters less than other algorithms. "
            "-1 with proper num_leaves usually optimal for spectral data."
        ),
        'lightgbm_min_child_samples': (
            "Minimum number of samples required in each leaf node. Controls overfitting by preventing "
            "very small leaves. 5 = allow small leaves (may overfit on small datasets). "
            "20-50 = moderate constraint (balanced). 100+ = conservative (smoother model). "
            "Critical for small spectroscopy datasets - use higher values (20-50)."
        ),
        'lightgbm_subsample': (
            "Fraction of training samples used per tree (also called bagging_fraction). "
            "1.0 = use all samples. 0.8-0.9 = recommended (adds robustness). "
            "0.5-0.7 = aggressive sampling (more diversity). "
            "Speeds up training and prevents overfitting. Must set bagging_freq>0 to activate."
        ),
        'lightgbm_colsample_bytree': (
            "Fraction of features used per tree (also called feature_fraction). Important for spectral "
            "data with many wavelengths. 1.0 = use all wavelengths. 0.8-0.9 = recommended. "
            "0.5-0.7 = aggressive feature sampling (increases diversity). "
            "Helps when you have thousands of correlated spectral features."
        ),
        'lightgbm_reg_alpha': (
            "L1 regularization on leaf weights (Lasso-style). Encourages sparse leaf values. "
            "0.0 = no L1 penalty (default). 0.1-0.5 = light regularization. "
            "1-5 = strong regularization. Useful for high-dimensional spectral data to prevent "
            "overfitting and perform implicit feature selection."
        ),
        'lightgbm_reg_lambda': (
            "L2 regularization on leaf weights (Ridge-style). Smooths leaf values. "
            "0.0 = no L2 penalty (default). 0.5-1.0 = light regularization. "
            "2-10 = moderate to strong regularization. Combines with reg_alpha for ElasticNet-style "
            "regularization. Helps prevent overfitting on noisy spectral data."
        ),

        # MLP
        'mlp_activation': (
            "Activation function for hidden layers. Introduces nonlinearity to learn complex patterns. "
            "relu = Rectified Linear Unit (max(0,x)) - fast, works well, standard choice. "
            "tanh = Hyperbolic tangent - smooth, outputs -1 to 1, good for normalized data. "
            "logistic = Sigmoid (0 to 1) - smooth, slower than relu. "
            "identity = Linear (no nonlinearity) - rarely useful. relu recommended for most cases."
        ),
        'mlp_solver': (
            "Optimization algorithm for training the neural network weights. "
            "adam = Adaptive learning rate optimizer - robust, fast, handles sparse gradients well (recommended). "
            "lbfgs = Quasi-Newton optimizer - good for small datasets, can converge faster. "
            "sgd = Stochastic Gradient Descent - requires careful learning_rate tuning. "
            "adam is best choice for spectral data in most cases."
        ),
        'mlp_alpha': (
            "L2 regularization parameter that penalizes large weights to prevent overfitting. "
            "Smaller values (0.0001-0.001) = less regularization (risk overfitting). "
            "Moderate values (0.01-0.1) = balanced regularization. "
            "Larger values (1-10) = strong regularization (may underfit). "
            "Optimal value depends on dataset size and complexity - tune via cross-validation."
        ),

        # SVR (need to check what SVR hyperparameters exist in the code)
        'svr_C': (
            "Regularization parameter controlling the tradeoff between smooth decision boundary and "
            "classifying training points correctly. Smaller C = more regularization (smoother, more "
            "errors allowed). Larger C = less regularization (fit training data closely, may overfit). "
            "Typical range: 0.1-100. Often tuned on log scale: 0.1, 1, 10, 100."
        ),
        'svr_kernel': (
            "Kernel function transforming data to higher dimensions. "
            "linear = Linear kernel (no transformation) - fast, good for linearly separable data. "
            "rbf = Radial Basis Function - captures nonlinear patterns, most versatile, default choice. "
            "poly = Polynomial - specific nonlinear patterns, less common. "
            "sigmoid = Similar to neural network activation. rbf recommended for spectral data."
        ),
        'svr_gamma': (
            "Kernel coefficient for rbf, poly, and sigmoid kernels. Defines how far the influence of "
            "a single training example reaches. Small gamma = far reach (smooth, may underfit). "
            "Large gamma = close reach (complex decision boundary, may overfit). "
            "scale = 1/(n_features × X.var()) - good default. auto = 1/n_features. Tune via cross-validation."
        ),

        # CatBoost
        'catboost_l2_leaf_reg': (
            "L2 regularization coefficient for leaf values (also called lambda). Controls overfitting. "
            "1.0 = light regularization. 3.0 = default (balanced). 10-30 = strong regularization. "
            "Higher values create smoother models that generalize better but may underfit. "
            "CatBoost is relatively robust to this parameter."
        ),
        'catboost_border_count': (
            "Number of splits for numerical features (spectral wavelengths). More splits = more precision "
            "in finding optimal split points but slower training. "
            "32-64 = fast but less precise. 128 = balanced. 254 = maximum precision (default, recommended). "
            "For spectroscopy with continuous wavelength data, higher values (128-254) are better."
        ),
        'catboost_bagging_temperature': (
            "Controls intensity of Bayesian bootstrap bagging. Adds randomness to training. "
            "0.0 = no bagging (deterministic). 1.0 = default (balanced randomness). "
            "3-10 = aggressive bagging (more diversity). Higher values make each tree more different, "
            "increasing ensemble diversity but potentially reducing individual tree accuracy."
        ),
        'catboost_random_strength': (
            "Amount of randomness to use for scoring splits. Adds randomness when choosing split points. "
            "0.0 = deterministic (no randomness). 1.0 = default (balanced). 2-5 = more randomness. "
            "Higher values prevent overfitting by making split selection less greedy. "
            "Helps with noisy spectral data by avoiding overly specific patterns."
        ),
    },

    # ===== RANKING PENALTIES =====
    'ranking': {
        'variable_penalty': (
            "Controls how much using many wavelengths affects model ranking. Uses cubic scaling "
            "for gentle impact at low values (exploration-friendly). "
            "0 = ignore variable count, rank only by performance (R² or Accuracy). "
            "2 = minimal penalty (~1% impact for using all wavelengths). "
            "5 = balanced penalty favoring parsimony without dominating performance. "
            "10 = strong preference for fewer wavelengths. "
            "Recommended: 2 for exploration, 5-7 for deployment model selection."
        ),
        'complexity_penalty': (
            "Controls how much model complexity (latent variables, tree depth) affects ranking. "
            "Uses cubic scaling for gentle impact at low values. "
            "0 = ignore complexity, rank only by performance. "
            "2 = minimal penalty for complex models. "
            "5 = balanced penalty preferring simpler models when performance is similar. "
            "10 = strong preference for simple, interpretable models. "
            "For PLS: penalizes many components. For trees: penalizes depth/estimators. "
            "Recommended: 2 for exploration, 5-7 for interpretable models."
        ),
    },

    # ===== CALIBRATION TRANSFER METHODS =====
    'calibration_transfer': {
        # Transfer Methods
        'method_DS': (
            "Direct Standardization (DS) is a simple pairwise calibration transfer method. "
            "Builds a linear transformation matrix F that directly maps slave spectra to master spectra: "
            "X_master ≈ X_slave × F. Fast and straightforward, works well when master and slave "
            "instruments have similar wavelength grids. Best for simple spectral differences. "
            "Requires paired samples measured on both instruments. Lambda parameter controls regularization."
        ),
        'method_PDS': (
            "Piecewise Direct Standardization (PDS) is a local version of DS that models each master "
            "wavelength independently using a sliding window of neighboring slave wavelengths. "
            "More flexible than global DS, better at handling nonlinear wavelength dependencies. "
            "Window size controls how many neighboring wavelengths are used (typical: 7-15). "
            "Larger windows = smoother transfer but may miss local spectral features. "
            "Good for instruments with slight wavelength misalignments."
        ),
        'method_TSR': (
            "Transfer by Sample Regression (TSR) selects a subset of representative transfer samples "
            "that span the spectral space, then uses only these samples to build the transformation. "
            "More efficient than using all transfer samples, reduces overfitting. Sample selection uses "
            "Kennard-Stone algorithm to ensure good coverage of spectral diversity. "
            "Number of samples controls subset size (typical: 10-30). Fewer = faster but may miss patterns, "
            "more = comprehensive but slower. Robust choice for heterogeneous sample sets."
        ),
        'method_CTAI': (
            "Calibration Transfer via Adaptive Integration (CTAI) combines spectral standardization "
            "with adaptive selection of informative wavelengths. Uses an iterative algorithm to identify "
            "and weight wavelengths that transfer well between instruments while down-weighting problematic "
            "regions (e.g., noise, nonlinear response). More sophisticated than DS/PDS, adapts to "
            "instrument-specific characteristics. Recommended when instruments have different noise profiles "
            "or response characteristics. Generally provides robust transfer with minimal tuning."
        ),
        'method_NSPFCE': (
            "Null-Space Projection followed by Feature Correlation Enhancement (NS-PFCE) is an advanced "
            "method that removes instrument-specific variance while preserving chemical information. "
            "Uses wavelength selection algorithms (VCPA-IRIV, CARS, SPA) to identify informative features, "
            "then projects out instrument-specific interference using null-space operations. "
            "Excellent for complex scenarios with significant instrumental differences (e.g., different "
            "detectors, optical configurations). Slower than other methods but very effective. "
            "Wavelength selection is critical for performance - VCPA-IRIV recommended for most cases."
        ),
        'method_JYPLS': (
            "Joint-Y Partial Least Squares Inverse (JYPLS-inv) uses PLS regression to model the "
            "master-slave relationship, treating master spectra as 'Y' and slave spectra as 'X'. "
            "The PLS model learns latent variables capturing the systematic spectral differences. "
            "Number of components controls model complexity (typical: 3-15, or 'Auto' for cross-validation). "
            "More flexible than DS for nonlinear relationships, but requires more transfer samples (30+). "
            "Sample selection uses Kennard-Stone for representativeness. "
            "Good when spectral differences are complex but systematic."
        ),

        # Transfer Parameters
        'param_ds_lambda': (
            "Regularization strength for Direct Standardization (DS). Controls the bias-variance tradeoff "
            "in the transformation matrix. Smaller values (0.0001-0.001) = less regularization, fits "
            "transfer samples more closely (may overfit). Larger values (0.01-0.1) = more regularization, "
            "smoother transformation (may underfit). Default 0.001 works well for most cases. "
            "Increase if transfer performance is poor on new samples (overfitting). "
            "Decrease if transfer doesn't correct spectral differences enough (underfitting)."
        ),
        'param_pds_window': (
            "Window size for Piecewise Direct Standardization (PDS). Defines how many neighboring slave "
            "wavelengths are used to predict each master wavelength. Must be odd number. "
            "Small windows (5-9) = more local, captures fine spectral details, may be noisy. "
            "Medium windows (11-15) = balanced, recommended for most applications (default: 11). "
            "Large windows (17-25) = more global, smoother, approaches regular DS. "
            "Match to your spectral resolution: higher resolution allows smaller windows."
        ),
        'param_tsr_samples': (
            "Number of representative samples selected for Transfer by Sample Regression (TSR). "
            "Subset selected using Kennard-Stone algorithm to span spectral diversity. "
            "Fewer samples (8-12) = faster, simpler model, may miss spectral patterns (default: 12). "
            "More samples (20-30) = more comprehensive, better coverage, slower. "
            "Rule of thumb: 10-20% of total transfer samples, minimum 10. "
            "Increase if transfer fails to capture sample diversity. Decrease if overfitting occurs."
        ),
        'param_jypls_samples': (
            "Number of representative samples selected for JYPLS-inv calibration transfer. "
            "Uses Kennard-Stone algorithm to select diverse subset from full transfer set. "
            "Typical range: 10-30 samples (default: 12). More samples = better coverage but slower. "
            "Fewer samples = faster but may miss important spectral variations. "
            "Should be at least 2-3× the number of PLS components used. "
            "Increase for heterogeneous sample sets or complex spectral differences."
        ),
        'param_jypls_components': (
            "Number of PLS latent variables for JYPLS-inv transfer model. Components capture systematic "
            "spectral differences between master and slave instruments. "
            "'Auto' = automatic optimization via cross-validation (recommended, slower). "
            "3-5 = simple spectral differences (baseline, scale). "
            "8-12 = moderate complexity (wavelength shifts, nonlinearities). "
            "15-20 = complex instrumental differences (different detectors, optics). "
            "Too few = underfitting (incomplete transfer). Too many = overfitting (noise transfer). "
            "Start with 'Auto' or 5-8 for typical applications."
        ),
        'param_nspfce_max_iter': (
            "Maximum iterations for NS-PFCE optimization algorithm. Controls convergence of the iterative "
            "null-space projection process. More iterations allow finding better projection but take longer. "
            "50-100 = fast, usually sufficient for simple cases (default: 100). "
            "200-500 = thorough optimization for complex instrumental differences. "
            "Algorithm may converge early (before max iterations) if tolerance is met. "
            "Increase if transfer quality is poor and you suspect incomplete convergence. "
            "Monitor convergence messages - if hitting max iterations, consider increasing."
        ),
        'param_nspfce_wavelength_selection': (
            "Enable wavelength selection for NS-PFCE to identify informative features. "
            "When checked: Uses feature selection algorithm (VCPA-IRIV, CARS, or SPA) to find wavelengths "
            "that transfer reliably between instruments while excluding problematic regions (noise, artifacts). "
            "Recommended: CHECKED for most applications. Dramatically improves transfer quality by focusing "
            "on stable, informative spectral regions. Unchecked = use all wavelengths (faster but may include "
            "noisy regions that degrade transfer). Disable only if you've pre-selected optimal wavelengths "
            "or working with very clean, well-matched instruments."
        ),
        'param_nspfce_selector': (
            "Wavelength selection algorithm for NS-PFCE method. Chooses which features transfer reliably. "
            "vcpa-iriv = Variable Combination Population Analysis + Iteratively Retaining Informative Variables. "
            "Most comprehensive, identifies stable informative wavelengths, recommended for spectroscopy (default). "
            "Slower but most robust. "
            "cars = Competitive Adaptive Reweighted Sampling. Fast, uses competitive mechanism. Good for "
            "large datasets where speed matters. "
            "spa = Successive Projections Algorithm. Very fast, projects orthogonal variables. Good for "
            "highly collinear spectral data. Less thorough than VCPA-IRIV. "
            "Default vcpa-iriv works well for most spectroscopy applications."
        ),
    }
}


class SpectralPredictApp:
    """Main application window with 6-tab design."""

    def __init__(self, root):
        self.root = root
        self.root.title("ASP - Advanced Spectral Prediction (OPTIMIZED)")

        # Set window size - use zoomed/maximized for better visibility
        try:
            self.root.state('zoomed')  # Windows/Linux
        except:
            # Fallback for systems that don't support 'zoomed'
            screen_width = self.root.winfo_screenwidth()
            screen_height = self.root.winfo_screenheight()
            window_width = int(screen_width * 0.85)
            window_height = int(screen_height * 0.85)
            self.root.geometry(f"{window_width}x{window_height}")

        # Configure modern theme
        self._configure_style()

        # Data variables
        self.X = None  # Spectral data (filtered by wavelength)
        self.X_original = None  # Original unfiltered spectral data
        self.y = None  # Target data
        self.ref = None  # Reference dataframe
        self.label_encoder = None  # Label encoder for classification with text labels

        # Results storage
        self.results_df = None  # Analysis results dataframe
        self.selected_model_config = None  # Selected model configuration for refinement
        self.results_sort_column = None  # Current column being sorted
        self.results_sort_reverse = False  # Sort direction (False = ascending, True = descending)

        # Refined/saved model storage (for model persistence)
        self.refined_model = None  # Fitted model from Model Development tab
        self.refined_preprocessor = None  # Fitted preprocessing pipeline
        self.refined_performance = None  # Performance metrics dict (R2, RMSE, etc.)
        self.refined_wavelengths = None  # List of wavelengths used in refined model (subset for derivative+subset)
        self.refined_full_wavelengths = None  # List of ALL wavelengths (for derivative+subset preprocessing)
        self.refined_config = None  # Configuration dict for refined model
        self.refined_y_proba = None  # Prediction probabilities for classification
        self.refined_cv_indices = None  # CV sample indices for mapping predictions back to specimen IDs
        self.refined_label_encoder = None  # Label encoder for categorical targets (classification)
        self.task_type_detection_label = None  # Will be created in Import tab

        # Data Management Tab (Tab 0) variables
        self.data_source_manager = DataSourceManager() if HAS_DATA_MANAGEMENT else None
        self.data_sources_tree = None  # TreeView for data sources
        self.current_active_dataset = None  # Currently active dataset for analysis
        self.merge_strategy_var = tk.StringVar(value='intersection')
        self.duplicate_handling_var = tk.StringVar(value='error')
        self.filter_type_var = tk.StringVar(value='regex')
        self.filter_column_var = tk.StringVar()
        self.filter_value_var = tk.StringVar()
        self.min_wavelength_var = tk.DoubleVar()
        self.max_wavelength_var = tk.DoubleVar()

        # Model Prediction Tab (Tab 8) variables - shifted from Tab 7
        self.loaded_models = []  # List of model dicts from load_model()
        self.prediction_data = None  # DataFrame with new spectral data
        self.predictions_df = None  # Results dataframe
        self.predictions_model_map = {}  # Map column names to model metadata
        self.consensus_info = {}  # Store consensus model details for display

        # Instrument Lab Tab (Tab 9) variables - REMOVED (functionality moved to Calibration Transfer)
        # self.instrument_profiles = {}  # Dict of instrument_id -> InstrumentProfile
        # self.current_instrument_data = None  # (wavelengths, spectra) for current instrument
        # self.instrument_spectral_data = {}  # Dict of instrument_id -> (wavelengths, X) - persistent storage

        # Track accent buttons for theme switching
        self.accent_buttons = []  # List of accent buttons created

        # Calibration Transfer Tab (Tab 10) variables - Redesigned wizard interface
        # Section A: Transfer Model
        self.current_transfer_model = None  # TransferModel object (loaded or built)
        self.current_master_data = None  # (wavelengths, X) tuple for building model
        self.current_slave_data = None  # (wavelengths, X) tuple for building model
        self.master_data_format = None  # 'csv', 'npy', 'folder', etc.
        self.slave_data_format = None  # 'csv', 'npy', 'folder', etc.

        # Enhanced loading with Y values (for JYPLS-inv)
        # Master data with Y values
        self.ct_master_X = None  # pd.DataFrame with sample IDs as index
        self.ct_master_y = None  # pd.Series with sample IDs as index
        self.ct_master_wavelengths = None  # np.ndarray of wavelengths
        self.ct_master_detected_type = None  # 'asd', 'csv', 'spc', etc.
        self.ct_master_spectra_path_var = tk.StringVar()  # Path to master spectra directory
        self.ct_master_reference_path_var = tk.StringVar()  # Path to master reference CSV
        self.ct_master_spectral_file_col_var = tk.StringVar()  # Spectral file column name
        self.ct_master_id_col_var = tk.StringVar()  # Specimen ID column name
        self.ct_master_target_col_var = tk.StringVar()  # Target variable column name

        # Slave data with Y values
        self.ct_slave_X = None  # pd.DataFrame with sample IDs as index
        self.ct_slave_y = None  # pd.Series with sample IDs as index
        self.ct_slave_wavelengths = None  # np.ndarray of wavelengths
        self.ct_slave_detected_type = None  # 'asd', 'csv', 'spc', etc.
        self.ct_slave_spectra_path_var = tk.StringVar()  # Path to slave spectra directory
        self.ct_slave_reference_path_var = tk.StringVar()  # Path to slave reference CSV
        self.ct_slave_spectral_file_col_var = tk.StringVar()  # Spectral file column name
        self.ct_slave_id_col_var = tk.StringVar()  # Specimen ID column name
        self.ct_slave_target_col_var = tk.StringVar()  # Target variable column name

        # Section B: Application Mode
        self.application_mode = None  # 'predict' or 'export'

        # Section C: Prediction Workflow (Mode A)
        self.current_prediction_model = None  # sklearn model for predictions
        self.new_slave_data_predict = None  # New slave data to transform and predict

        # Section D: Export Workflow (Mode B)
        self.new_slave_data_export = None  # New slave data to transform and export
        self.transformed_spectra = None  # Transformed spectra ready for export

        # Legacy variables (kept for backward compatibility with existing methods)
        self.ct_master_model_dict = None  # Loaded master model for predictions
        self.ct_X_master_common = None  # Master spectra on common grid
        self.ct_X_slave_common = None  # Slave spectra on common grid
        self.ct_wavelengths_common = None  # Common wavelength grid
        self.ct_transfer_model = None  # Current TransferModel object
        self.ct_master_instrument_id = tk.StringVar()  # Selected master instrument
        self.ct_slave_instrument_id = tk.StringVar()  # Selected slave instrument
        self.ct_pred_transfer_model = None  # Transfer model loaded for prediction
        self.ct_pred_y_pred = None  # Predictions from transferred spectra
        self.ct_pred_sample_ids = None  # Sample IDs for predictions
        self.ct_multiinstrument_data = None  # Multi-instrument dataset for equalization
        self.ct_equalized_wavelengths = None  # Wavelengths after equalization
        self.ct_equalized_X = None  # Equalized spectra
        self.ct_equalized_sample_ids = None  # Sample IDs with instrument prefixes

        # Transfer Model Registry - persistent storage of built transfer models
        self.transfer_model_registry = {}  # Dict of model_key -> TransferModel
        # model_key format: "MasterID_SlaveID_Method"

        # GUI variables
        self.spectral_data_path = tk.StringVar()  # Unified path for spectral data
        self.detected_type = None  # Auto-detected type: "asd", "csv", "spc", "combined", or "combined_excel"
        self.combined_file_path = None  # Path to combined CSV/TXT/Excel file (if detected_type == "combined" or "combined_excel")
        self.combined_sheet_name = None  # Sheet name for Excel combined format
        self.combined_metadata = None  # Metadata from combined file parsing
        self.reference_file = tk.StringVar()
        self.spectral_file_column = tk.StringVar()
        self.id_column = tk.StringVar()
        self.target_column = tk.StringVar()
        self.wavelength_min = tk.StringVar(value="")  # Import filter
        self.wavelength_max = tk.StringVar(value="")  # Import filter

        # Analysis wavelength restriction (further filters for model training only)
        self.enable_analysis_wl_restriction = tk.BooleanVar(value=False)
        self.analysis_wl_min = tk.StringVar(value="")
        self.analysis_wl_max = tk.StringVar(value="")

        # Analysis variables
        self.output_dir = tk.StringVar(value="outputs")
        self.folds = tk.IntVar(value=5)
        self.task_type = tk.StringVar(value="auto")  # auto, regression, or classification
        self.task_type.trace_add('write', lambda *args: self._on_task_type_changed())

        # NEW: User-friendly penalty system (0-10 scale)
        # 0 = only performance (R²) matters, 10 = strong penalty
        self.variable_penalty = tk.IntVar(value=0)     # Penalty for using many variables
        self.complexity_penalty = tk.IntVar(value=0)   # Penalty for model complexity

        self.max_n_components = tk.IntVar(value=8)
        self.max_iter = tk.IntVar(value=100)  # OPTIMIZED: Reduced from 500 to 100 (Phase A)
        self.show_progress = tk.BooleanVar(value=True)

        # Reflectance/Absorbance tracking and conversion
        self.use_absorbance = tk.BooleanVar(value=False)  # Legacy - for conversion display
        self.original_data_type = tk.StringVar(value="reflectance")  # Detected from file
        self.current_data_type = tk.StringVar(value="reflectance")  # Current displayed type
        self.type_confidence = 0.0  # Detection confidence score
        self.type_detection_method = ""  # Method used for detection
        self.data_has_been_converted = False  # Track if conversion occurred

        # Spectrum exclusion tracking
        self.excluded_spectra = set()  # Set of indices of excluded spectra

        # Interactive plot annotation tracking
        self.plot_annotations = {}  # Dict of {canvas_id: annotation_object} for click-to-show info
        self.active_annotation = None  # Currently visible annotation object

        # Plot coloring variables
        self.pca_color_var = tk.StringVar(value='Y Value')
        self.regression_pred_color_var = tk.StringVar(value='Y Value')
        self.pred_results_color_var = tk.StringVar(value='Y Value')
        self.residual_color_var = tk.StringVar(value='Y Value')

        # Validation set tracking
        self.validation_enabled = tk.BooleanVar(value=False)
        self.validation_percentage = tk.DoubleVar(value=20.0)
        self.validation_algorithm = tk.StringVar(value="SPXY")
        self.validation_indices = set()  # Sample indices in validation set
        self.validation_X = None  # Stored validation spectral data
        self.validation_y = None  # Stored validation target data

        # Tier selection
        self.model_tier = tk.StringVar(value="quick")  # quick, standard, comprehensive, experimental, custom
        self._updating_from_tier = False  # Flag to prevent infinite loops when updating checkboxes

        # Model selection (original models)
        # Standard tier models (enabled by default)
        self.use_pls = tk.BooleanVar(value=True)
        self.use_plsda = tk.BooleanVar(value=True)         # PLS-DA for classification
        self.use_ridge = tk.BooleanVar(value=True)
        self.use_lasso = tk.BooleanVar(value=True)
        self.use_elasticnet = tk.BooleanVar(value=True)  # Moved to standard tier
        self.use_randomforest = tk.BooleanVar(value=True)
        self.use_lightgbm = tk.BooleanVar(value=True)    # Moved to standard tier

        # Comprehensive/experimental tier models (disabled by default)
        self.use_xgboost = tk.BooleanVar(value=False)      # Comprehensive (too slow for standard)
        self.use_catboost = tk.BooleanVar(value=False)     # Comprehensive
        self.use_neuralboosted = tk.BooleanVar(value=False)  # Comprehensive
        self.use_svr = tk.BooleanVar(value=False)          # Experimental (very slow) - for regression
        self.use_svm = tk.BooleanVar(value=False)          # Experimental (very slow) - for classification
        self.use_mlp = tk.BooleanVar(value=False)          # Experimental

        # Create model name to checkbox mapping
        self.model_checkboxes = {
            'PLS': self.use_pls,
            'PLS-DA': self.use_plsda,
            'Ridge': self.use_ridge,
            'Lasso': self.use_lasso,
            'ElasticNet': self.use_elasticnet,
            'RandomForest': self.use_randomforest,
            'XGBoost': self.use_xgboost,
            'LightGBM': self.use_lightgbm,
            'CatBoost': self.use_catboost,
            'SVR': self.use_svr,
            'SVM': self.use_svm,
            'MLP': self.use_mlp,
            'NeuralBoosted': self.use_neuralboosted
        }

        # Add trace callbacks to model checkboxes to switch to Custom tier when manually changed
        for var in self.model_checkboxes.values():
            var.trace_add('write', self._on_model_checkbox_changed)

        # Preprocessing method selection
        self.use_raw = tk.BooleanVar(value=False)
        self.use_snv = tk.BooleanVar(value=True)
        self.use_sg1 = tk.BooleanVar(value=True)  # 1st derivative
        self.use_sg2 = tk.BooleanVar(value=True)  # 2nd derivative
        self.use_deriv_snv = tk.BooleanVar(value=True)  # deriv_snv (derivative then SNV)

        # Subset Analysis options
        self.enable_variable_subsets = tk.BooleanVar(value=True)  # Top-N variable analysis
        self.enable_region_subsets = tk.BooleanVar(value=True)  # Spectral region analysis
        self.n_top_regions = tk.IntVar(value=5)  # Number of top regions to analyze (5, 10, 15, 20)

        # Top-N variable counts (checkboxes for each)
        self.var_10 = tk.BooleanVar(value=True)
        self.var_20 = tk.BooleanVar(value=True)
        self.var_50 = tk.BooleanVar(value=True)
        self.var_100 = tk.BooleanVar(value=True)
        self.var_250 = tk.BooleanVar(value=True)
        self.var_500 = tk.BooleanVar(value=False)
        self.var_1000 = tk.BooleanVar(value=False)

        # Window size selections (default: only 17 checked)
        self.window_7 = tk.BooleanVar(value=False)
        self.window_11 = tk.BooleanVar(value=False)
        self.window_17 = tk.BooleanVar(value=True)
        self.window_19 = tk.BooleanVar(value=False)
        self.window_custom = tk.StringVar(value="")  # Custom window size

        # Advanced model options (NeuralBoosted)
        self.n_estimators_50 = tk.BooleanVar(value=False)
        self.n_estimators_100 = tk.BooleanVar(value=True)  # Default
        self.n_estimators_custom = tk.StringVar(value="")  # Custom value
        self.lr_005 = tk.BooleanVar(value=False)
        self.lr_01 = tk.BooleanVar(value=True)  # Default
        self.lr_02 = tk.BooleanVar(value=True)  # Default
        self.lr_03 = tk.BooleanVar(value=True)  # Default - OPTIMAL per empirical analysis

        # NeuralBoosted hidden_layer_size (MLP hidden layer neurons)
        self.neuralboosted_hidden_3 = tk.BooleanVar(value=True)   # 3 ⭐ standard
        self.neuralboosted_hidden_5 = tk.BooleanVar(value=True)   # 5 ⭐ standard
        self.neuralboosted_hidden_custom = tk.StringVar(value="")

        # NeuralBoosted activation function
        self.neuralboosted_activation_tanh = tk.BooleanVar(value=True)      # tanh ⭐ standard
        self.neuralboosted_activation_identity = tk.BooleanVar(value=True)  # identity ⭐ standard
        self.neuralboosted_activation_relu = tk.BooleanVar(value=False)     # relu
        self.neuralboosted_activation_logistic = tk.BooleanVar(value=False) # logistic

        # Random Forest options
        self.rf_n_trees_100 = tk.BooleanVar(value=True)  # Default
        self.rf_n_trees_200 = tk.BooleanVar(value=False)
        self.rf_n_trees_500 = tk.BooleanVar(value=False)
        self.rf_n_trees_custom = tk.StringVar(value="")  # Custom value
        self.rf_max_depth_none = tk.BooleanVar(value=True)   # Default: unlimited depth
        self.rf_max_depth_30 = tk.BooleanVar(value=True)     # Default: max_depth=30
        self.rf_max_depth_custom = tk.StringVar(value="")    # Custom max_depth value

        # RandomForest min_samples_split (minimum samples required to split a node)
        self.rf_min_samples_split_2 = tk.BooleanVar(value=True)  # 2 ⭐ standard (models.py default)
        self.rf_min_samples_split_5 = tk.BooleanVar(value=False)  # 5
        self.rf_min_samples_split_10 = tk.BooleanVar(value=False)  # 10
        self.rf_min_samples_split_20 = tk.BooleanVar(value=False)  # 20
        self.rf_min_samples_split_custom = tk.StringVar(value="")  # Custom value

        # RandomForest min_samples_leaf (minimum samples required at a leaf node)
        self.rf_min_samples_leaf_1 = tk.BooleanVar(value=True)  # 1 ⭐ standard (models.py default)
        self.rf_min_samples_leaf_2 = tk.BooleanVar(value=False)  # 2
        self.rf_min_samples_leaf_5 = tk.BooleanVar(value=False)  # 5
        self.rf_min_samples_leaf_10 = tk.BooleanVar(value=False)  # 10
        self.rf_min_samples_leaf_custom = tk.StringVar(value="")  # Custom value

        # RandomForest max_features (number of features to consider for splits)
        self.rf_max_features_sqrt = tk.BooleanVar(value=True)  # 'sqrt' ⭐ standard (models.py default)
        self.rf_max_features_log2 = tk.BooleanVar(value=False)  # 'log2'
        self.rf_max_features_none = tk.BooleanVar(value=False)  # None (all features)
        self.rf_max_features_custom = tk.StringVar(value="")  # Custom value (float 0-1 or int)

        # RandomForest bootstrap (whether to use bootstrap samples)
        self.rf_bootstrap_true = tk.BooleanVar(value=True)  # True ⭐ standard (models.py default)
        self.rf_bootstrap_false = tk.BooleanVar(value=False)  # False (use all samples)

        # RandomForest max_leaf_nodes (maximum number of leaf nodes)
        self.rf_max_leaf_nodes_none = tk.BooleanVar(value=True)  # None ⭐ standard (unlimited, models.py default)
        self.rf_max_leaf_nodes_50 = tk.BooleanVar(value=False)  # 50
        self.rf_max_leaf_nodes_100 = tk.BooleanVar(value=False)  # 100
        self.rf_max_leaf_nodes_custom = tk.StringVar(value="")  # Custom value

        # RandomForest min_impurity_decrease (minimum impurity decrease to split)
        self.rf_min_impurity_decrease_0 = tk.BooleanVar(value=True)  # 0.0 ⭐ standard (no threshold, models.py default)
        self.rf_min_impurity_decrease_001 = tk.BooleanVar(value=False)  # 0.01
        self.rf_min_impurity_decrease_01 = tk.BooleanVar(value=False)  # 0.1
        self.rf_min_impurity_decrease_custom = tk.StringVar(value="")  # Custom value

        # Ridge Regression alpha options (default: all checked per models.py)
        self.ridge_alpha_0001 = tk.BooleanVar(value=True)   # 0.001
        self.ridge_alpha_001 = tk.BooleanVar(value=True)    # 0.01
        self.ridge_alpha_01 = tk.BooleanVar(value=True)     # 0.1
        self.ridge_alpha_1 = tk.BooleanVar(value=True)      # 1.0
        self.ridge_alpha_10 = tk.BooleanVar(value=True)     # 10.0
        self.ridge_alpha_custom = tk.StringVar(value="")    # Custom value

        # Ridge solver (optimization algorithm)
        self.ridge_solver_auto = tk.BooleanVar(value=True)  # 'auto' ⭐ standard (models.py default)
        self.ridge_solver_svd = tk.BooleanVar(value=False)  # 'svd' (exact solution)
        self.ridge_solver_cholesky = tk.BooleanVar(value=False)  # 'cholesky' (fast for n_samples > n_features)
        self.ridge_solver_lsqr = tk.BooleanVar(value=False)  # 'lsqr' (iterative, good for large datasets)
        self.ridge_solver_sag = tk.BooleanVar(value=False)  # 'sag' (stochastic average gradient)

        # Ridge tol (tolerance for stopping criterion)
        self.ridge_tol_1e4 = tk.BooleanVar(value=True)  # 1e-4 ⭐ standard (models.py default)
        self.ridge_tol_1e3 = tk.BooleanVar(value=False)  # 1e-3
        self.ridge_tol_1e5 = tk.BooleanVar(value=False)  # 1e-5
        self.ridge_tol_custom = tk.StringVar(value="")  # Custom value

        # Lasso Regression alpha options (default: all checked per models.py)
        self.lasso_alpha_0001 = tk.BooleanVar(value=True)   # 0.001
        self.lasso_alpha_001 = tk.BooleanVar(value=True)    # 0.01
        self.lasso_alpha_01 = tk.BooleanVar(value=True)     # 0.1
        self.lasso_alpha_1 = tk.BooleanVar(value=True)      # 1.0
        self.lasso_alpha_custom = tk.StringVar(value="")    # Custom value

        # Lasso selection (coordinate descent algorithm variant)
        self.lasso_selection_cyclic = tk.BooleanVar(value=True)  # 'cyclic' ⭐ standard (models.py default)
        self.lasso_selection_random = tk.BooleanVar(value=False)  # 'random' (can be faster for large datasets)

        # Lasso tol (tolerance for stopping criterion)
        self.lasso_tol_1e4 = tk.BooleanVar(value=True)  # 1e-4 ⭐ standard (models.py default)
        self.lasso_tol_1e3 = tk.BooleanVar(value=False)  # 1e-3
        self.lasso_tol_1e5 = tk.BooleanVar(value=False)  # 1e-5
        self.lasso_tol_custom = tk.StringVar(value="")  # Custom value

        # XGBoost Hyperparameters (spectroscopy-optimized defaults)
        # n_estimators (number of boosting rounds)
        self.xgb_n_estimators_100 = tk.BooleanVar(value=True)   # ⭐ standard
        self.xgb_n_estimators_200 = tk.BooleanVar(value=True)   # ⭐ standard
        self.xgb_n_estimators_custom = tk.StringVar(value="")

        # learning_rate (step size shrinkage)
        self.xgb_lr_005 = tk.BooleanVar(value=True)   # 0.05 ⭐ standard
        self.xgb_lr_01 = tk.BooleanVar(value=True)    # 0.1 ⭐ standard
        self.xgb_lr_custom = tk.StringVar(value="")

        # max_depth (maximum tree depth)
        self.xgb_max_depth_3 = tk.BooleanVar(value=True)    # ⭐ standard
        self.xgb_max_depth_6 = tk.BooleanVar(value=True)    # ⭐ standard
        self.xgb_max_depth_9 = tk.BooleanVar(value=False)   # comprehensive
        self.xgb_max_depth_custom = tk.StringVar(value="")

        # subsample (row sampling ratio)
        self.xgb_subsample_08 = tk.BooleanVar(value=True)   # 0.8 ⭐ standard
        self.xgb_subsample_10 = tk.BooleanVar(value=True)   # 1.0 ⭐ standard
        self.xgb_subsample_custom = tk.StringVar(value="")

        # colsample_bytree (column sampling ratio)
        self.xgb_colsample_08 = tk.BooleanVar(value=True)   # 0.8 ⭐ standard
        self.xgb_colsample_10 = tk.BooleanVar(value=True)   # 1.0 ⭐ standard
        self.xgb_colsample_custom = tk.StringVar(value="")

        # reg_alpha (L1 regularization)
        self.xgb_reg_alpha_0 = tk.BooleanVar(value=True)     # 0 ⭐ standard
        self.xgb_reg_alpha_01 = tk.BooleanVar(value=True)    # 0.1 ⭐ standard
        self.xgb_reg_alpha_05 = tk.BooleanVar(value=False)   # 0.5 comprehensive
        self.xgb_reg_alpha_custom = tk.StringVar(value="")

        # reg_lambda (L2 regularization) - comprehensive tier
        self.xgb_reg_lambda_10 = tk.BooleanVar(value=False)  # 1.0 comprehensive
        self.xgb_reg_lambda_50 = tk.BooleanVar(value=False)  # 5.0 comprehensive
        self.xgb_reg_lambda_custom = tk.StringVar(value="")

        # min_child_weight (minimum sum of instance weight in child)
        self.xgb_min_child_weight_1 = tk.BooleanVar(value=True)  # 1 ⭐ standard
        self.xgb_min_child_weight_3 = tk.BooleanVar(value=False)  # 3 comprehensive
        self.xgb_min_child_weight_5 = tk.BooleanVar(value=False)  # 5 comprehensive
        self.xgb_min_child_weight_7 = tk.BooleanVar(value=False)  # 7 comprehensive
        self.xgb_min_child_weight_custom = tk.StringVar(value="")

        # gamma (minimum loss reduction for split)
        self.xgb_gamma_0 = tk.BooleanVar(value=True)  # 0 ⭐ standard
        self.xgb_gamma_01 = tk.BooleanVar(value=False)  # 0.1 comprehensive
        self.xgb_gamma_03 = tk.BooleanVar(value=False)  # 0.3 comprehensive
        self.xgb_gamma_05 = tk.BooleanVar(value=False)  # 0.5 comprehensive
        self.xgb_gamma_10 = tk.BooleanVar(value=False)  # 1.0 comprehensive
        self.xgb_gamma_custom = tk.StringVar(value="")

        # ElasticNet Hyperparameters
        # alpha (regularization strength)
        self.elasticnet_alpha_001 = tk.BooleanVar(value=True)   # 0.01 ⭐ standard
        self.elasticnet_alpha_01 = tk.BooleanVar(value=True)    # 0.1 ⭐ standard
        self.elasticnet_alpha_10 = tk.BooleanVar(value=True)    # 1.0 ⭐ standard
        self.elasticnet_alpha_custom = tk.StringVar(value="")

        # l1_ratio (L1 vs L2 mix: 0=Ridge, 1=Lasso)
        self.elasticnet_l1_ratio_03 = tk.BooleanVar(value=True)  # 0.3 ⭐ standard
        self.elasticnet_l1_ratio_05 = tk.BooleanVar(value=True)  # 0.5 ⭐ standard (balanced)
        self.elasticnet_l1_ratio_07 = tk.BooleanVar(value=True)  # 0.7 ⭐ standard
        self.elasticnet_l1_ratio_custom = tk.StringVar(value="")

        # ElasticNet selection (coordinate descent algorithm variant)
        self.elasticnet_selection_cyclic = tk.BooleanVar(value=True)  # 'cyclic' ⭐ standard (models.py default)
        self.elasticnet_selection_random = tk.BooleanVar(value=False)  # 'random' (can be faster for large datasets)

        # ElasticNet tol (tolerance for stopping criterion)
        self.elasticnet_tol_1e4 = tk.BooleanVar(value=True)  # 1e-4 ⭐ standard (models.py default)
        self.elasticnet_tol_1e3 = tk.BooleanVar(value=False)  # 1e-3
        self.elasticnet_tol_1e5 = tk.BooleanVar(value=False)  # 1e-5
        self.elasticnet_tol_custom = tk.StringVar(value="")  # Custom value

        # PLS Hyperparameters
        # max_iter (maximum number of iterations)
        self.pls_max_iter_500 = tk.BooleanVar(value=True)   # 500 ⭐ standard (default)
        self.pls_max_iter_1000 = tk.BooleanVar(value=False)  # 1000 comprehensive
        self.pls_max_iter_custom = tk.StringVar(value="")

        # tol (convergence tolerance)
        self.pls_tol_1e7 = tk.BooleanVar(value=False)  # 1e-7 comprehensive
        self.pls_tol_1e6 = tk.BooleanVar(value=True)   # 1e-6 ⭐ standard (default)
        self.pls_tol_1e5 = tk.BooleanVar(value=False)  # 1e-5 comprehensive
        self.pls_tol_custom = tk.StringVar(value="")

        # LightGBM Hyperparameters
        # n_estimators
        self.lightgbm_n_estimators_50 = tk.BooleanVar(value=False)  # 50 comprehensive
        self.lightgbm_n_estimators_100 = tk.BooleanVar(value=True)  # 100 ⭐ standard
        self.lightgbm_n_estimators_200 = tk.BooleanVar(value=True)  # 200 ⭐ standard
        self.lightgbm_n_estimators_custom = tk.StringVar(value="")

        # learning_rate
        self.lightgbm_lr_005 = tk.BooleanVar(value=False)  # 0.05 comprehensive
        self.lightgbm_lr_01 = tk.BooleanVar(value=True)  # 0.1 ⭐ standard
        self.lightgbm_lr_02 = tk.BooleanVar(value=False)  # 0.2 comprehensive
        self.lightgbm_lr_custom = tk.StringVar(value="")

        # num_leaves (max number of leaves per tree)
        self.lightgbm_num_leaves_31 = tk.BooleanVar(value=True)  # 31 ⭐ standard (default)
        self.lightgbm_num_leaves_50 = tk.BooleanVar(value=True)  # 50 ⭐ standard
        self.lightgbm_num_leaves_70 = tk.BooleanVar(value=False)  # 70 comprehensive
        self.lightgbm_num_leaves_custom = tk.StringVar(value="")

        # max_depth (maximum tree depth, -1 = no limit)
        self.lightgbm_max_depth_m1 = tk.BooleanVar(value=True)  # -1 ⭐ standard (no limit)
        self.lightgbm_max_depth_5 = tk.BooleanVar(value=False)  # 5
        self.lightgbm_max_depth_10 = tk.BooleanVar(value=False)  # 10
        self.lightgbm_max_depth_20 = tk.BooleanVar(value=False)  # 20
        self.lightgbm_max_depth_50 = tk.BooleanVar(value=False)  # 50
        self.lightgbm_max_depth_custom = tk.StringVar(value="")

        # min_child_samples (minimum samples required in a leaf)
        self.lightgbm_min_child_samples_5 = tk.BooleanVar(value=True)  # 5 ⭐ standard (fixed - was 20)
        self.lightgbm_min_child_samples_10 = tk.BooleanVar(value=False)  # 10
        self.lightgbm_min_child_samples_20 = tk.BooleanVar(value=False)  # 20 (too restrictive for small datasets)
        self.lightgbm_min_child_samples_50 = tk.BooleanVar(value=False)  # 50
        self.lightgbm_min_child_samples_100 = tk.BooleanVar(value=False)  # 100
        self.lightgbm_min_child_samples_custom = tk.StringVar(value="")

        # subsample (fraction of data to use for each tree)
        self.lightgbm_subsample_05 = tk.BooleanVar(value=False)  # 0.5
        self.lightgbm_subsample_07 = tk.BooleanVar(value=False)  # 0.7
        self.lightgbm_subsample_08 = tk.BooleanVar(value=True)  # 0.8 ⭐ standard (models.py default)
        self.lightgbm_subsample_085 = tk.BooleanVar(value=False)  # 0.85
        self.lightgbm_subsample_10 = tk.BooleanVar(value=False)  # 1.0 (no subsampling)
        self.lightgbm_subsample_custom = tk.StringVar(value="")

        # colsample_bytree (fraction of features to use per tree)
        self.lightgbm_colsample_bytree_05 = tk.BooleanVar(value=False)  # 0.5
        self.lightgbm_colsample_bytree_07 = tk.BooleanVar(value=False)  # 0.7
        self.lightgbm_colsample_bytree_08 = tk.BooleanVar(value=True)  # 0.8 ⭐ standard (models.py default)
        self.lightgbm_colsample_bytree_085 = tk.BooleanVar(value=False)  # 0.85
        self.lightgbm_colsample_bytree_10 = tk.BooleanVar(value=False)  # 1.0 (all features)
        self.lightgbm_colsample_bytree_custom = tk.StringVar(value="")

        # reg_alpha (L1 regularization)
        self.lightgbm_reg_alpha_00 = tk.BooleanVar(value=False)  # 0.0 (no L1)
        self.lightgbm_reg_alpha_01 = tk.BooleanVar(value=True)  # 0.1 ⭐ standard (models.py default)
        self.lightgbm_reg_alpha_05 = tk.BooleanVar(value=False)  # 0.5
        self.lightgbm_reg_alpha_10 = tk.BooleanVar(value=False)  # 1.0
        self.lightgbm_reg_alpha_custom = tk.StringVar(value="")

        # reg_lambda (L2 regularization)
        self.lightgbm_reg_lambda_00 = tk.BooleanVar(value=False)  # 0.0 (no L2)
        self.lightgbm_reg_lambda_05 = tk.BooleanVar(value=False)  # 0.5
        self.lightgbm_reg_lambda_10 = tk.BooleanVar(value=True)  # 1.0 ⭐ standard (models.py default)
        self.lightgbm_reg_lambda_20 = tk.BooleanVar(value=False)  # 2.0
        self.lightgbm_reg_lambda_custom = tk.StringVar(value="")

        # CatBoost Hyperparameters
        # iterations (equivalent to n_estimators)
        self.catboost_iterations_100 = tk.BooleanVar(value=True)  # ⭐ standard
        self.catboost_iterations_200 = tk.BooleanVar(value=True)  # ⭐ standard
        self.catboost_iterations_custom = tk.StringVar(value="")

        # learning_rate
        self.catboost_lr_005 = tk.BooleanVar(value=True)  # 0.05 ⭐ standard
        self.catboost_lr_01 = tk.BooleanVar(value=True)  # 0.1 ⭐ standard
        self.catboost_lr_custom = tk.StringVar(value="")

        # depth (tree depth)
        self.catboost_depth_4 = tk.BooleanVar(value=True)  # 4 ⭐ standard
        self.catboost_depth_6 = tk.BooleanVar(value=True)  # 6 ⭐ standard
        self.catboost_depth_custom = tk.StringVar(value="")

        # l2_leaf_reg (L2 leaf regularization)
        self.catboost_l2_leaf_reg_10 = tk.BooleanVar(value=False)  # 1.0
        self.catboost_l2_leaf_reg_30 = tk.BooleanVar(value=True)  # 3.0 ⭐ standard
        self.catboost_l2_leaf_reg_100 = tk.BooleanVar(value=False)  # 10.0
        self.catboost_l2_leaf_reg_300 = tk.BooleanVar(value=False)  # 30.0
        self.catboost_l2_leaf_reg_custom = tk.StringVar(value="")

        # border_count (number of splits for numerical features)
        self.catboost_border_count_32 = tk.BooleanVar(value=False)  # 32
        self.catboost_border_count_64 = tk.BooleanVar(value=False)  # 64
        self.catboost_border_count_128 = tk.BooleanVar(value=False)  # 128
        self.catboost_border_count_254 = tk.BooleanVar(value=True)  # 254 ⭐ standard
        self.catboost_border_count_custom = tk.StringVar(value="")

        # bagging_temperature (Bayesian bagging intensity)
        self.catboost_bagging_temperature_00 = tk.BooleanVar(value=False)  # 0.0
        self.catboost_bagging_temperature_05 = tk.BooleanVar(value=False)  # 0.5
        self.catboost_bagging_temperature_10 = tk.BooleanVar(value=True)  # 1.0 ⭐ standard
        self.catboost_bagging_temperature_30 = tk.BooleanVar(value=False)  # 3.0
        self.catboost_bagging_temperature_custom = tk.StringVar(value="")

        # random_strength (randomness for scoring splits)
        self.catboost_random_strength_05 = tk.BooleanVar(value=False)  # 0.5
        self.catboost_random_strength_10 = tk.BooleanVar(value=True)  # 1.0 ⭐ standard
        self.catboost_random_strength_20 = tk.BooleanVar(value=False)  # 2.0
        self.catboost_random_strength_50 = tk.BooleanVar(value=False)  # 5.0
        self.catboost_random_strength_custom = tk.StringVar(value="")

        # SVR Hyperparameters
        # kernel type
        self.svr_kernel_rbf = tk.BooleanVar(value=True)     # RBF ⭐ standard
        self.svr_kernel_linear = tk.BooleanVar(value=True)  # Linear ⭐ standard

        # C (regularization parameter)
        self.svr_C_10 = tk.BooleanVar(value=True)   # 1.0 ⭐ standard
        self.svr_C_100 = tk.BooleanVar(value=True)  # 10.0 ⭐ standard
        self.svr_C_custom = tk.StringVar(value="")

        # gamma (kernel coefficient for RBF)
        self.svr_gamma_scale = tk.BooleanVar(value=True)  # 'scale' ⭐ standard
        self.svr_gamma_auto = tk.BooleanVar(value=False)  # 'auto' comprehensive
        self.svr_gamma_custom = tk.StringVar(value="")

        # epsilon (width of epsilon-insensitive tube)
        self.svr_epsilon_001 = tk.BooleanVar(value=False)  # 0.01
        self.svr_epsilon_005 = tk.BooleanVar(value=False)  # 0.05
        self.svr_epsilon_01 = tk.BooleanVar(value=True)    # 0.1 ⭐ standard
        self.svr_epsilon_02 = tk.BooleanVar(value=False)   # 0.2
        self.svr_epsilon_05 = tk.BooleanVar(value=False)   # 0.5

        # degree (for polynomial kernel)
        self.svr_degree_2 = tk.BooleanVar(value=False)
        self.svr_degree_3 = tk.BooleanVar(value=True)  # 3 ⭐ standard
        self.svr_degree_4 = tk.BooleanVar(value=False)
        self.svr_degree_5 = tk.BooleanVar(value=False)

        # coef0 (independent term in kernel function)
        self.svr_coef0_00 = tk.BooleanVar(value=True)  # 0.0 ⭐ standard
        self.svr_coef0_05 = tk.BooleanVar(value=False)  # 0.5
        self.svr_coef0_10 = tk.BooleanVar(value=False)  # 1.0
        self.svr_coef0_20 = tk.BooleanVar(value=False)  # 2.0

        # shrinking (use shrinking heuristic)
        self.svr_shrinking_true = tk.BooleanVar(value=True)  # True ⭐ standard
        self.svr_shrinking_false = tk.BooleanVar(value=False)  # False

        # MLP Hyperparameters
        # hidden_layer_sizes (neural network architecture)
        self.mlp_hidden_64 = tk.BooleanVar(value=True)      # (64,) ⭐ standard - 1 layer
        self.mlp_hidden_128_64 = tk.BooleanVar(value=True)  # (128,64) ⭐ standard - 2 layers
        self.mlp_hidden_custom = tk.StringVar(value="")      # e.g., "100,50,25"

        # alpha (L2 regularization penalty)
        self.mlp_alpha_1e3 = tk.BooleanVar(value=True)  # 0.001 ⭐ standard
        self.mlp_alpha_custom = tk.StringVar(value="")

        # learning_rate_init (initial learning rate)
        self.mlp_lr_init_1e3 = tk.BooleanVar(value=True)  # 0.001 ⭐ standard
        self.mlp_lr_init_custom = tk.StringVar(value="")

        # activation function
        self.mlp_activation_relu = tk.BooleanVar(value=True)  # relu ⭐ standard
        self.mlp_activation_tanh = tk.BooleanVar(value=False)
        self.mlp_activation_logistic = tk.BooleanVar(value=False)
        self.mlp_activation_identity = tk.BooleanVar(value=False)

        # solver (weight optimization)
        self.mlp_solver_adam = tk.BooleanVar(value=True)  # adam ⭐ standard
        self.mlp_solver_sgd = tk.BooleanVar(value=False)
        self.mlp_solver_lbfgs = tk.BooleanVar(value=False)

        # batch_size
        self.mlp_batch_auto = tk.BooleanVar(value=True)  # 'auto' ⭐ standard
        self.mlp_batch_32 = tk.BooleanVar(value=False)
        self.mlp_batch_64 = tk.BooleanVar(value=False)
        self.mlp_batch_128 = tk.BooleanVar(value=False)
        self.mlp_batch_256 = tk.BooleanVar(value=False)

        # learning_rate_schedule
        self.mlp_lr_schedule_constant = tk.BooleanVar(value=True)  # constant ⭐ standard
        self.mlp_lr_schedule_invscaling = tk.BooleanVar(value=False)
        self.mlp_lr_schedule_adaptive = tk.BooleanVar(value=False)

        # momentum (for SGD solver)
        self.mlp_momentum_07 = tk.BooleanVar(value=False)
        self.mlp_momentum_08 = tk.BooleanVar(value=False)
        self.mlp_momentum_09 = tk.BooleanVar(value=True)  # 0.9 ⭐ standard
        self.mlp_momentum_095 = tk.BooleanVar(value=False)
        self.mlp_momentum_099 = tk.BooleanVar(value=False)

        # ========== MODEL DEVELOPMENT TAB HYPERPARAMETERS (Single Value Selection) ==========
        # These variables control hyperparameters for single model training in Model Development tab
        # Unlike Analysis Config (checkboxes for grid search), these use single values (Combobox/Spinbox)

        # PLS Hyperparameters
        self.refine_pls_n_components = tk.IntVar(value=10)  # Number of components
        self.refine_pls_max_iter = tk.IntVar(value=500)  # Max iterations
        self.refine_pls_tol = tk.StringVar(value="1e-6")  # Convergence tolerance

        # Ridge Regression Hyperparameters
        self.refine_ridge_alpha = tk.StringVar(value="1.0")  # Regularization strength
        self.refine_ridge_solver = tk.StringVar(value="auto")  # Solver algorithm
        self.refine_ridge_tol = tk.StringVar(value="1e-4")  # Convergence tolerance

        # Lasso Regression Hyperparameters
        self.refine_lasso_alpha = tk.StringVar(value="1.0")  # Regularization strength
        self.refine_lasso_selection = tk.StringVar(value="cyclic")  # Feature selection method
        self.refine_lasso_tol = tk.StringVar(value="1e-4")  # Convergence tolerance
        self.refine_lasso_max_iter = tk.IntVar(value=1000)  # Max iterations for convergence

        # ElasticNet Hyperparameters
        self.refine_elasticnet_alpha = tk.StringVar(value="1.0")  # Regularization strength
        self.refine_elasticnet_l1_ratio = tk.StringVar(value="0.5")  # L1 vs L2 mix ratio
        self.refine_elasticnet_selection = tk.StringVar(value="cyclic")  # Feature selection method
        self.refine_elasticnet_tol = tk.StringVar(value="1e-4")  # Convergence tolerance
        self.refine_elasticnet_max_iter = tk.IntVar(value=1000)  # Max iterations for convergence

        # RandomForest Hyperparameters
        self.refine_rf_n_estimators = tk.IntVar(value=200)  # Number of trees
        self.refine_rf_max_depth = tk.StringVar(value="None")  # Maximum tree depth
        self.refine_rf_min_samples_split = tk.IntVar(value=2)  # Min samples to split
        self.refine_rf_min_samples_leaf = tk.IntVar(value=1)  # Min samples at leaf
        self.refine_rf_max_features = tk.StringVar(value="sqrt")  # Max features per split
        self.refine_rf_bootstrap = tk.BooleanVar(value=True)  # Bootstrap sampling
        self.refine_rf_max_leaf_nodes = tk.StringVar(value="None")  # Max leaf nodes
        self.refine_rf_min_impurity_decrease = tk.DoubleVar(value=0.0)  # Min impurity decrease

        # XGBoost Hyperparameters
        self.refine_xgb_n_estimators = tk.IntVar(value=100)  # Number of boosting rounds
        self.refine_xgb_learning_rate = tk.DoubleVar(value=0.1)  # Learning rate (eta)
        self.refine_xgb_max_depth = tk.IntVar(value=6)  # Maximum tree depth
        self.refine_xgb_subsample = tk.DoubleVar(value=0.8)  # Row subsample ratio
        self.refine_xgb_colsample_bytree = tk.DoubleVar(value=0.8)  # Column subsample ratio
        self.refine_xgb_reg_alpha = tk.DoubleVar(value=0.1)  # L1 regularization
        self.refine_xgb_reg_lambda = tk.DoubleVar(value=1.0)  # L2 regularization
        self.refine_xgb_min_child_weight = tk.IntVar(value=1)  # Min sum of instance weight in child
        self.refine_xgb_gamma = tk.DoubleVar(value=0.0)  # Min loss reduction to split

        # LightGBM Hyperparameters
        self.refine_lgbm_n_estimators = tk.IntVar(value=100)  # Number of boosting rounds
        self.refine_lgbm_learning_rate = tk.DoubleVar(value=0.1)  # Learning rate
        self.refine_lgbm_num_leaves = tk.IntVar(value=31)  # Max number of leaves
        self.refine_lgbm_max_depth = tk.IntVar(value=-1)  # Maximum tree depth (-1 = no limit)
        self.refine_lgbm_min_child_samples = tk.IntVar(value=5)  # Min samples in leaf
        self.refine_lgbm_subsample = tk.DoubleVar(value=0.8)  # Row subsample ratio
        self.refine_lgbm_colsample_bytree = tk.DoubleVar(value=0.8)  # Column subsample ratio
        self.refine_lgbm_reg_alpha = tk.DoubleVar(value=0.1)  # L1 regularization
        self.refine_lgbm_reg_lambda = tk.DoubleVar(value=1.0)  # L2 regularization

        # CatBoost Hyperparameters
        self.refine_catboost_iterations = tk.IntVar(value=100)  # Number of boosting iterations
        self.refine_catboost_learning_rate = tk.DoubleVar(value=0.1)  # Learning rate
        self.refine_catboost_depth = tk.IntVar(value=6)  # Tree depth
        self.refine_catboost_l2_leaf_reg = tk.DoubleVar(value=3.0)  # L2 regularization
        self.refine_catboost_border_count = tk.IntVar(value=128)  # Number of splits for numerical features
        self.refine_catboost_bagging_temperature = tk.DoubleVar(value=1.0)  # Bayesian bootstrap intensity
        self.refine_catboost_random_strength = tk.DoubleVar(value=1.0)  # Randomness for scoring splits

        # SVR Hyperparameters
        self.refine_svr_kernel = tk.StringVar(value="rbf")  # Kernel type
        self.refine_svr_C = tk.DoubleVar(value=1.0)  # Regularization parameter
        self.refine_svr_gamma = tk.StringVar(value="scale")  # Kernel coefficient
        self.refine_svr_epsilon = tk.DoubleVar(value=0.1)  # Epsilon-tube width
        self.refine_svr_degree = tk.IntVar(value=3)  # Degree for polynomial kernel
        self.refine_svr_coef0 = tk.DoubleVar(value=0.0)  # Independent term in kernel
        self.refine_svr_shrinking = tk.BooleanVar(value=True)  # Use shrinking heuristic

        # MLP Hyperparameters
        self.refine_mlp_hidden_layer_sizes = tk.StringVar(value="(64,)")  # Hidden layer architecture
        self.refine_mlp_alpha = tk.StringVar(value="0.001")  # L2 regularization
        self.refine_mlp_learning_rate_init = tk.StringVar(value="0.001")  # Initial learning rate
        self.refine_mlp_activation = tk.StringVar(value="relu")  # Activation function
        self.refine_mlp_solver = tk.StringVar(value="adam")  # Weight optimization solver
        self.refine_mlp_batch_size = tk.StringVar(value="auto")  # Minibatch size
        self.refine_mlp_learning_rate = tk.StringVar(value="constant")  # Learning rate schedule
        self.refine_mlp_momentum = tk.DoubleVar(value=0.9)  # Momentum for SGD solver
        self.refine_mlp_max_iter = tk.IntVar(value=200)  # Maximum iterations

        # NeuralBoosted Hyperparameters
        self.refine_neuralboosted_n_estimators = tk.IntVar(value=100)  # Number of boosting stages
        self.refine_neuralboosted_learning_rate = tk.DoubleVar(value=0.1)  # Learning rate
        self.refine_neuralboosted_hidden_layer_size = tk.IntVar(value=3)  # Hidden layer size
        self.refine_neuralboosted_activation = tk.StringVar(value="tanh")  # Activation function
        self.refine_neuralboosted_early_stopping = tk.BooleanVar(value=False)  # Early stopping

        # Variable selection methods (multiple selection enabled)
        self.varsel_importance = tk.BooleanVar(value=True)  # Default enabled
        self.varsel_spa = tk.BooleanVar(value=False)
        self.varsel_uve = tk.BooleanVar(value=False)
        self.varsel_uve_spa = tk.BooleanVar(value=False)
        self.varsel_ipls = tk.BooleanVar(value=False)
        self.apply_uve_prefilter = tk.BooleanVar(value=False)  # Apply UVE before main selection
        self.uve_cutoff_multiplier = tk.DoubleVar(value=1.0)  # UVE threshold (0.7-1.5)
        self.uve_n_components = tk.StringVar(value="")  # PLS components for UVE (empty = auto)
        self.spa_n_random_starts = tk.IntVar(value=10)  # Random starts for SPA
        self.ipls_n_intervals = tk.IntVar(value=20)  # Number of intervals for iPLS

        # CSV export option
        self.export_preprocessed_csv = tk.BooleanVar(value=False)

        # Ensemble methods (Phase 2)
        self.enable_ensembles = tk.BooleanVar(value=False)
        self.ensemble_simple_average = tk.BooleanVar(value=False)
        self.ensemble_region_weighted = tk.BooleanVar(value=True)
        self.ensemble_mixture_experts = tk.BooleanVar(value=True)
        self.ensemble_stacking = tk.BooleanVar(value=True)
        self.ensemble_stacking_region = tk.BooleanVar(value=False)
        self.ensemble_n_regions = tk.IntVar(value=5)  # Number of regions for region-based ensembles
        self.ensemble_top_n = tk.IntVar(value=15)  # Number of top models to include in ensemble
        self.ensemble_results = None  # Store ensemble predictions and metrics
        self.training_data_cache = None  # Cache for manual ensemble retraining

        # Outlier detection variables (Phase 3)
        self.n_pca_components = tk.IntVar(value=5)
        self.y_min_bound = tk.StringVar(value="")
        self.y_max_bound = tk.StringVar(value="")
        self.outlier_report = None  # Store most recent outlier detection report

        # Progress tracking
        self.progress_monitor = None
        self.analysis_thread = None
        self.analysis_start_time = None

        # Plotting
        self.plot_frames = {}
        self.plot_canvases = {}

        # Configure event debouncing (prevent slowdown on tab switches)
        self._configure_timers = {}

        # Theme system (will be populated by _create_top_bar())
        self.theme_buttons = {}

        self._create_ui()

    def _is_categorical_target(self):
        """Check if target variable is categorical (non-numeric)."""
        if self.y is None:
            return False
        return (self.y.dtype == 'object' or
                self.y.dtype.name == 'category' or
                not np.issubdtype(self.y.dtype, np.number))

    def _get_available_color_variables(self):
        """Get list of available variables for plot coloring.

        Returns list of options: ['None', 'Y Value', ...metadata columns]
        """
        options = ['None', 'Y Value']
        # Check combined file metadata first, then reference file metadata
        if hasattr(self, 'combined_metadata_df') and self.combined_metadata_df is not None and len(self.combined_metadata_df.columns) > 0:
            options.extend(sorted(self.combined_metadata_df.columns.tolist()))
        elif self.ref is not None and len(self.ref.columns) > 0:
            options.extend(sorted(self.ref.columns.tolist()))
        return options

    def _is_categorical_variable(self, variable_name, values=None):
        """Check if a variable is categorical or continuous.

        Parameters
        ----------
        variable_name : str
            Name of the variable ('Y Value' or metadata column name)
        values : array-like, optional
            Values to check. If None, will look up from self.y or self.ref

        Returns
        -------
        bool
            True if categorical, False if continuous
        """
        if variable_name == 'Y Value':
            return self._is_categorical_target()
        elif variable_name == 'None':
            return False
        else:
            # Check metadata column (from combined file or reference file)
            if values is None:
                # Check combined file metadata first
                if hasattr(self, 'combined_metadata_df') and self.combined_metadata_df is not None and variable_name in self.combined_metadata_df.columns:
                    values = self.combined_metadata_df[variable_name]
                elif self.ref is not None and variable_name in self.ref.columns:
                    values = self.ref[variable_name]
                else:
                    return False

            # Check dtype
            if hasattr(values, 'dtype'):
                return (values.dtype == 'object' or
                        values.dtype.name == 'category' or
                        not np.issubdtype(values.dtype, np.number))
            return False

    def _apply_color_scheme(self, ax, fig, x_data, y_data, color_by, indices=None, **scatter_kwargs):
        """Apply color scheme to scatter plot based on selected variable.

        Parameters
        ----------
        ax : matplotlib.axes.Axes
            The axes to plot on
        fig : matplotlib.figure.Figure
            The figure (needed for colorbar)
        x_data : array-like
            X coordinates
        y_data : array-like
            Y coordinates
        color_by : str
            Variable to color by ('None', 'Y Value', or metadata column name)
        indices : array-like, optional
            Indices into self.y and self.ref for the data points
            If None, assumes x_data and y_data align with self.y and self.ref
        **scatter_kwargs : dict
            Additional arguments passed to ax.scatter()

        Returns
        -------
        scatter
            The scatter plot object (for potential colorbar addition)
        has_legend : bool
            True if categorical legend was added
        """
        if indices is None:
            indices = np.arange(len(x_data))

        # Default scatter arguments
        default_kwargs = {'alpha': 0.6, 'edgecolors': 'black', 'linewidths': 0.5, 's': 50}
        default_kwargs.update(scatter_kwargs)

        if color_by == 'None':
            # Single color
            scatter = ax.scatter(x_data, y_data, color='steelblue', **default_kwargs)
            return scatter, False

        elif color_by == 'Y Value':
            # Color by target variable
            color_values = self.y.iloc[indices].values if len(indices) <= len(self.y) else self.y.values

            if self._is_categorical_target():
                # Categorical Y: discrete colors
                unique_classes = np.unique(color_values)
                n_classes = len(unique_classes)
                colors = plt.cm.Set1(np.linspace(0, 1, min(n_classes, 9)))
                if n_classes > 9:
                    colors = plt.cm.tab20(np.linspace(0, 1, min(n_classes, 20)))

                color_map = {cls: colors[i % len(colors)] for i, cls in enumerate(unique_classes)}

                # Plot each class separately for legend
                for cls in unique_classes:
                    mask = color_values == cls
                    if np.any(mask):
                        ax.scatter(x_data[mask], y_data[mask],
                                 c=[color_map[cls]], label=str(cls), **default_kwargs)

                ax.legend(title='Category', loc='best', framealpha=0.9)
                return None, True
            else:
                # Continuous Y: colormap
                # Ensure numeric (should already be for regression, but be safe)
                numeric_y = pd.to_numeric(color_values, errors='coerce')
                scatter = ax.scatter(x_data, y_data, c=numeric_y,
                                   cmap='viridis', **default_kwargs)
                cbar = fig.colorbar(scatter, ax=ax, label='Y Value')
                return scatter, False

        else:
            # Color by metadata column (from combined file or reference file)
            metadata_source = None
            if hasattr(self, 'combined_metadata_df') and self.combined_metadata_df is not None and color_by in self.combined_metadata_df.columns:
                metadata_source = self.combined_metadata_df
            elif self.ref is not None and color_by in self.ref.columns:
                metadata_source = self.ref

            if metadata_source is None:
                # Fallback to no color
                scatter = ax.scatter(x_data, y_data, color='steelblue', **default_kwargs)
                return scatter, False

            metadata_values = metadata_source[color_by].iloc[indices].values if len(indices) <= len(metadata_source) else metadata_source[color_by].values

            if self._is_categorical_variable(color_by, metadata_values):
                # Categorical metadata: discrete colors
                unique_vals = np.unique(metadata_values[pd.notna(metadata_values)])
                n_vals = len(unique_vals)

                # Choose colormap based on number of unique values
                if n_vals <= 10:
                    colors = plt.cm.tab10(np.linspace(0, 1, 10))
                elif n_vals <= 20:
                    colors = plt.cm.tab20(np.linspace(0, 1, 20))
                else:
                    # For many categories, use repeated colormap
                    colors = plt.cm.tab20(np.linspace(0, 1, 20))

                color_map = {val: colors[i % len(colors)] for i, val in enumerate(unique_vals)}

                # Plot each category separately for legend
                for val in unique_vals:
                    mask = metadata_values == val
                    if np.any(mask):
                        ax.scatter(x_data[mask], y_data[mask],
                                 c=[color_map[val]], label=str(val), **default_kwargs)

                # Handle NaN values if present
                nan_mask = pd.isna(metadata_values)
                if np.any(nan_mask):
                    ax.scatter(x_data[nan_mask], y_data[nan_mask],
                             c='lightgray', label='N/A', **default_kwargs)

                ax.legend(title=color_by, loc='best', framealpha=0.9, fontsize=8)
                return None, True
            else:
                # Continuous metadata: colormap
                # Convert to numeric if needed (handles string numeric values)
                numeric_values = pd.to_numeric(metadata_values, errors='coerce')

                # Handle NaN values (including conversion failures)
                valid_mask = pd.notna(numeric_values)

                if np.any(valid_mask):
                    scatter = ax.scatter(x_data[valid_mask], y_data[valid_mask],
                                       c=numeric_values[valid_mask],
                                       cmap='viridis', **default_kwargs)
                    cbar = fig.colorbar(scatter, ax=ax, label=color_by)

                # Plot NaN values in gray
                if np.any(~valid_mask):
                    ax.scatter(x_data[~valid_mask], y_data[~valid_mask],
                             c='lightgray', label='N/A', **default_kwargs)

                return scatter if np.any(valid_mask) else None, False

    def _configure_style(self):
        """Configure modern Wabi-Sabi aesthetic with multiple theme support."""
        style = ttk.Style()

        # Define theme skins: 6 light themes (Classic + 5 Japanese-inspired)
        self.themes = {
            'classic': {  # Classic - Traditional, professional, familiar
                'name': '📊 Classic',
                'bg': '#F0F0F0',
                'bg_secondary': '#E0E0E0',
                'panel': '#F8F8F8',  # Changed from harsh #FFFFFF to softer #F8F8F8
                'sidebar': '#D4D4D4',
                'sidebar_hover': '#C0C0C0',
                'text': '#000000',
                'text_light': '#666666',
                'text_inverse': '#FFFFFF',
                'accent': '#0078D7',
                'accent_dark': '#005A9E',
                'accent_gradient': ['#0078D7', '#5EB8FF'],
                'success': '#107C10',
                'warning': '#FF8C00',
                'border': '#999999',  # Darker, stronger border
                'border_light': '#CCCCCC',  # Secondary lighter border
                'shadow': '#BEBEBE',  # Much darker shadow for depth
                'tab_bg': '#F8F8F8',  # Changed from harsh #FFFFFF to softer #F8F8F8
                'tab_active': '#0078D7',
                'card_bg': '#F8F8F8',  # Changed from harsh #FFFFFF to softer #F8F8F8
            },
            'sakura': {  # Cherry Blossom - Soft, elegant, feminine
                'name': '🌸 Sakura',
                'bg': '#FFF8F8',
                'bg_secondary': '#FFE8E8',
                'panel': '#FFFFFF',
                'sidebar': '#FFD1D1',
                'sidebar_hover': '#FFC1C1',
                'text': '#5D4157',
                'text_light': '#9B8A96',
                'text_inverse': '#FFFFFF',
                'accent': '#E85A8A',  # Improved contrast for better readability
                'accent_dark': '#D04A7A',
                'accent_gradient': ['#E85A8A', '#FFB6D9'],
                'success': '#82C785',
                'warning': '#F4A261',
                'border': '#FFB0B0',  # Darker, stronger border
                'border_light': '#FFD1D1',  # Secondary lighter border
                'shadow': '#FFC8C8',  # Much darker shadow for depth
                'tab_bg': '#FFFFFF',
                'tab_active': '#E85A8A',
                'card_bg': '#FFFFFF',
            },
            'matcha': {  # Green Tea - Calm, natural, balanced
                'name': '🍵 Matcha',
                'bg': '#F8FBF6',
                'bg_secondary': '#E8F5E0',
                'panel': '#FFFFFF',
                'sidebar': '#B8D4A8',
                'sidebar_hover': '#A8C498',
                'text': '#2D4A2B',
                'text_light': '#6B8268',
                'text_inverse': '#FFFFFF',
                'accent': '#6BB85C',  # Improved contrast for better readability
                'accent_dark': '#5AA84D',
                'accent_gradient': ['#6BB85C', '#B8E6A8'],
                'success': '#6BBD6C',
                'warning': '#E8A547',
                'border': '#A0C890',  # Darker, stronger border
                'border_light': '#D0E5C8',  # Secondary lighter border
                'shadow': '#C8DDB8',  # Much darker shadow for depth
                'tab_bg': '#FFFFFF',
                'tab_active': '#6BB85C',
                'card_bg': '#FFFFFF',
            },
            'sumie': {  # Ink Painting - Minimalist, monochromatic, zen
                'name': '🖌️ Sumi-e',
                'bg': '#F5F5F5',
                'bg_secondary': '#E8E8E8',
                'panel': '#FFFFFF',
                'sidebar': '#4A4A4A',
                'sidebar_hover': '#5A5A5A',
                'text': '#2C2C2C',
                'text_light': '#7A7A7A',
                'text_inverse': '#FFFFFF',
                'accent': '#5A5A5A',
                'accent_dark': '#3A3A3A',
                'accent_gradient': ['#5A5A5A', '#8A8A8A'],
                'success': '#6B9B6C',
                'warning': '#D89A5A',
                'border': '#A0A0A0',  # Darker, stronger border
                'border_light': '#D8D8D8',  # Secondary lighter border
                'shadow': '#B0B0B0',  # Much darker shadow for depth
                'tab_bg': '#FFFFFF',
                'tab_active': '#5A5A5A',
                'card_bg': '#FFFFFF',
            },
            'yuhi': {  # Sunset - Warm, vibrant, energetic
                'name': '🌅 Yuhi',
                'bg': '#FFF9F5',
                'bg_secondary': '#FFE8D8',
                'panel': '#FFFFFF',
                'sidebar': '#FF9A6C',
                'sidebar_hover': '#FF8A5C',
                'text': '#4A3A2F',
                'text_light': '#8A7A6F',
                'text_inverse': '#FFFFFF',
                'accent': '#E85A3A',  # Improved contrast for better readability
                'accent_dark': '#D04A2A',
                'accent_gradient': ['#E85A3A', '#FFB494'],
                'success': '#7FC77F',
                'warning': '#FFA726',
                'border': '#FFB090',  # Darker, stronger border
                'border_light': '#FFD1B8',  # Secondary lighter border
                'shadow': '#FFC8B0',  # Much darker shadow for depth
                'tab_bg': '#FFFFFF',
                'tab_active': '#E85A3A',
                'card_bg': '#FFFFFF',
            },
            'ocean': {  # Ocean Wave - Deep, sophisticated, modern
                'name': '🌊 Ocean',
                'bg': '#F5F9FB',
                'bg_secondary': '#E0EEF5',
                'panel': '#FFFFFF',
                'sidebar': '#5BA3C4',
                'sidebar_hover': '#4B93B4',
                'text': '#1E3A52',
                'text_light': '#5E7A92',
                'text_inverse': '#FFFFFF',
                'accent': '#2D7AA8',  # Improved contrast for better readability
                'accent_dark': '#246A98',
                'accent_gradient': ['#2D7AA8', '#7DC4E8'],
                'success': '#5CB85C',
                'warning': '#F0AD4E',
                'border': '#7AB8D8',  # Darker, stronger border
                'border_light': '#A8D4E8',  # Secondary lighter border
                'shadow': '#B0D0E8',  # Much darker shadow for depth
                'tab_bg': '#FFFFFF',
                'tab_active': '#2D7AA8',
                'card_bg': '#FFFFFF',
            }
        }

        # Set default theme (can be changed by user)
        self.current_theme_name = tk.StringVar(value='classic')
        self._apply_theme('classic')

    def _apply_theme(self, theme_name):
        """Apply a specific theme to the application."""
        if theme_name not in self.themes:
            theme_name = 'ocean'

        self.colors = self.themes[theme_name]
        self.current_theme_name.set(theme_name)

        # Configure root window
        self.root.configure(bg=self.colors['bg'])

        # Get modern font stack (try Inter, SF Pro, fallback to system fonts)
        import platform
        system = platform.system()
        if system == 'Darwin':  # macOS
            heading_font = ('SF Pro Display', 'Helvetica Neue', 'Arial')
            body_font = ('SF Pro Text', 'Helvetica Neue', 'Arial')
        elif system == 'Windows':
            heading_font = ('Segoe UI', 'Arial')
            body_font = ('Segoe UI', 'Arial')
        else:  # Linux
            heading_font = ('Inter', 'Ubuntu', 'DejaVu Sans', 'Arial')
            body_font = ('Inter', 'Ubuntu', 'DejaVu Sans', 'Arial')

        style = ttk.Style()

        # Modern button styles with gradients (simulated with colors)
        # Unified sizing to match accent buttons for visual consistency
        style.configure('Modern.TButton',
                       font=(body_font, 10),
                       padding=(15, 10),  # Increased vertical padding for better alignment
                       borderwidth=0,
                       relief='flat',
                       foreground=self.colors['text'])
        style.map('Modern.TButton',
                 background=[('active', self.colors['accent']),
                           ('!disabled', self.colors['panel'])],
                 foreground=[('active', self.colors['text_inverse']),
                           ('!disabled', self.colors['text'])])

        style.configure('Accent.TButton',
                       font=(body_font, 11, 'bold'),
                       padding=(20, 12),
                       background='#0078D4',  # Explicit blue background
                       foreground='#FFFFFF',  # Explicit white text
                       borderwidth=1,
                       relief='solid')
        style.map('Accent.TButton',
                 background=[('active', '#005A9E'),        # Darker blue on hover
                           ('!disabled', '#0078D4')],      # Blue normally
                 foreground=[('active', '#FFFFFF'),        # White text on hover
                           ('!disabled', '#FFFFFF')])      # White text normally

        # Default style for all ttk.Button widgets (IMPORTANT: prevents invisible text)
        style.configure('TButton',
                       font=(body_font, 10),
                       padding=(15, 8),
                       borderwidth=1,  # Add border for visibility
                       relief='solid',
                       foreground='#000000',  # Explicit black text
                       background='#FFFFFF')  # Explicit white background

        style.map('TButton',
                 background=[('active', self.colors['accent']),      # Colored on hover
                           ('!disabled', '#FFFFFF')],               # White normally
                 foreground=[('active', self.colors['text_inverse']),# White text on hover
                           ('!disabled', '#000000')])               # Black text normally

        # Secondary button style (for less prominent actions)
        style.configure('Secondary.TButton',
                       font=(body_font, 10),
                       padding=(12, 6),
                       borderwidth=1,
                       relief='solid',
                       foreground=self.colors['text'],
                       background=self.colors['bg_secondary'])

        style.map('Secondary.TButton',
                 background=[('active', self.colors['panel']),
                           ('!disabled', self.colors['bg_secondary'])],
                 foreground=[('active', self.colors['text']),
                           ('!disabled', self.colors['text'])])

        # Frame styling
        style.configure('TFrame', background=self.colors['bg'])
        style.configure('Card.TFrame', background=self.colors['card_bg'], relief='flat')
        style.configure('Sidebar.TFrame', background=self.colors['sidebar'])

        # Label hierarchy with modern typography
        style.configure('TLabel',
                       background=self.colors['bg'],
                       foreground=self.colors['text'],
                       font=(body_font, 10))
        style.configure('Title.TLabel',
                       font=(heading_font, 28, 'bold'),
                       foreground=self.colors['text'],
                       background=self.colors['bg'])
        style.configure('Heading.TLabel',
                       font=(heading_font, 16, 'bold'),
                       foreground=self.colors['text'],
                       background=self.colors['bg'])
        style.configure('Subheading.TLabel',
                       font=(heading_font, 12, 'bold'),
                       foreground=self.colors['accent'],
                       background=self.colors['bg'])
        style.configure('Caption.TLabel',
                       font=(body_font, 9),
                       foreground=self.colors['text_light'],
                       background=self.colors['bg'])
        style.configure('SidebarLabel.TLabel',
                       font=(body_font, 11),
                       foreground=self.colors['text_inverse'],
                       background=self.colors['sidebar'],
                       padding=(15, 10))
        style.configure('CardLabel.TLabel',
                       background=self.colors['card_bg'],
                       foreground=self.colors['text'],
                       font=(body_font, 10))

        # Notebook styling - Modern tab design with improved visibility
        # Selected tabs now use accent color for text (not background) for better contrast
        style.configure('TNotebook',
                       background=self.colors['bg'],
                       borderwidth=0,
                       tabmargins=[0, 0, 0, 0])
        style.configure('TNotebook.Tab',
                       font=(body_font, 11, 'bold'),  # Bold for selected emphasis
                       padding=(24, 12),  # More generous padding
                       borderwidth=0)
        style.map('TNotebook.Tab',
                 background=[('selected', self.colors['bg']),           # Selected tabs match background
                           ('!selected', self.colors['bg_secondary'])], # Unselected tabs slightly different
                 foreground=[('selected', self.colors['accent']),       # Accent color when selected (high contrast)
                           ('!selected', self.colors['text_light'])])   # Muted when not selected

        # Entry and input styling - stronger borders for better definition
        style.configure('TEntry',
                       fieldbackground=self.colors['card_bg'],
                       foreground=self.colors['text'],
                       borderwidth=2,
                       relief='solid',
                       bordercolor=self.colors['border'])
        style.map('TEntry',
                 bordercolor=[('focus', self.colors['accent'])])

        # LabelFrame styling - add strong borders and better visual definition
        style.configure('TLabelframe',
                       background=self.colors['bg'],
                       borderwidth=2,
                       relief='solid',
                       bordercolor=self.colors['border'])
        style.configure('TLabelframe.Label',
                       background=self.colors['bg'],
                       foreground=self.colors['text'],
                       font=(body_font, 11, 'bold'))

        # Combobox styling - add stronger borders for better definition
        style.configure('TCombobox',
                       fieldbackground=self.colors['panel'],
                       foreground=self.colors['text'],
                       background=self.colors['panel'],
                       borderwidth=2,
                       relief='solid',
                       bordercolor=self.colors['border'])
        style.map('TCombobox',
                 fieldbackground=[('readonly', self.colors['panel'])],
                 selectbackground=[('readonly', self.colors['accent'])],
                 selectforeground=[('readonly', self.colors['text_inverse'])],
                 bordercolor=[('focus', self.colors['accent'])])

        # Checkbutton styling
        style.configure('TCheckbutton',
                       background=self.colors['bg'],
                       foreground=self.colors['text'],
                       font=(body_font, 10))

        # Radiobutton styling
        style.configure('TRadiobutton',
                       background=self.colors['bg'],
                       foreground=self.colors['text'],
                       font=(body_font, 10))

    def _create_top_bar(self):
        """Create a beautiful top bar with app title and theme switcher."""
        # Get platform-appropriate font
        import platform
        system = platform.system()
        if system == 'Darwin':  # macOS
            title_font = ('SF Pro Display', 32, 'bold')
            subtitle_font = ('SF Pro Text', 12)
            label_font = ('SF Pro Text', 11)
            button_font = ('SF Pro Text', 10, 'bold')
        elif system == 'Windows':
            title_font = ('Segoe UI', 32, 'bold')
            subtitle_font = ('Segoe UI', 12)
            label_font = ('Segoe UI', 11)
            button_font = ('Segoe UI', 10, 'bold')
        else:  # Linux
            title_font = ('Ubuntu', 32, 'bold')
            subtitle_font = ('Ubuntu', 12)
            label_font = ('Ubuntu', 11)
            button_font = ('Ubuntu', 10, 'bold')

        top_bar = tk.Frame(self.root, bg=self.colors['bg'], height=140)
        top_bar.pack(fill='x', padx=20, pady=(20, 10))
        top_bar.pack_propagate(False)

        # Left side: Logo and title
        title_frame = tk.Frame(top_bar, bg=self.colors['bg'])
        title_frame.pack(side='left', fill='y')

        # ASP Logo - Rainbow cobra with spectral bar (150px tall)
        self.logo_label = self._create_logo_label(title_frame, size=150)
        self.logo_label.pack(side='left', padx=(0, 15), pady=0)

        # "Advanced Spectral Prediction" text to the right of logo - larger, single line
        text_frame = tk.Frame(title_frame, bg=self.colors['bg'])
        text_frame.pack(side='left', fill='y', pady=40)

        tk.Label(text_frame,
                text="Advanced Spectral Prediction",
                font=('Segoe UI', 24, 'bold'),
                fg=self.colors['text'],
                bg=self.colors['bg']).pack(anchor='w')

        # Right side: Theme switcher with beautiful buttons
        theme_frame = tk.Frame(top_bar, bg=self.colors['bg'])
        theme_frame.pack(side='right', fill='y', padx=10)

        tk.Label(theme_frame,
                text="Theme:",
                font=label_font,
                fg=self.colors['text_light'],
                bg=self.colors['bg']).pack(side='left', padx=(0, 10))

        # Create theme buttons with hover effects
        self.theme_buttons = {}
        for theme_name, theme_data in self.themes.items():
            btn = tk.Button(theme_frame,
                          text=theme_data['name'],
                          font=button_font,
                          fg='white',
                          bg=theme_data['accent'],
                          activebackground=theme_data['accent_dark'],
                          relief='flat',
                          borderwidth=0,
                          padx=15,
                          pady=8,
                          cursor='hand2',
                          command=lambda tn=theme_name: self._switch_theme(tn))
            btn.pack(side='left', padx=3)

            # Add hover effect
            def on_enter(e, b=btn, td=theme_data):
                b['bg'] = td['accent_dark']

            def on_leave(e, b=btn, td=theme_data):
                b['bg'] = td['accent']

            btn.bind('<Enter>', on_enter)
            btn.bind('<Leave>', on_leave)

            self.theme_buttons[theme_name] = btn

        # Add a stronger separator line for better visual definition
        separator = tk.Frame(self.root, bg=self.colors['border'], height=3)
        separator.pack(fill='x', padx=20)

    def _switch_theme(self, theme_name):
        """Switch to a new theme with smooth transition effect."""
        # Store current tab selection
        current_tab = self.notebook.index(self.notebook.select())

        # Apply new theme
        self._apply_theme(theme_name)

        # Update all widgets to reflect new theme
        self._update_widget_colors(self.root)

        # Update accent buttons to use new theme colors
        self._update_accent_buttons()

        # Restore tab selection
        self.notebook.select(current_tab)

        # Show a subtle notification
        self._show_theme_notification(self.themes[theme_name]['name'])

    def _update_accent_buttons(self):
        """Update all accent buttons to use current theme colors and rebind hover handlers."""
        for btn in self.accent_buttons:
            try:
                # Check if button still exists
                if btn.winfo_exists():
                    # Update button colors to new theme
                    btn.config(
                        fg=self.colors['text_inverse'],
                        bg=self.colors['accent'],
                        activeforeground=self.colors['text_inverse'],
                        activebackground=self.colors['accent_dark']
                    )

                    # Unbind old hover handlers
                    btn.unbind('<Enter>')
                    btn.unbind('<Leave>')

                    # Rebind with new theme colors
                    def on_enter(e, b=btn):
                        b.config(bg=self.colors['accent_dark'])

                    def on_leave(e, b=btn):
                        b.config(bg=self.colors['accent'])

                    btn.bind('<Enter>', on_enter)
                    btn.bind('<Leave>', on_leave)
            except:
                # Button might have been destroyed, skip it
                pass

    def _update_widget_colors(self, widget):
        """Recursively update all widget colors to match current theme."""
        try:
            # Update widget background if it has one
            if hasattr(widget, 'configure'):
                widget_type = widget.winfo_class()

                # Update different widget types appropriately
                if widget_type == 'Frame':
                    # Check if this frame should use card_bg (based on parent or current bg)
                    try:
                        current_bg = widget.cget('bg')
                        # If current bg looks like it was card_bg, update to new card_bg
                        # Check against all possible card_bg values from themes
                        card_bg_values = ['#FFFFFF', '#2D3548', '#3C3836']  # Light, Midnight, Obsidian
                        if current_bg in card_bg_values or current_bg == self.colors.get('card_bg'):
                            widget.configure(bg=self.colors['card_bg'])
                        else:
                            widget.configure(bg=self.colors['bg'])
                    except:
                        widget.configure(bg=self.colors['bg'])
                elif widget_type == 'Label':
                    # Check if label should use card_bg based on current background
                    try:
                        current_bg = widget.cget('bg')
                        card_bg_values = ['#FFFFFF', '#2D3548', '#3C3836']
                        if current_bg in card_bg_values or current_bg == self.colors.get('card_bg'):
                            widget.configure(bg=self.colors['card_bg'], fg=self.colors['text'])
                        else:
                            widget.configure(bg=self.colors['bg'], fg=self.colors['text'])
                    except:
                        widget.configure(bg=self.colors['bg'], fg=self.colors['text'])
                elif widget_type == 'Canvas':
                    widget.configure(bg=self.colors['bg'])
                elif widget_type == 'Button':
                    # Skip theme buttons as they have custom colors
                    if widget not in self.theme_buttons.values():
                        widget.configure(bg=self.colors['panel'], fg=self.colors['text'])

            # Recursively update children
            for child in widget.winfo_children():
                self._update_widget_colors(child)

        except Exception:
            # Some widgets might not support color changes
            pass

    def _show_theme_notification(self, theme_name):
        """Show a beautiful notification when theme changes."""
        # Get platform-appropriate font
        import platform
        system = platform.system()
        if system == 'Darwin':  # macOS
            notif_font = ('SF Pro Text', 11)
        elif system == 'Windows':
            notif_font = ('Segoe UI', 11)
        else:  # Linux
            notif_font = ('Ubuntu', 11)

        # Create a temporary notification label
        notif = tk.Label(self.root,
                        text=f"✨ Theme changed to {theme_name}",
                        font=notif_font,
                        fg=self.colors['text_inverse'],
                        bg=self.colors['accent'],
                        padx=20,
                        pady=10)
        notif.place(relx=0.5, rely=0.95, anchor='center')

        # Fade out after 2 seconds
        def fade_out():
            try:
                notif.destroy()
            except:
                pass

        self.root.after(2000, fade_out)

    def _create_accent_button(self, parent, text, command, **kwargs):
        """Create an accent button with proper colors that work across themes.

        This replaces ttk.Button with style='Accent.TButton' to ensure
        text is always visible (white on colored background).

        NOTE: Uses tk.Button instead of ttk.Button because Windows has a bug
        where ttk.Button ignores foreground color settings, making text invisible
        when background and text are the same color. tk.Button provides reliable
        cross-platform color control for accent-colored buttons.
        """
        # Get platform-appropriate font
        import platform
        system = platform.system()
        if system == 'Darwin':
            button_font = ('SF Pro Text', 11, 'bold')
        elif system == 'Windows':
            button_font = ('Segoe UI', 11, 'bold')
        else:
            button_font = ('Ubuntu', 11, 'bold')

        btn = tk.Button(parent,
                       text=text,
                       command=command,
                       font=button_font,
                       fg=self.colors['text_inverse'],  # White text
                       bg=self.colors['accent'],  # Colored background
                       activeforeground=self.colors['text_inverse'],
                       activebackground=self.colors['accent_dark'],
                       disabledforeground=self.colors['text_inverse'],  # Keep white text when disabled
                       relief='flat',
                       borderwidth=0,
                       padx=20,
                       pady=12,
                       cursor='hand2',
                       **kwargs)

        # Add hover effect
        def on_enter(e):
            btn.config(bg=self.colors['accent_dark'])

        def on_leave(e):
            btn.config(bg=self.colors['accent'])

        btn.bind('<Enter>', on_enter)
        btn.bind('<Leave>', on_leave)

        # Track this button for theme switching
        self.accent_buttons.append(btn)

        return btn

    def _create_logo_label(self, parent, size=150):
        """Load and display the rainbow cobra logo PNG image with spectral bar.

        Uses the beautiful rainbow cobra logo with spectral bar and UV/IR label.
        Automatically removes white background for transparency.
        """
        if not HAS_PIL:
            # Fallback to text if PIL not available
            logo_label = tk.Label(parent, text="ASP",
                                 font=('Arial', size//3, 'bold'),
                                 fg=self.colors['accent'], bg=self.colors['bg'])
            return logo_label

        try:
            # Load the ASP logo with spectral bar (no text overlay)
            logo_path = Path(__file__).parent / "asp_logo_final.png"
            if not logo_path.exists():
                # Try gradient version as fallback
                logo_path = Path(__file__).parent / "asp_logo_gradient.png"
                if not logo_path.exists():
                    # Fallback to text
                    logo_label = tk.Label(parent, text="ASP",
                                         font=('Arial', size//3, 'bold'),
                                         fg=self.colors['accent'], bg=self.colors['bg'])
                    return logo_label

            # Load and process the image
            img = Image.open(logo_path)

            # Convert to RGBA if not already (preserve existing transparency)
            if img.mode != 'RGBA':
                img = img.convert('RGBA')

            # Resize with high quality (preserving transparency)
            img = img.resize((size, size), Image.Resampling.LANCZOS)
            photo = ImageTk.PhotoImage(img)

            # Create label with image
            logo_label = tk.Label(parent, image=photo, bg=self.colors['bg'])
            logo_label.image = photo  # Keep a reference to prevent garbage collection

            return logo_label

        except Exception as e:
            # Fallback to text if image loading fails
            logo_label = tk.Label(parent, text="ASP",
                                 font=('Arial', size//3, 'bold'),
                                 fg=self.colors['accent'], bg=self.colors['bg'])
            return logo_label

    def _play_completion_chime(self):
        """Play a pleasant wind chime sound when analysis completes."""
        try:
            if HAS_WINSOUND:
                # Play a wind chime-like sequence of harmonious tones
                # Using frequencies from a major pentatonic scale for pleasant sound
                # C5, E5, G5, C6 (523, 659, 784, 1047 Hz)
                chime_notes = [
                    (523, 150),   # C5
                    (659, 150),   # E5
                    (784, 150),   # G5
                    (1047, 200),  # C6 (slightly longer for nice ending)
                ]

                def play_sequence():
                    """Play the chime sequence in background thread."""
                    try:
                        for freq, duration in chime_notes:
                            winsound.Beep(freq, duration)
                            import time
                            time.sleep(0.08)  # Small gap between notes for wind chime effect
                    except Exception:
                        pass

                # Play in background thread so it doesn't block UI
                import threading
                threading.Thread(target=play_sequence, daemon=True).start()
            else:
                # Fallback to terminal bell
                print('\a')
        except Exception:
            pass  # Silently fail if sound doesn't work

    def play_sound(self, sound_type='success'):
        """Play a simple notification sound.

        Args:
            sound_type: Type of sound to play ('success', 'error', etc.)
        """
        try:
            if HAS_WINSOUND:
                if sound_type == 'success':
                    # Play a simple success beep (higher pitch, short duration)
                    import threading
                    def beep():
                        try:
                            winsound.Beep(800, 100)  # 800 Hz for 100ms
                        except Exception:
                            pass
                    threading.Thread(target=beep, daemon=True).start()
                else:
                    # Default beep
                    import threading
                    def beep():
                        try:
                            winsound.Beep(600, 100)
                        except Exception:
                            pass
                    threading.Thread(target=beep, daemon=True).start()
        except Exception:
            pass  # Silently fail if sound doesn't work

    def _create_running_figure(self, parent, size=48):
        """Create an animated running figure for the Analysis Progress tab.

        Simple stick figure animation with 4 frames cycling through running poses.
        """
        canvas = tk.Canvas(parent, width=size, height=size,
                          bg=self.colors['bg'], highlightthickness=0)

        # Animation state
        canvas.animation_frame = 0
        canvas.animation_running = False
        canvas.animation_id = None

        # Store scale and color for animation
        canvas.anim_size = size
        canvas.anim_color = self.colors['accent']

        def draw_frame(frame_num):
            """Draw a specific frame of the running animation (0-3)."""
            canvas.delete('all')
            scale = size / 48.0
            color = self.colors['accent']

            # Calculate offset based on frame for forward motion
            x_offset = (frame_num * 3) % 12

            # Head
            canvas.create_oval(
                (18 + x_offset)*scale, 8*scale, (26 + x_offset)*scale, 16*scale,
                fill=color, outline=''
            )

            # Body
            canvas.create_line(
                (22 + x_offset)*scale, 16*scale, (22 + x_offset)*scale, 30*scale,
                fill=color, width=3*scale
            )

            # Arms (alternate based on frame)
            if frame_num % 2 == 0:
                # Left arm back, right arm forward
                canvas.create_line(
                    (22 + x_offset)*scale, 20*scale, (16 + x_offset)*scale, 28*scale,
                    fill=color, width=2*scale
                )
                canvas.create_line(
                    (22 + x_offset)*scale, 20*scale, (28 + x_offset)*scale, 16*scale,
                    fill=color, width=2*scale
                )
            else:
                # Left arm forward, right arm back
                canvas.create_line(
                    (22 + x_offset)*scale, 20*scale, (28 + x_offset)*scale, 28*scale,
                    fill=color, width=2*scale
                )
                canvas.create_line(
                    (22 + x_offset)*scale, 20*scale, (16 + x_offset)*scale, 16*scale,
                    fill=color, width=2*scale
                )

            # Legs (alternate based on frame)
            if frame_num in [0, 2]:
                # Left leg forward, right leg back
                canvas.create_line(
                    (22 + x_offset)*scale, 30*scale, (18 + x_offset)*scale, 42*scale,
                    fill=color, width=2.5*scale
                )
                canvas.create_line(
                    (22 + x_offset)*scale, 30*scale, (26 + x_offset)*scale, 38*scale,
                    fill=color, width=2.5*scale
                )
            else:
                # Left leg back, right leg forward
                canvas.create_line(
                    (22 + x_offset)*scale, 30*scale, (26 + x_offset)*scale, 42*scale,
                    fill=color, width=2.5*scale
                )
                canvas.create_line(
                    (22 + x_offset)*scale, 30*scale, (18 + x_offset)*scale, 38*scale,
                    fill=color, width=2.5*scale
                )

        def animate():
            """Cycle through animation frames."""
            if canvas.animation_running:
                draw_frame(canvas.animation_frame)
                canvas.animation_frame = (canvas.animation_frame + 1) % 4
                canvas.animation_id = canvas.after(150, animate)  # 150ms per frame

        # Store animation functions
        canvas.start_animation = lambda: setattr(canvas, 'animation_running', True) or animate()
        canvas.stop_animation = lambda: setattr(canvas, 'animation_running', False)
        canvas.draw_frame = draw_frame

        # Draw initial frame (static)
        draw_frame(0)

        return canvas

    # ========== Modern Layout Helper Methods ==========

    def _create_card(self, parent, title=None, subtitle=None):
        """Create a modern card container with gradient border and visual pop."""
        # Create multi-layer card with stronger borders and shadows for solid appearance
        # Outermost layer - much deeper shadow for substantial depth (6px instead of 3px)
        shadow_outer = tk.Frame(parent, bg=self.colors['shadow'], padx=6, pady=6)

        # Gradient border layer - stronger accent border (3px instead of 2px)
        # We simulate a gradient by using the accent color
        gradient_border = tk.Frame(shadow_outer, bg=self.colors['accent'], padx=3, pady=3)
        gradient_border.pack(fill='both', expand=True)

        # Inner card background
        card = tk.Frame(gradient_border, bg=self.colors['card_bg'], padx=20, pady=20)
        card.pack(fill='both', expand=True)

        if title:
            # Card title with accent color and larger font for impact
            title_label = tk.Label(card,
                                  text=title,
                                  font=('Segoe UI', 15, 'bold'),
                                  fg=self.colors['accent'],
                                  bg=self.colors['card_bg'],
                                  anchor='w')
            title_label.pack(fill='x', pady=(0, 5))

        if subtitle:
            # Card subtitle with improved styling
            subtitle_label = tk.Label(card,
                                     text=subtitle,
                                     font=('Segoe UI', 10),
                                     fg=self.colors['text_light'],
                                     bg=self.colors['card_bg'],
                                     anchor='w')
            subtitle_label.pack(fill='x', pady=(0, 15))

        return shadow_outer, card

    def _create_section_header(self, parent, text, row, column=0, columnspan=3):
        """Create a modern section header with accent line."""
        header_frame = tk.Frame(parent, bg=self.colors['bg'])
        header_frame.grid(row=row, column=column, columnspan=columnspan, sticky='ew', pady=(20, 10))

        # Accent line on the left
        accent_line = tk.Frame(header_frame, bg=self.colors['accent'], width=4)
        accent_line.pack(side='left', fill='y', padx=(0, 10))

        # Header text
        header_label = tk.Label(header_frame,
                               text=text,
                               font=('Segoe UI', 16, 'bold'),
                               fg=self.colors['text'],
                               bg=self.colors['bg'],
                               anchor='w')
        header_label.pack(side='left', fill='x')

        return header_frame

    def _create_button_with_gradient(self, parent, text, command, style='accent'):
        """Create a modern button with gradient-like effect."""
        if style == 'accent':
            bg_color = self.colors['accent']
            hover_color = self.colors['accent_dark']
        else:
            bg_color = self.colors['panel']
            hover_color = self.colors['bg_secondary']

        btn = tk.Button(parent,
                       text=text,
                       font=('Segoe UI', 11, 'bold'),
                       fg=self.colors['text_inverse'] if style == 'accent' else self.colors['text'],
                       bg=bg_color,
                       activebackground=hover_color,
                       relief='flat',
                       borderwidth=0,
                       padx=25,
                       pady=12,
                       cursor='hand2',
                       command=command)

        # Add hover effect
        def on_enter(e):
            btn['bg'] = hover_color

        def on_leave(e):
            btn['bg'] = bg_color

        btn.bind('<Enter>', on_enter)
        btn.bind('<Leave>', on_leave)

        return btn

    def _create_info_badge(self, parent, text, bg_color=None):
        """Create a small info badge/pill."""
        if bg_color is None:
            bg_color = self.colors['bg_secondary']

        badge = tk.Label(parent,
                        text=text,
                        font=('Segoe UI', 9, 'bold'),
                        fg=self.colors['text'],
                        bg=bg_color,
                        padx=10,
                        pady=4)
        return badge

    def _create_grid_layout(self, parent, num_columns=2):
        """Create a responsive grid layout container."""
        grid_frame = tk.Frame(parent, bg=self.colors['bg'])

        # Configure columns to distribute space evenly
        for i in range(num_columns):
            grid_frame.columnconfigure(i, weight=1, uniform='col')

        return grid_frame

    def _create_checkbox_group(self, parent, title, variables_dict, columns=3):
        """Create a modern checkbox group with grid layout."""
        # Create card for checkbox group
        card_outer, card = self._create_card(parent, title=title)

        # Create grid for checkboxes
        checkbox_frame = tk.Frame(card, bg=self.colors['card_bg'])
        checkbox_frame.pack(fill='both', expand=True)

        row, col = 0, 0
        for label, var in variables_dict.items():
            cb = ttk.Checkbutton(checkbox_frame,
                                text=label,
                                variable=var,
                                style='TCheckbutton')
            cb.grid(row=row, column=col, sticky='w', padx=10, pady=5)

            col += 1
            if col >= columns:
                col = 0
                row += 1

        return card_outer

    def _create_collapsible_section(self, parent, title, expanded=True):
        """Create a collapsible section with expand/collapse animation."""
        section_frame = tk.Frame(parent, bg=self.colors['bg'])

        # Header with toggle button
        header = tk.Frame(section_frame, bg=self.colors['bg_secondary'], cursor='hand2')
        header.pack(fill='x', pady=(5, 0))

        # Expand/collapse indicator
        indicator = tk.Label(header,
                            text='▼' if expanded else '▶',
                            font=('Segoe UI', 12),
                            fg=self.colors['text'],
                            bg=self.colors['bg_secondary'],
                            padx=10)
        indicator.pack(side='left')

        # Section title
        title_label = tk.Label(header,
                              text=title,
                              font=('Segoe UI', 13, 'bold'),
                              fg=self.colors['text'],
                              bg=self.colors['bg_secondary'],
                              anchor='w')
        title_label.pack(side='left', fill='x', expand=True, padx=(0, 10), pady=8)

        # Content frame (initially visible if expanded)
        content = tk.Frame(section_frame, bg=self.colors['bg'])
        if expanded:
            content.pack(fill='both', expand=True, pady=(0, 5))

        # Toggle function
        is_expanded = [expanded]  # Use list to allow modification in nested function

        def toggle():
            if is_expanded[0]:
                content.pack_forget()
                indicator.config(text='▶')
                is_expanded[0] = False
            else:
                content.pack(fill='both', expand=True, pady=(0, 5))
                indicator.config(text='▼')
                is_expanded[0] = True

        header.bind('<Button-1>', lambda e: toggle())
        indicator.bind('<Button-1>', lambda e: toggle())
        title_label.bind('<Button-1>', lambda e: toggle())

        return section_frame, content

    # ========== End of Modern Layout Helpers ==========

    def _create_ui(self):
        """Create 10-tab user interface with theme switching."""
        # Create top bar with theme switcher and title
        self._create_top_bar()

        # Create main content area with notebook
        self.notebook = ttk.Notebook(self.root)
        self.notebook.pack(fill='both', expand=True, padx=20, pady=(0, 20))

        # Create tabs
        self._create_tab0_data_management()  # NEW: Data Management tab (Tab 0)
        self._create_tab1_import_preview()
        self._create_tab2_data_viewer()
        self._create_tab3_data_quality_check()
        self._create_tab4_analysis_config()
        self._create_tab5_progress()
        self._create_tab6_results()
        self._create_tab7_refine_model()
        self._create_tab8_model_prediction()
        self._create_tab9_multi_model_comparison()
        self._create_tab10_calibration_transfer()

        # Bind tab change event
        self.notebook.bind('<<NotebookTabChanged>>', self._on_tab_changed)

    def _create_tab0_data_management(self):
        """Tab 0: Data Management - Import, merge, and manipulate multiple data sources."""
        # Create main tab frame
        self.tab0 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab0, text='  🗃️ Data Management  ')

        # Check if data management module is available
        if not HAS_DATA_MANAGEMENT:
            error_label = ttk.Label(self.tab0,
                                   text="Data Management module not available. Please check installation.",
                                   style='Heading.TLabel')
            error_label.pack(pady=20)
            return

        # Create notebook for subtabs
        self.data_mgmt_notebook = ttk.Notebook(self.tab0)
        self.data_mgmt_notebook.pack(fill='both', expand=True)

        # Create subtabs
        self._create_tab0a_import_sources()
        self._create_tab0b_merge_combine()
        self._create_tab0c_data_manipulation()
        self._create_tab0d_view_merged_data()

    def _create_tab0a_import_sources(self):
        """Subtab 0A: Import Sources - Load and manage multiple data sources."""
        # Create subtab frame with scrolling
        tab0a = ttk.Frame(self.data_mgmt_notebook, style='TFrame')
        self.data_mgmt_notebook.add(tab0a, text='  📥 Import Sources  ')

        # Create canvas for scrolling
        canvas = tk.Canvas(tab0a, background='#f0f0f0')
        scrollbar = ttk.Scrollbar(tab0a, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas, style='TFrame')

        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )

        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        # Pack scrolling components
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        # Add padding frame
        content_frame = ttk.Frame(scrollable_frame, style='TFrame', padding="20 10 20 10")
        content_frame.pack(fill='both', expand=True)

        # Create sections
        row = 0

        # === Section 1: Data Sources List ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "📋 Data Sources",
            expanded=True
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        # TreeView for data sources
        tree_frame = ttk.Frame(section_content)
        tree_frame.pack(fill='both', expand=True, pady=5)

        # Create TreeView with columns
        columns = ('Name', 'Format', 'Samples', 'Wavelengths', 'Range', 'Y', 'Metadata')
        self.data_sources_tree = ttk.Treeview(tree_frame, columns=columns, height=8, show='tree headings')

        # Configure columns
        self.data_sources_tree.column('#0', width=50, minwidth=50)
        self.data_sources_tree.column('Name', width=150, minwidth=100)
        self.data_sources_tree.column('Format', width=80, minwidth=60)
        self.data_sources_tree.column('Samples', width=80, minwidth=60)
        self.data_sources_tree.column('Wavelengths', width=100, minwidth=80)
        self.data_sources_tree.column('Range', width=150, minwidth=100)
        self.data_sources_tree.column('Y', width=50, minwidth=40)
        self.data_sources_tree.column('Metadata', width=70, minwidth=50)

        # Set headings
        self.data_sources_tree.heading('#0', text='ID')
        self.data_sources_tree.heading('Name', text='Name')
        self.data_sources_tree.heading('Format', text='Format')
        self.data_sources_tree.heading('Samples', text='Samples')
        self.data_sources_tree.heading('Wavelengths', text='Wavelengths')
        self.data_sources_tree.heading('Range', text='Range (nm)')
        self.data_sources_tree.heading('Y', text='Y')
        self.data_sources_tree.heading('Metadata', text='Metadata')

        # Add scrollbars
        tree_scroll_y = ttk.Scrollbar(tree_frame, orient='vertical', command=self.data_sources_tree.yview)
        tree_scroll_x = ttk.Scrollbar(tree_frame, orient='horizontal', command=self.data_sources_tree.xview)
        self.data_sources_tree.configure(yscrollcommand=tree_scroll_y.set, xscrollcommand=tree_scroll_x.set)

        # Pack TreeView and scrollbars
        self.data_sources_tree.grid(row=0, column=0, sticky='nsew')
        tree_scroll_y.grid(row=0, column=1, sticky='ns')
        tree_scroll_x.grid(row=1, column=0, sticky='ew')

        tree_frame.grid_rowconfigure(0, weight=1)
        tree_frame.grid_columnconfigure(0, weight=1)

        # Buttons for source management
        button_frame = ttk.Frame(section_content)
        button_frame.pack(fill='x', pady=5)

        tk.Button(button_frame, text="➕ Add Source", command=self._add_data_source,
                 bg='#0078D4', fg='white', font=('TkDefaultFont', 10, 'bold'),
                 relief='raised', bd=2, padx=15, pady=8).pack(side='left', padx=2)
        ttk.Button(button_frame, text="➖ Remove Source", command=self._remove_data_source).pack(side='left', padx=2)
        ttk.Button(button_frame, text="🔄 Refresh", command=self._refresh_sources_tree).pack(side='left', padx=2)
        ttk.Button(button_frame, text="💾 Save Config", command=self._save_source_config).pack(side='left', padx=2)
        ttk.Button(button_frame, text="📂 Load Config", command=self._load_source_config).pack(side='left', padx=2)
        ttk.Button(button_frame, text="🗑️ Clear All", command=self._clear_all_sources).pack(side='left', padx=2)

        # === Section 2: Add New Source ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "➕ Add New Source",
            expanded=False
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        # Source loading controls (similar to Tab 1)
        load_frame = ttk.LabelFrame(section_content, text="Load Data", padding="10")
        load_frame.pack(fill='x', pady=5)

        # Info label
        info_text = "Supports: Directories (ASD/CSV/SPC) • Combined Files (CSV/Excel)"
        ttk.Label(load_frame, text=info_text, font=('TkDefaultFont', 8), foreground='gray').grid(
            row=0, column=0, columnspan=3, sticky='w', padx=5, pady=(0, 5))

        # Spectral data directory or file
        ttk.Label(load_frame, text="Spectral Data:").grid(row=1, column=0, sticky='w', padx=5, pady=2)
        self.dm_spectra_path_var = tk.StringVar()
        ttk.Entry(load_frame, textvariable=self.dm_spectra_path_var, width=50).grid(row=1, column=1, padx=5, pady=2)
        ttk.Button(load_frame, text="Browse", command=self._browse_dm_spectra).grid(row=1, column=2, padx=5, pady=2)

        # Reference CSV or Excel (optional - only for directories)
        ttk.Label(load_frame, text="Reference (CSV/Excel):").grid(row=2, column=0, sticky='w', padx=5, pady=2)
        ttk.Label(load_frame, text="(optional, for directories only)", font=('TkDefaultFont', 8), foreground='gray').grid(
            row=2, column=1, sticky='w', padx=5, pady=0)
        self.dm_reference_path_var = tk.StringVar()
        ttk.Entry(load_frame, textvariable=self.dm_reference_path_var, width=50).grid(row=3, column=1, padx=5, pady=2)
        ttk.Button(load_frame, text="Browse", command=self._browse_dm_reference).grid(row=3, column=2, padx=5, pady=2)

        # Source name
        ttk.Label(load_frame, text="Source Name:").grid(row=4, column=0, sticky='w', padx=5, pady=2)
        self.dm_source_name_var = tk.StringVar()
        ttk.Entry(load_frame, textvariable=self.dm_source_name_var, width=50).grid(row=4, column=1, padx=5, pady=2)

        # Load button
        tk.Button(load_frame, text="Load Source", command=self._load_dm_source,
                 bg='#0078D4', fg='white', font=('TkDefaultFont', 10, 'bold'),
                 relief='raised', bd=2, padx=15, pady=8).grid(row=5, column=1, pady=10)

        # === Section 3: Source Details ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "📊 Source Details",
            expanded=False
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        # Details text widget
        self.source_details_text = tk.Text(section_content, height=10, width=60, wrap='word')
        self.source_details_text.pack(fill='both', expand=True, pady=5)

        # === Section 4: Quick Actions ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "⚡ Quick Actions",
            expanded=True
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        # Quick action buttons
        action_frame = ttk.Frame(section_content)
        action_frame.pack(fill='x', pady=5)

        tk.Button(action_frame, text="🚀 Use for Analysis",
                 command=self._use_for_analysis,
                 bg='#0078D4', fg='white', font=('TkDefaultFont', 11, 'bold'),
                 relief='raised', bd=2, padx=20, pady=10).pack(side='left', padx=5)

        ttk.Label(action_frame, text="← Sends selected data to analysis pipeline").pack(side='left', padx=5)

    def _create_tab0b_merge_combine(self):
        """Subtab 0B: Merge & Combine - Merge multiple data sources."""
        tab0b = ttk.Frame(self.data_mgmt_notebook, style='TFrame')
        self.data_mgmt_notebook.add(tab0b, text='  🔗 Merge & Combine  ')

        # Create scrollable frame
        canvas = tk.Canvas(tab0b, background='#f0f0f0')
        scrollbar = ttk.Scrollbar(tab0b, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas, style='TFrame')

        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )

        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        content_frame = ttk.Frame(scrollable_frame, style='TFrame', padding="20 10 20 10")
        content_frame.pack(fill='both', expand=True)

        row = 0

        # === Section 1: Source Selection ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "📋 Select Sources to Merge",
            expanded=True
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        # Checkboxes for source selection (will be populated dynamically)
        self.merge_source_vars = {}
        self.merge_sources_frame = ttk.Frame(section_content)
        self.merge_sources_frame.pack(fill='both', expand=True, pady=5)

        # === Section 2: Merge Strategy ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "🎯 Merge Strategy",
            expanded=True
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        strategy_frame = ttk.LabelFrame(section_content, text="Wavelength Alignment", padding="10")
        strategy_frame.pack(fill='x', pady=5)

        ttk.Radiobutton(strategy_frame, text="Intersection (common wavelengths only)",
                       variable=self.merge_strategy_var, value='intersection').pack(anchor='w', pady=2)
        ttk.Radiobutton(strategy_frame, text="Union (all wavelengths, NaN for missing)",
                       variable=self.merge_strategy_var, value='union').pack(anchor='w', pady=2)
        ttk.Radiobutton(strategy_frame, text="Interpolation (resample to common grid)",
                       variable=self.merge_strategy_var, value='interpolation').pack(anchor='w', pady=2)

        # Duplicate handling
        dup_frame = ttk.LabelFrame(section_content, text="Duplicate Sample Handling", padding="10")
        dup_frame.pack(fill='x', pady=5)

        ttk.Radiobutton(dup_frame, text="Error (stop if duplicates found)",
                       variable=self.duplicate_handling_var, value='error').pack(anchor='w', pady=2)
        ttk.Radiobutton(dup_frame, text="Keep First",
                       variable=self.duplicate_handling_var, value='keep_first').pack(anchor='w', pady=2)
        ttk.Radiobutton(dup_frame, text="Keep Last",
                       variable=self.duplicate_handling_var, value='keep_last').pack(anchor='w', pady=2)
        ttk.Radiobutton(dup_frame, text="Rename (prefix with source name)",
                       variable=self.duplicate_handling_var, value='rename').pack(anchor='w', pady=2)

        # === Section 3: Merge Preview ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "👁️ Merge Preview",
            expanded=False
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        # Preview text
        self.merge_preview_text = tk.Text(section_content, height=10, width=60, wrap='word')
        self.merge_preview_text.pack(fill='both', expand=True, pady=5)

        # === Section 4: Execute Merge ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "✅ Execute Merge",
            expanded=True
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        merge_button_frame = ttk.Frame(section_content)
        merge_button_frame.pack(fill='x', pady=5)

        ttk.Button(merge_button_frame, text="Preview Merge", command=self._preview_merge).pack(side='left', padx=5)
        tk.Button(merge_button_frame, text="Execute Merge", command=self._execute_merge,
                 bg='#0078D4', fg='white', font=('TkDefaultFont', 10, 'bold'),
                 relief='raised', bd=2, padx=15, pady=8).pack(side='left', padx=5)
        ttk.Button(merge_button_frame, text="🚀 Merge & Use for Analysis", command=self._merge_and_use).pack(side='left', padx=5)

    def _create_tab0c_data_manipulation(self):
        """Subtab 0C: Data Manipulation - Filter and transform data."""
        tab0c = ttk.Frame(self.data_mgmt_notebook, style='TFrame')
        self.data_mgmt_notebook.add(tab0c, text='  ✂️ Data Manipulation  ')

        # Create scrollable frame
        canvas = tk.Canvas(tab0c, background='#f0f0f0')
        scrollbar = ttk.Scrollbar(tab0c, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas, style='TFrame')

        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )

        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        content_frame = ttk.Frame(scrollable_frame, style='TFrame', padding="20 10 20 10")
        content_frame.pack(fill='both', expand=True)

        row = 0

        # === Section 1: Sample Filtering ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "🔍 Sample Filtering",
            expanded=True
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        filter_frame = ttk.LabelFrame(section_content, text="Filter Options", padding="10")
        filter_frame.pack(fill='x', pady=5)

        # Filter type
        ttk.Label(filter_frame, text="Filter Type:").grid(row=0, column=0, sticky='w', padx=5, pady=2)
        ttk.Radiobutton(filter_frame, text="Regex Pattern",
                       variable=self.filter_type_var, value='regex').grid(row=0, column=1, sticky='w', padx=5)
        ttk.Radiobutton(filter_frame, text="Value Range",
                       variable=self.filter_type_var, value='range').grid(row=0, column=2, sticky='w', padx=5)
        ttk.Radiobutton(filter_frame, text="Sample List",
                       variable=self.filter_type_var, value='list').grid(row=0, column=3, sticky='w', padx=5)

        # Filter column (for metadata filtering)
        ttk.Label(filter_frame, text="Column:").grid(row=1, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(filter_frame, textvariable=self.filter_column_var, width=30).grid(row=1, column=1, columnspan=2, padx=5, pady=2)

        # Filter value
        ttk.Label(filter_frame, text="Value/Pattern:").grid(row=2, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(filter_frame, textvariable=self.filter_value_var, width=30).grid(row=2, column=1, columnspan=2, padx=5, pady=2)

        ttk.Button(filter_frame, text="Apply Filter", command=self._apply_sample_filter).grid(row=3, column=1, pady=10)

        # === Section 2: Wavelength Operations ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "📏 Wavelength Operations",
            expanded=True
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        wl_frame = ttk.LabelFrame(section_content, text="Wavelength Trimming", padding="10")
        wl_frame.pack(fill='x', pady=5)

        # Min wavelength
        ttk.Label(wl_frame, text="Min Wavelength (nm):").grid(row=0, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(wl_frame, textvariable=self.min_wavelength_var, width=15).grid(row=0, column=1, padx=5, pady=2)

        # Max wavelength
        ttk.Label(wl_frame, text="Max Wavelength (nm):").grid(row=1, column=0, sticky='w', padx=5, pady=2)
        ttk.Entry(wl_frame, textvariable=self.max_wavelength_var, width=15).grid(row=1, column=1, padx=5, pady=2)

        ttk.Button(wl_frame, text="Trim Wavelengths", command=self._trim_wavelengths).grid(row=2, column=1, pady=10)

        # === Section 3: Export Options ===
        section, section_content = self._create_collapsible_section(
            content_frame,
            "💾 Export Options",
            expanded=False
        )
        section.grid(row=row, column=0, sticky='ew', pady=5)
        row += 1

        export_frame = ttk.Frame(section_content)
        export_frame.pack(fill='x', pady=5)

        ttk.Button(export_frame, text="Export to CSV", command=self._export_to_csv).pack(side='left', padx=5)
        ttk.Button(export_frame, text="Export to Excel", command=self._export_to_excel).pack(side='left', padx=5)

    def _create_tab0d_view_merged_data(self):
        """Subtab 0D: View & Edit Merged Data - Spreadsheet view with column management."""
        from tksheet import Sheet

        tab0d = ttk.Frame(self.data_mgmt_notebook, style='TFrame')
        self.data_mgmt_notebook.add(tab0d, text='  📊 View Merged Data  ')

        # Top controls
        control_frame = ttk.Frame(tab0d)
        control_frame.pack(fill='x', padx=10, pady=10)

        ttk.Button(control_frame, text="🔄 Load Merged Data",
                  command=self._load_merged_to_viewer).pack(side='left', padx=5)
        ttk.Button(control_frame, text="🗑️ Delete Selected Column",
                  command=self._delete_merged_column).pack(side='left', padx=5)
        ttk.Button(control_frame, text="💾 Save Changes",
                  command=self._save_merged_changes).pack(side='left', padx=5)
        ttk.Label(control_frame, text="Right-click column header to add column",
                 style='Caption.TLabel').pack(side='left', padx=15)

        # Info label
        self.merged_info_label = ttk.Label(tab0d, text="No merged data loaded", style='Caption.TLabel')
        self.merged_info_label.pack(padx=10, pady=5)

        # Sheet frame
        sheet_frame = ttk.Frame(tab0d)
        sheet_frame.pack(fill='both', expand=True, padx=10, pady=10)

        # Create tksheet
        self.merged_data_sheet = Sheet(
            sheet_frame,
            headers=[],
            data=[],
            theme='light blue',
            height=500
        )
        # Enable Excel-like bindings
        self.merged_data_sheet.enable_bindings(
            "single_select",
            "column_select",
            "row_select",
            "drag_select",
            "right_click_popup_menu",
            "rc_select",
            "copy",           # Ctrl+C to copy
            "paste",          # Ctrl+V to paste
            "cut",            # Ctrl+X to cut
            "delete",         # Delete key to clear cells
            "edit_cell",      # Double-click or F2 to edit
            "ctrl_click_select",  # Ctrl+click for multi-select
            "arrowkeys"       # Arrow keys to navigate
        )
        self.merged_data_sheet.pack(fill='both', expand=True)

        # Bind right-click on column headers
        self.merged_data_sheet.bind("<Button-3>", self._on_merged_sheet_right_click, add="+")

    def _create_tab1_import_preview(self):
        """Tab 1: Import & Preview - Data loading + spectral plots."""
        # Create main tab frame
        self.tab1 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab1, text='  📁 Import & Preview  ')

        # Create notebook for subtabs
        self.import_notebook = ttk.Notebook(self.tab1)
        self.import_notebook.pack(fill='both', expand=True)

        # Create subtabs
        self._create_tab1a_data()
        self._create_tab1b_plots()

    def _create_tab1a_data(self):
        """Subtab 1A: Data - Data loading and configuration."""
        # Create subtab frame with scrolling
        tab1a = ttk.Frame(self.import_notebook, style='TFrame')
        self.import_notebook.add(tab1a, text='  📂 Data  ')

        # Create canvas for scrolling
        canvas = tk.Canvas(tab1a, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab1a, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab1", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # === SECTION 1: Input Data ===
        self._create_section_header(content_frame, "1. Input Data", row=row, columnspan=3)
        row += 1

        # Create card for input data
        input_card_outer, input_card = self._create_card(content_frame, title="Data Files",
                                                          subtitle="Select spectral data directory and reference CSV")
        input_card_outer.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1

        # Inner frame for file inputs (to support grid layout within card)
        input_frame = tk.Frame(input_card, bg=self.colors['card_bg'])
        input_frame.pack(fill='both', expand=True)

        input_row = 0

        # Spectral data directory (unified input with auto-detection)
        ttk.Label(input_frame, text="Spectral File Directory:").grid(row=input_row, column=0, sticky=tk.W, pady=10)
        ttk.Entry(input_frame, textvariable=self.spectral_data_path, width=60).grid(row=input_row, column=1, padx=10)
        ttk.Button(input_frame, text="Browse...", command=self._browse_spectral_data, style='Modern.TButton').grid(row=input_row, column=2)
        input_row += 1

        # Detection status label
        self.detection_status = ttk.Label(input_frame, text="", style='Caption.TLabel')
        self.detection_status.grid(row=input_row, column=1, sticky=tk.W, padx=10, pady=(0, 10))
        input_row += 1

        # Reference file
        ttk.Label(input_frame, text="Reference CSV/Excel:").grid(row=input_row, column=0, sticky=tk.W, pady=10)
        ttk.Entry(input_frame, textvariable=self.reference_file, width=60).grid(row=input_row, column=1, padx=10)
        ttk.Button(input_frame, text="Browse...", command=self._browse_reference_file, style='Modern.TButton').grid(row=input_row, column=2)
        input_row += 1

        # === Load Data Button (moved up for easier access) ===
        self.load_button = self._create_button_with_gradient(content_frame, text="📊 Load Data & Generate Plots",
                                                              command=self._load_and_plot_data, style='accent')
        self.load_button.grid(row=row, column=0, columnspan=3, pady=(20, 10))
        row += 1

        # Status
        self.tab1_status = ttk.Label(content_frame, text="Ready to load data", style='Caption.TLabel')
        self.tab1_status.grid(row=row, column=0, columnspan=3, pady=(0, 20))
        row += 1

        # === SECTION 2: Advanced Configuration (Collapsible) ===
        self._create_section_header(content_frame, "2. Advanced Configuration (Optional)", row=row, columnspan=3)
        row += 1

        # Create card for all configuration options
        config_card_outer, config_card = self._create_card(content_frame, title="Data Configuration",
                                                           subtitle="Column mapping, wavelength range, and data type settings")
        config_card_outer.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1

        # Inner frame for all config options using grid layout
        config_frame = tk.Frame(config_card, bg=self.colors['card_bg'])
        config_frame.pack(fill='both', expand=True)

        cfg_row = 0

        # Column Mapping section
        ttk.Label(config_frame, text="Column Mapping:", style='Subheading.TLabel').grid(row=cfg_row, column=0, columnspan=4, sticky=tk.W, pady=(5, 10))
        cfg_row += 1

        # Use 2-column layout for compactness
        ttk.Label(config_frame, text="Spectral File:").grid(row=cfg_row, column=0, sticky=tk.W, pady=5, padx=(0, 5))
        self.spectral_file_combo = ttk.Combobox(config_frame, textvariable=self.spectral_file_column, width=25)
        self.spectral_file_combo.grid(row=cfg_row, column=1, sticky=tk.W, padx=5)

        ttk.Label(config_frame, text="Specimen ID:").grid(row=cfg_row, column=2, sticky=tk.W, pady=5, padx=(20, 5))
        self.id_combo = ttk.Combobox(config_frame, textvariable=self.id_column, width=25)
        self.id_combo.grid(row=cfg_row, column=3, sticky=tk.W, padx=5)
        cfg_row += 1

        ttk.Label(config_frame, text="Target Variable:").grid(row=cfg_row, column=0, sticky=tk.W, pady=5, padx=(0, 5))
        self.target_combo = ttk.Combobox(config_frame, textvariable=self.target_column, width=25)
        self.target_combo.grid(row=cfg_row, column=1, sticky=tk.W, padx=5)

        ttk.Button(config_frame, text="🔍 Auto-Detect", command=self._auto_detect_columns,
                  style='Modern.TButton').grid(row=cfg_row, column=2, columnspan=2, sticky=tk.W, padx=20)
        cfg_row += 1

        # Task Type
        ttk.Label(config_frame, text="Analysis Type:").grid(row=cfg_row, column=0, sticky=tk.W, pady=(15, 5), padx=(0, 5))
        task_type_subframe = ttk.Frame(config_frame)
        task_type_subframe.grid(row=cfg_row, column=1, columnspan=3, sticky=tk.W, pady=(15, 5))
        ttk.Radiobutton(task_type_subframe, text="Auto-detect", variable=self.task_type, value="auto").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(task_type_subframe, text="Regression", variable=self.task_type, value="regression").pack(side=tk.LEFT, padx=5)
        ttk.Radiobutton(task_type_subframe, text="Classification", variable=self.task_type, value="classification").pack(side=tk.LEFT, padx=5)
        self.task_type_detection_label = ttk.Label(task_type_subframe, text="", style='Caption.TLabel')
        self.task_type_detection_label.pack(side=tk.LEFT, padx=10)
        cfg_row += 1

        # Wavelength Range
        ttk.Label(config_frame, text="Wavelength Range:").grid(row=cfg_row, column=0, sticky=tk.W, pady=(15, 5), padx=(0, 5))
        wl_subframe = ttk.Frame(config_frame)
        wl_subframe.grid(row=cfg_row, column=1, columnspan=3, sticky=tk.W, pady=(15, 5))
        ttk.Entry(wl_subframe, textvariable=self.wavelength_min, width=12).pack(side=tk.LEFT, padx=2)
        ttk.Label(wl_subframe, text="to").pack(side=tk.LEFT, padx=5)
        ttk.Entry(wl_subframe, textvariable=self.wavelength_max, width=12).pack(side=tk.LEFT, padx=2)
        ttk.Label(wl_subframe, text="nm (auto-fills)", style='Caption.TLabel').pack(side=tk.LEFT, padx=10)
        self.update_wl_button = ttk.Button(wl_subframe, text="Update Plots", command=self._update_wavelengths,
                                          style='Modern.TButton', state='disabled')
        self.update_wl_button.pack(side=tk.LEFT, padx=5)
        cfg_row += 1

        # Wavelength recommendation
        ttk.Label(config_frame, text="💡 Tip: Import full spectrum for best preprocessing results. Refine wavelengths later in Analysis Configuration.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=cfg_row, column=0, columnspan=4, sticky=tk.W, pady=(5, 0))
        cfg_row += 1

        # Data Type
        ttk.Label(config_frame, text="Data Type:").grid(row=cfg_row, column=0, sticky=tk.W, pady=(15, 5), padx=(0, 5))
        data_type_subframe = ttk.Frame(config_frame)
        data_type_subframe.grid(row=cfg_row, column=1, columnspan=3, sticky=tk.W, pady=(15, 5))

        self.data_type_status_label = ttk.Label(data_type_subframe, text="No data loaded",
                                                style='Caption.TLabel', foreground=self.colors['text_light'])
        self.data_type_status_label.pack(side=tk.LEFT, padx=(0, 10))

        ttk.Radiobutton(data_type_subframe, text="Reflectance",
                       variable=self.current_data_type,
                       value="reflectance",
                       command=self._on_data_type_override,
                       state='disabled').pack(side=tk.LEFT, padx=5)

        ttk.Radiobutton(data_type_subframe, text="Absorbance",
                       variable=self.current_data_type,
                       value="absorbance",
                       command=self._on_data_type_override,
                       state='disabled').pack(side=tk.LEFT, padx=5)

        self.convert_data_button = ttk.Button(data_type_subframe, text="Convert to Absorbance",
                                             command=self._convert_and_replot,
                                             style='Modern.TButton',
                                             state='disabled')
        self.convert_data_button.pack(side=tk.LEFT, padx=10)

        # Keep legacy checkbox for backwards compatibility (hidden)
        self.absorbance_checkbox = ttk.Checkbutton(data_type_subframe, text="Convert to Absorbance (log10(1/R))",
                                                   variable=self.use_absorbance,
                                                   command=self._toggle_absorbance,
                                                   state='disabled')
        cfg_row += 1

        # Spectrum Selection
        ttk.Label(config_frame, text="Spectrum Selection:").grid(row=cfg_row, column=0, sticky=tk.W, pady=(15, 5), padx=(0, 5))
        exclusion_subframe = ttk.Frame(config_frame)
        exclusion_subframe.grid(row=cfg_row, column=1, columnspan=3, sticky=tk.W, pady=(15, 5))

        self.reset_exclusions_button = ttk.Button(exclusion_subframe, text="Reset Exclusions",
                                                 command=self._reset_exclusions,
                                                 style='Modern.TButton',
                                                 state='disabled')
        self.reset_exclusions_button.pack(side=tk.LEFT, padx=(0, 10))

        self.exclusion_status = ttk.Label(exclusion_subframe, text="No spectra excluded", style='Caption.TLabel')
        self.exclusion_status.pack(side=tk.LEFT, padx=5)

        ttk.Label(exclusion_subframe, text="(Click spectra in plots to toggle)",
                 style='Caption.TLabel').pack(side=tk.LEFT, padx=10)

    def _create_tab1b_plots(self):
        """Subtab 1B: Plots - Spectral visualization."""
        # Create subtab frame
        tab1b = ttk.Frame(self.import_notebook, style='TFrame')
        self.import_notebook.add(tab1b, text='  📊 Plots  ')

        # Main container with padding
        content_frame = ttk.Frame(tab1b, style='TFrame', padding="30")
        content_frame.pack(fill='both', expand=True)

        # Create sub-notebook for plots
        self.plot_notebook = ttk.Notebook(content_frame)
        self.plot_notebook.pack(fill='both', expand=True, pady=10)

        # Placeholder for plots
        placeholder = ttk.Frame(self.plot_notebook)
        self.plot_notebook.add(placeholder, text="Load data to see plots")
        ttk.Label(placeholder, text="Load data in the Data subtab to generate spectral plots",
                 style='Caption.TLabel').pack(expand=True)

    def _create_tab2_data_viewer(self):
        """Tab 2: Data Viewer - Excel-like spreadsheet view with smooth scrolling."""
        from tksheet import Sheet

        self.tab2 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab2, text='  📋 Data Viewer  ')

        content_frame = ttk.Frame(self.tab2, style='TFrame', padding="30")
        content_frame.pack(fill='both', expand=True)

        # Control panel
        control_frame = ttk.Frame(content_frame)
        control_frame.pack(fill='x', pady=(0, 10))

        # Left side controls
        self.show_excluded_data_viewer = tk.BooleanVar(value=True)
        ttk.Checkbutton(control_frame, text="Show excluded samples",
                       variable=self.show_excluded_data_viewer,
                       command=self._populate_data_viewer).pack(side='left', padx=(0, 10))

        # Right side controls
        ttk.Button(control_frame, text="📥 Export All Data to CSV",
                  command=self._export_data_viewer_to_csv,
                  style='Modern.TButton').pack(side='right', padx=(5, 0))

        # Status bar
        self.data_viewer_status = ttk.Label(content_frame, text="", style='Caption.TLabel')
        self.data_viewer_status.pack(fill='x', pady=(0, 5))

        # Create frame for tksheet
        sheet_frame = ttk.Frame(content_frame)
        sheet_frame.pack(fill='both', expand=True)

        # Create tksheet (Excel-like table with virtual scrolling)
        self.data_viewer_sheet = Sheet(
            sheet_frame,
            data=[],
            headers=[],
            show_row_index=True,
            show_header=True,
            show_top_left=True,
            empty_horizontal=0,
            empty_vertical=0,
            height=600,
            width=1200,
        )

        # Enable Excel-like interactions
        self.data_viewer_sheet.enable_bindings(
            "single_select",
            "row_select",
            "column_width_resize",
            "double_click_column_resize",
            "arrowkeys",
            "right_click_popup_menu",
            "rc_select",
            "copy",
        )

        self.data_viewer_sheet.grid(row=0, column=0, sticky='nsew')
        sheet_frame.grid_rowconfigure(0, weight=1)
        sheet_frame.grid_columnconfigure(0, weight=1)

        # Info label at bottom
        self.data_viewer_info = ttk.Label(content_frame,
            text="Load data in the Import & Preview tab to view it here.",
            style='Caption.TLabel')
        self.data_viewer_info.pack(pady=(10, 0))


    def _create_tab3_data_quality_check(self):
        """Tab 3: Data Quality Check - Outlier detection and exclusion."""
        self.tab3 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab3, text='  🔍 Data Quality Check  ')

        # Create scrollable content
        canvas = tk.Canvas(self.tab3, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(self.tab3, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab3", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # === SECTION 1: Controls ===
        ttk.Label(content_frame, text="1. Outlier Detection Parameters", style='Heading.TLabel').grid(row=row, column=0, columnspan=3, sticky=tk.W, pady=(0, 15))
        row += 1

        controls_frame = ttk.LabelFrame(content_frame, text="Detection Settings", padding="20")
        controls_frame.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # PCA Components
        ttk.Label(controls_frame, text="PCA Components:").grid(row=0, column=0, sticky=tk.W, pady=8, padx=(0, 10))
        ttk.Spinbox(controls_frame, from_=2, to=20, textvariable=self.n_pca_components, width=12).grid(row=0, column=1, sticky=tk.W)
        ttk.Label(controls_frame, text="Number of PCs for outlier detection", style='Caption.TLabel').grid(row=0, column=2, sticky=tk.W, padx=10)

        # Y Range (optional)
        ttk.Label(controls_frame, text="Y Range (optional):").grid(row=1, column=0, sticky=tk.W, pady=8, padx=(0, 10))
        y_range_frame = ttk.Frame(controls_frame)
        y_range_frame.grid(row=1, column=1, columnspan=2, sticky=tk.W)
        ttk.Label(y_range_frame, text="Min:").pack(side='left', padx=(0, 5))
        self.y_min_entry = ttk.Entry(y_range_frame, textvariable=self.y_min_bound, width=10)
        self.y_min_entry.pack(side='left', padx=(0, 10))
        ttk.Label(y_range_frame, text="Max:").pack(side='left', padx=(0, 5))
        self.y_max_entry = ttk.Entry(y_range_frame, textvariable=self.y_max_bound, width=10)
        self.y_max_entry.pack(side='left')

        # Buttons
        button_frame = ttk.Frame(controls_frame)
        button_frame.grid(row=2, column=0, columnspan=3, pady=20)
        self.run_outlier_btn = self._create_accent_button(button_frame, text="Run Outlier Detection",
                                                           command=self._run_outlier_detection)
        self.run_outlier_btn.pack(side='left', padx=5)
        self.export_report_btn = ttk.Button(button_frame, text="Export Report",
                                           command=self._export_outlier_report, style='Modern.TButton')
        self.export_report_btn.pack(side='left', padx=5)

        # === SECTION 2: Visualizations ===
        ttk.Label(content_frame, text="2. Outlier Detection Plots", style='Heading.TLabel').grid(row=row, column=0, columnspan=3, sticky=tk.W, pady=(25, 15))
        row += 1

        # Create notebook for visualization tabs
        self.outlier_plot_notebook = ttk.Notebook(content_frame)
        self.outlier_plot_notebook.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=10)
        content_frame.grid_rowconfigure(row, weight=1)
        row += 1

        # Create placeholder frames for each plot type
        self.pca_plot_frame = ttk.Frame(self.outlier_plot_notebook)
        self.outlier_plot_notebook.add(self.pca_plot_frame, text="PCA Scores")

        self.t2_plot_frame = ttk.Frame(self.outlier_plot_notebook)
        self.outlier_plot_notebook.add(self.t2_plot_frame, text="Hotelling T²")

        self.q_plot_frame = ttk.Frame(self.outlier_plot_notebook)
        self.outlier_plot_notebook.add(self.q_plot_frame, text="Q-Residuals")

        self.maha_plot_frame = ttk.Frame(self.outlier_plot_notebook)
        self.outlier_plot_notebook.add(self.maha_plot_frame, text="Mahalanobis")

        self.y_dist_plot_frame = ttk.Frame(self.outlier_plot_notebook)
        self.outlier_plot_notebook.add(self.y_dist_plot_frame, text="Y Distribution")

        # === SECTION 2.5: Class/Target Distribution Analysis ===
        ttk.Label(content_frame, text="2.5 Class/Target Distribution Analysis", style='Heading.TLabel').grid(row=row, column=0, columnspan=3, sticky=tk.W, pady=(25, 15))
        row += 1

        # Distribution info frame
        dist_frame = ttk.LabelFrame(content_frame, text="Distribution Analysis", padding="20")
        dist_frame.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Distribution summary label
        self.dist_summary_label = ttk.Label(dist_frame, text="Run analysis to see class/target distribution",
                                           style='Caption.TLabel', wraplength=800, justify='left')
        self.dist_summary_label.grid(row=0, column=0, columnspan=3, sticky=tk.W, pady=10)

        # Warning label (hidden by default)
        self.imbalance_warning_label = ttk.Label(dist_frame, text="",
                                                 foreground='#ff6b6b', font=('Segoe UI', 10, 'bold'),
                                                 wraplength=800, justify='left')
        self.imbalance_warning_label.grid(row=1, column=0, columnspan=3, sticky=tk.W, pady=5)

        # Recommendation label
        self.imbalance_recommendation_label = ttk.Label(dist_frame, text="",
                                                        style='Caption.TLabel', wraplength=800, justify='left')
        self.imbalance_recommendation_label.grid(row=2, column=0, columnspan=3, sticky=tk.W, pady=5)

        # === SECTION 2.6: Imbalance Handling (Optional) ===
        ttk.Label(content_frame, text="2.6 Imbalance Handling (Optional)", style='Heading.TLabel').grid(row=row, column=0, columnspan=3, sticky=tk.W, pady=(25, 15))
        row += 1

        imbalance_frame = ttk.LabelFrame(content_frame, text="Imbalance Handling Settings", padding="20")
        imbalance_frame.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Enable checkbox
        self.enable_imbalance_handling = tk.BooleanVar(value=False)
        ttk.Checkbutton(imbalance_frame, text="Enable imbalance handling",
                       variable=self.enable_imbalance_handling,
                       command=self._toggle_imbalance_controls).grid(row=0, column=0, columnspan=3, sticky=tk.W, pady=10)

        # Method selection
        ttk.Label(imbalance_frame, text="Method:").grid(row=1, column=0, sticky=tk.W, pady=8, padx=(20, 10))
        self.imbalance_method = tk.StringVar(value="smote")
        self.imbalance_method_combo = ttk.Combobox(imbalance_frame, textvariable=self.imbalance_method,
                                                   state='disabled', width=25)
        self.imbalance_method_combo['values'] = [
            'smote', 'adasyn', 'borderline_smote', 'random_undersampler',
            'tomek_links', 'smote_tomek', 'class_weight',
            'binning', 'rare_boost', 'balanced'
        ]
        self.imbalance_method_combo.grid(row=1, column=1, sticky=tk.W, pady=8)
        self.imbalance_method_combo.bind('<<ComboboxSelected>>', self._update_imbalance_method_description)

        # Method description
        self.imbalance_method_desc = ttk.Label(imbalance_frame, text="SMOTE - Synthetic oversampling (standard)",
                                              style='Caption.TLabel', wraplength=400, justify='left')
        self.imbalance_method_desc.grid(row=1, column=2, sticky=tk.W, padx=10)

        # Parameters frame (method-specific)
        params_subframe = ttk.Frame(imbalance_frame)
        params_subframe.grid(row=2, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10, padx=(20, 0))

        # SMOTE/ADASYN parameters
        self.k_neighbors_label = ttk.Label(params_subframe, text="k_neighbors:")
        self.k_neighbors_label.grid(row=0, column=0, sticky=tk.W, pady=5, padx=(0, 10))
        self.k_neighbors = tk.IntVar(value=5)
        self.k_neighbors_spin = ttk.Spinbox(params_subframe, from_=1, to=20, textvariable=self.k_neighbors,
                                           width=10, state='disabled')
        self.k_neighbors_spin.grid(row=0, column=1, sticky=tk.W)
        self.k_neighbors_desc = ttk.Label(params_subframe, text="Number of nearest neighbors for SMOTE",
                                         style='Caption.TLabel')
        self.k_neighbors_desc.grid(row=0, column=2, sticky=tk.W, padx=10)

        # Binning parameters (for regression)
        self.n_bins_label = ttk.Label(params_subframe, text="n_bins:")
        self.n_bins = tk.IntVar(value=5)
        self.n_bins_spin = ttk.Spinbox(params_subframe, from_=3, to=10, textvariable=self.n_bins,
                                      width=10, state='disabled')

        # Boost factor (for rare_boost)
        self.boost_factor_label = ttk.Label(params_subframe, text="boost_factor:")
        self.boost_factor = tk.DoubleVar(value=2.0)
        self.boost_factor_spin = ttk.Spinbox(params_subframe, from_=1.1, to=5.0, increment=0.1,
                                            textvariable=self.boost_factor, width=10, state='disabled')

        # Store references for easy access
        self.imbalance_widgets = {
            'method_combo': self.imbalance_method_combo,
            'k_neighbors_label': self.k_neighbors_label,
            'k_neighbors_spin': self.k_neighbors_spin,
            'k_neighbors_desc': self.k_neighbors_desc,
            'n_bins_label': self.n_bins_label,
            'n_bins_spin': self.n_bins_spin,
            'boost_factor_label': self.boost_factor_label,
            'boost_factor_spin': self.boost_factor_spin
        }

        # === SECTION 3: Summary Table ===
        ttk.Label(content_frame, text="3. Outlier Summary", style='Heading.TLabel').grid(row=row, column=0, columnspan=3, sticky=tk.W, pady=(25, 15))
        row += 1

        # Create frame for treeview
        tree_frame = ttk.Frame(content_frame)
        tree_frame.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E, tk.N, tk.S), pady=10)
        content_frame.grid_rowconfigure(row, weight=1)
        row += 1

        # Create Treeview with scrollbars
        columns = ('Sample', 'Y_Value', 'T2', 'Q', 'Maha', 'Y', 'Flags')
        self.outlier_tree = ttk.Treeview(tree_frame, columns=columns, show='headings', selectmode='extended', height=10)

        # Define column headings
        self.outlier_tree.heading('Sample', text='Sample')
        self.outlier_tree.heading('Y_Value', text='Y Value')
        self.outlier_tree.heading('T2', text='T² Outlier')
        self.outlier_tree.heading('Q', text='Q Outlier')
        self.outlier_tree.heading('Maha', text='Maha Outlier')
        self.outlier_tree.heading('Y', text='Y Outlier')
        self.outlier_tree.heading('Flags', text='Total Flags')

        # Configure column widths
        self.outlier_tree.column('Sample', width=80, anchor='center')
        self.outlier_tree.column('Y_Value', width=100, anchor='center')
        self.outlier_tree.column('T2', width=80, anchor='center')
        self.outlier_tree.column('Q', width=80, anchor='center')
        self.outlier_tree.column('Maha', width=80, anchor='center')
        self.outlier_tree.column('Y', width=80, anchor='center')
        self.outlier_tree.column('Flags', width=100, anchor='center')

        # Add scrollbars
        vsb = ttk.Scrollbar(tree_frame, orient="vertical", command=self.outlier_tree.yview)
        hsb = ttk.Scrollbar(tree_frame, orient="horizontal", command=self.outlier_tree.xview)
        self.outlier_tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)

        self.outlier_tree.grid(row=0, column=0, sticky='nsew')
        vsb.grid(row=0, column=1, sticky='ns')
        hsb.grid(row=1, column=0, sticky='ew')

        tree_frame.grid_rowconfigure(0, weight=1)
        tree_frame.grid_columnconfigure(0, weight=1)

        # === SECTION 4: Selection Controls ===
        ttk.Label(content_frame, text="4. Sample Selection & Exclusion", style='Heading.TLabel').grid(row=row, column=0, columnspan=3, sticky=tk.W, pady=(25, 15))
        row += 1

        selection_frame = ttk.LabelFrame(content_frame, text="Auto-Select Outliers", padding="20")
        selection_frame.grid(row=row, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Selection checkboxes
        self.select_all_flagged = tk.BooleanVar(value=False)
        self.select_high_conf = tk.BooleanVar(value=True)
        self.select_moderate_conf = tk.BooleanVar(value=False)

        ttk.Checkbutton(selection_frame, text="Select all flagged samples",
                       variable=self.select_all_flagged,
                       command=self._auto_select_flagged).grid(row=0, column=0, sticky=tk.W, pady=5)
        ttk.Checkbutton(selection_frame, text="High confidence (3+ flags)",
                       variable=self.select_high_conf,
                       command=self._auto_select_high_confidence).grid(row=1, column=0, sticky=tk.W, pady=5)
        ttk.Checkbutton(selection_frame, text="Moderate confidence (2 flags)",
                       variable=self.select_moderate_conf,
                       command=self._auto_select_moderate_confidence).grid(row=2, column=0, sticky=tk.W, pady=5)

        # Status and action buttons
        self.outlier_selection_status = ttk.Label(selection_frame, text="No samples selected", style='Caption.TLabel')
        self.outlier_selection_status.grid(row=3, column=0, pady=10)

        self._create_accent_button(selection_frame, "Mark Selected for Exclusion",
                                    self._mark_selected_for_exclusion).grid(row=4, column=0, pady=10)

        # Overall status
        self.tab2_status = ttk.Label(content_frame, text="Load data and run outlier detection to begin", style='Caption.TLabel')
        self.tab2_status.grid(row=row, column=0, columnspan=3)

    def _create_tab4_analysis_config(self):
        """Tab 4: Analysis Configuration - Organized into 5 subtabs for better navigation."""
        self.tab4 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab4, text='  ⚙️ Analysis Configuration  ')

        # Create nested notebook for subtabs
        self.config_notebook = ttk.Notebook(self.tab4)
        self.config_notebook.pack(fill='both', expand=True, padx=20, pady=(0, 20))

        # Create the 5 subtabs
        self._create_tab4a_basic_settings()
        self._create_tab4b_variable_selection()
        self._create_tab4c_model_configuration()
        self._create_tab4d_ensemble_methods()
        self._create_tab4e_validation()

    def _create_tab4a_basic_settings(self):
        """Subtab 4A: Basic Settings - CV folds, penalties, and preprocessing."""
        tab4a = ttk.Frame(self.config_notebook, style='TFrame')
        self.config_notebook.add(tab4a, text='  ⚙️ Basic Settings  ')

        # Create scrollable content
        canvas = tk.Canvas(tab4a, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab4a, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab4a", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Run Analysis button at the top
        run_frame = tk.Frame(content_frame, bg=self.colors['bg'])
        run_frame.grid(row=row, column=0, columnspan=2, sticky='ew', pady=(0, 20))
        self._create_accent_button(run_frame, "▶ Run Analysis", self._run_analysis).pack(side='left')
        row += 1

        # === Analysis Options ===
        self._create_section_header(content_frame, "Analysis Options", row=row, columnspan=2)
        row += 1

        # Create card for general settings
        options_card_outer, options_card = self._create_card(content_frame, title="General Settings",
                                                               subtitle="Configure cross-validation, penalties, and output settings")
        options_card_outer.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1
        # Inner frame for grid layout within card
        options_frame = tk.Frame(options_card, bg=self.colors['card_bg'])
        options_frame.pack(fill='both', expand=True)

        # CV Folds
        ttk.Label(options_frame, text="CV Folds:").grid(row=0, column=0, sticky=tk.W, pady=8, padx=(0, 10))
        ttk.Spinbox(options_frame, from_=3, to=10, textvariable=self.folds, width=12).grid(row=0, column=1, sticky=tk.W)

        # NEW: Variable Count Penalty (0-10 scale)
        var_penalty_label = ttk.Label(options_frame, text="Variable Count Penalty (0-10):", style='Subheading.TLabel')
        var_penalty_label.grid(row=1, column=0, sticky=tk.W, pady=(15, 5), padx=(0, 10))
        CreateToolTip(var_penalty_label, text=TOOLTIP_CONTENT['ranking']['variable_penalty'], delay=500)
        var_penalty_spinbox = ttk.Spinbox(options_frame, from_=0, to=10, textvariable=self.variable_penalty, width=10)
        var_penalty_spinbox.grid(row=2, column=0, sticky=tk.W, padx=(0, 10))
        CreateToolTip(var_penalty_spinbox, text=TOOLTIP_CONTENT['ranking']['variable_penalty'], delay=500)

        # NEW: Model Complexity Penalty (0-10 scale)
        comp_penalty_label = ttk.Label(options_frame, text="Model Complexity Penalty (0-10):", style='Subheading.TLabel')
        comp_penalty_label.grid(row=3, column=0, sticky=tk.W, pady=(15, 5), padx=(0, 10))
        CreateToolTip(comp_penalty_label, text=TOOLTIP_CONTENT['ranking']['complexity_penalty'], delay=500)
        comp_penalty_spinbox = ttk.Spinbox(options_frame, from_=0, to=10, textvariable=self.complexity_penalty, width=10)
        comp_penalty_spinbox.grid(row=4, column=0, sticky=tk.W, padx=(0, 10))
        CreateToolTip(comp_penalty_spinbox, text=TOOLTIP_CONTENT['ranking']['complexity_penalty'], delay=500)

        # Info label explaining the penalty system
        ttk.Label(options_frame, text="💡 Penalties affect ranking gently at low values (exploration-friendly). 0 = rank only by performance, 5 = balanced, 10 = strongly prefer simplicity",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=5, column=0, columnspan=3, sticky=tk.W, pady=(10, 0))

        # === Wavelength Restriction for Analysis ===
        ttk.Separator(options_frame, orient='horizontal').grid(row=6, column=0, columnspan=3, sticky='ew', pady=(20, 10))

        # Enable/disable checkbox
        self.wl_restrict_checkbox = ttk.Checkbutton(options_frame,
                                                     text="✓ Restrict wavelengths for model training",
                                                     variable=self.enable_analysis_wl_restriction)
        self.wl_restrict_checkbox.grid(row=7, column=0, columnspan=3, sticky=tk.W, pady=(0, 5))

        # Help text explaining the difference from import filter
        ttk.Label(options_frame,
                 text="Further restrict wavelength range for analysis only (does not affect data import or plots)",
                 style='Caption.TLabel', foreground=self.colors['text_light']).grid(row=8, column=0, columnspan=3, sticky=tk.W, padx=(20, 0))

        # Wavelength range inputs
        wl_restrict_subframe = ttk.Frame(options_frame)
        wl_restrict_subframe.grid(row=9, column=0, columnspan=3, sticky=tk.W, pady=(8, 0), padx=(20, 0))
        ttk.Label(wl_restrict_subframe, text="Analysis Range:").pack(side=tk.LEFT, padx=(0, 5))
        ttk.Entry(wl_restrict_subframe, textvariable=self.analysis_wl_min, width=12).pack(side=tk.LEFT, padx=2)
        ttk.Label(wl_restrict_subframe, text="to").pack(side=tk.LEFT, padx=5)
        ttk.Entry(wl_restrict_subframe, textvariable=self.analysis_wl_max, width=12).pack(side=tk.LEFT, padx=2)
        ttk.Label(wl_restrict_subframe, text="nm", style='Caption.TLabel').pack(side=tk.LEFT, padx=5)

        # Auto-select checkbox when both range values are entered
        def auto_check_wavelength_restriction(*args):
            """Auto-check the restrict wavelengths checkbox when both min and max are entered."""
            try:
                min_val = self.analysis_wl_min.get().strip()
                max_val = self.analysis_wl_max.get().strip()

                # Only auto-check if both fields have numeric values
                if min_val and max_val:
                    # Try to convert to float to ensure they're numeric
                    float(min_val)
                    float(max_val)
                    # Auto-check the checkbox
                    self.enable_analysis_wl_restriction.set(True)
            except (ValueError, AttributeError):
                # If conversion fails or variables not initialized, do nothing
                pass

        # Add trace callbacks to both min and max variables
        self.analysis_wl_min.trace_add('write', auto_check_wavelength_restriction)
        self.analysis_wl_max.trace_add('write', auto_check_wavelength_restriction)

        # Preset buttons for common ranges
        preset_frame = ttk.Frame(options_frame)
        preset_frame.grid(row=10, column=0, columnspan=3, sticky=tk.W, pady=(5, 0), padx=(20, 0))
        ttk.Label(preset_frame, text="Presets:", style='Caption.TLabel').pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(preset_frame, text="UV (10-400)",
                  command=lambda: self._set_analysis_wl_preset(10, 400),
                  style='Modern.TButton').pack(side=tk.LEFT, padx=2)
        ttk.Button(preset_frame, text="VIS (400-800)",
                  command=lambda: self._set_analysis_wl_preset(400, 800),
                  style='Modern.TButton').pack(side=tk.LEFT, padx=2)
        ttk.Button(preset_frame, text="NIR (800-2500)",
                  command=lambda: self._set_analysis_wl_preset(800, 2500),
                  style='Modern.TButton').pack(side=tk.LEFT, padx=2)
        ttk.Button(preset_frame, text="MIR (2500-25000)",
                  command=lambda: self._set_analysis_wl_preset(2500, 25000),
                  style='Modern.TButton').pack(side=tk.LEFT, padx=2)

        # Performance note
        ttk.Label(options_frame,
                 text="💡 Tip: Restricting wavelengths speeds up training (fewer features = faster models)",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=11, column=0, columnspan=3, sticky=tk.W, pady=(10, 0), padx=(20, 0))

        ttk.Separator(options_frame, orient='horizontal').grid(row=12, column=0, columnspan=3, sticky='ew', pady=(15, 10))

        # Output directory
        ttk.Label(options_frame, text="Output Directory:").grid(row=13, column=0, sticky=tk.W, pady=(0, 8), padx=(0, 10))
        ttk.Entry(options_frame, textvariable=self.output_dir, width=25).grid(row=13, column=1, sticky=tk.W)

        # Progress monitor
        ttk.Checkbutton(options_frame, text="Show live progress monitor", variable=self.show_progress).grid(row=14, column=0, columnspan=3, sticky=tk.W, pady=10)

        # === Preprocessing Methods ===
        self._create_section_header(content_frame, "Preprocessing Methods", row=row, columnspan=2)
        row += 1

        # Create card for preprocessing
        preprocess_card_outer, preprocess_card = self._create_card(content_frame, title="Select Preprocessing",
                                                                     subtitle="Choose spectral preprocessing techniques")
        preprocess_card_outer.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1
        # Inner frame for grid layout within card
        preprocess_frame = tk.Frame(preprocess_card, bg=self.colors['card_bg'])
        preprocess_frame.pack(fill='both', expand=True)

        self.raw_checkbox = ttk.Checkbutton(preprocess_frame, text="✓ Raw (no preprocessing)", variable=self.use_raw)
        self.raw_checkbox.grid(row=0, column=0, sticky=tk.W, pady=5)
        ttk.Label(preprocess_frame, text="Baseline, unprocessed spectra", style='Caption.TLabel').grid(row=0, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.raw_checkbox, text=TOOLTIP_CONTENT['preprocessing']['Raw'], delay=500)

        self.snv_checkbox = ttk.Checkbutton(preprocess_frame, text="✓ SNV (Standard Normal Variate)", variable=self.use_snv)
        self.snv_checkbox.grid(row=1, column=0, sticky=tk.W, pady=5)
        ttk.Label(preprocess_frame, text="Scatter correction", style='Caption.TLabel').grid(row=1, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.snv_checkbox, text=TOOLTIP_CONTENT['preprocessing']['SNV'], delay=500)

        self.sg1_checkbox = ttk.Checkbutton(preprocess_frame, text="✓ SG1 (1st derivative)", variable=self.use_sg1)
        self.sg1_checkbox.grid(row=2, column=0, sticky=tk.W, pady=5)
        ttk.Label(preprocess_frame, text="Removes baseline drift", style='Caption.TLabel').grid(row=2, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.sg1_checkbox, text=TOOLTIP_CONTENT['preprocessing']['SG1'], delay=500)

        self.sg2_checkbox = ttk.Checkbutton(preprocess_frame, text="✓ SG2 (2nd derivative)", variable=self.use_sg2)
        self.sg2_checkbox.grid(row=3, column=0, sticky=tk.W, pady=5)
        ttk.Label(preprocess_frame, text="Peak enhancement", style='Caption.TLabel').grid(row=3, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.sg2_checkbox, text=TOOLTIP_CONTENT['preprocessing']['SG2'], delay=500)

        # Advanced: deriv_snv option
        self.deriv_snv_checkbox = ttk.Checkbutton(preprocess_frame, text="deriv_snv (advanced)", variable=self.use_deriv_snv)
        self.deriv_snv_checkbox.grid(row=4, column=0, sticky=tk.W, pady=5)
        ttk.Label(preprocess_frame, text="Derivative then SNV (less common)", style='Caption.TLabel').grid(row=4, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.deriv_snv_checkbox, text=TOOLTIP_CONTENT['preprocessing']['deriv_snv'], delay=500)

        # Derivative window size settings
        window_size_label = ttk.Label(preprocess_frame, text="Derivative Window Sizes:", style='Subheading.TLabel')
        window_size_label.grid(row=6, column=0, columnspan=2, sticky=tk.W, pady=(15, 5))
        CreateToolTip(window_size_label, text=TOOLTIP_CONTENT['preprocessing']['window_size'], delay=500)
        ttk.Label(preprocess_frame, text="Select one or more (default: 17 only)", style='Caption.TLabel').grid(row=7, column=0, columnspan=2, sticky=tk.W)

        window_frame = ttk.Frame(preprocess_frame)
        window_frame.grid(row=8, column=0, columnspan=2, sticky=tk.W, pady=5)

        ttk.Checkbutton(window_frame, text="Window=7", variable=self.window_7).grid(row=0, column=0, padx=5, pady=2)
        ttk.Checkbutton(window_frame, text="Window=11", variable=self.window_11).grid(row=0, column=1, padx=5, pady=2)
        ttk.Checkbutton(window_frame, text="Window=17 ⭐", variable=self.window_17).grid(row=0, column=2, padx=5, pady=2)
        ttk.Checkbutton(window_frame, text="Window=19", variable=self.window_19).grid(row=0, column=3, padx=5, pady=2)
        ttk.Label(window_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5), pady=2)
        ttk.Entry(window_frame, textvariable=self.window_custom, width=10).grid(row=0, column=5, padx=5, pady=2)
        ttk.Label(window_frame, text="(comma-separated, e.g., 13,15,21)", style='Caption.TLabel').grid(row=0, column=6, padx=5, pady=2)

    def _create_tab4b_variable_selection(self):
        """Subtab 4B: Variable Selection - Subset analysis and variable selection methods."""
        tab4b = ttk.Frame(self.config_notebook, style='TFrame')
        self.config_notebook.add(tab4b, text='  🎯 Variable Selection  ')

        # Create scrollable content
        canvas = tk.Canvas(tab4b, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab4b, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab4b", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Run Analysis button at the top
        run_frame = tk.Frame(content_frame, bg=self.colors['bg'])
        run_frame.grid(row=row, column=0, columnspan=2, sticky='ew', pady=(0, 20))
        self._create_accent_button(run_frame, "▶ Run Analysis", self._run_analysis).pack(side='left')
        row += 1

        # === Subset Analysis ===
        self._create_section_header(content_frame, "Subset Analysis", row=row, columnspan=2)
        row += 1

        # Create card for subset analysis
        subset_card_outer, subset_card = self._create_card(content_frame, title="Variable & Region Subsets",
                                                             subtitle="Test models using variable and region subsets")
        subset_card_outer.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1
        # Inner frame for grid layout within card
        subset_frame = tk.Frame(subset_card, bg=self.colors['card_bg'])
        subset_frame.pack(fill='both', expand=True)

        # Enable/disable toggles
        ttk.Checkbutton(subset_frame, text="✓ Enable Top-N Variable Analysis", variable=self.enable_variable_subsets).grid(row=0, column=0, columnspan=4, sticky=tk.W, pady=5)
        ttk.Label(subset_frame, text="Test models using only the N most important wavelengths", style='Caption.TLabel').grid(row=1, column=0, columnspan=4, sticky=tk.W, padx=(20, 0))

        ttk.Checkbutton(subset_frame, text="✓ Enable Spectral Region Analysis", variable=self.enable_region_subsets).grid(row=2, column=0, columnspan=4, sticky=tk.W, pady=(10, 5))
        ttk.Label(subset_frame, text="Test models using auto-detected spectral regions (e.g., 2000-2050nm)", style='Caption.TLabel').grid(row=3, column=0, columnspan=4, sticky=tk.W, padx=(20, 0))

        # Region analysis depth (radio buttons)
        ttk.Label(subset_frame, text="Region Analysis Depth:", style='Caption.TLabel').grid(row=4, column=0, columnspan=4, sticky=tk.W, padx=(20, 0), pady=(5, 2))
        region_depth_frame = ttk.Frame(subset_frame)
        region_depth_frame.grid(row=5, column=0, columnspan=4, sticky=tk.W, padx=(20, 0), pady=(0, 5))
        ttk.Radiobutton(region_depth_frame, text="Shallow (5 regions)", variable=self.n_top_regions, value=5).grid(row=0, column=0, padx=5)
        ttk.Radiobutton(region_depth_frame, text="Medium (10 regions)", variable=self.n_top_regions, value=10).grid(row=0, column=1, padx=5)
        ttk.Radiobutton(region_depth_frame, text="Deep (15 regions)", variable=self.n_top_regions, value=15).grid(row=0, column=2, padx=5)
        ttk.Radiobutton(region_depth_frame, text="Thorough (20 regions)", variable=self.n_top_regions, value=20).grid(row=0, column=3, padx=5)

        # Top-N variable counts
        ttk.Label(subset_frame, text="Top-N Variable Counts:", style='Subheading.TLabel').grid(row=6, column=0, columnspan=4, sticky=tk.W, pady=(15, 5))
        ttk.Label(subset_frame, text="Select which N values to test (default: 10, 20, 50, 100, 250)", style='Caption.TLabel').grid(row=7, column=0, columnspan=4, sticky=tk.W)

        var_frame = ttk.Frame(subset_frame)
        var_frame.grid(row=8, column=0, columnspan=4, sticky=tk.W, pady=5)

        ttk.Checkbutton(var_frame, text="N=10 ⭐", variable=self.var_10).grid(row=0, column=0, padx=5, pady=2)
        ttk.Checkbutton(var_frame, text="N=20 ⭐", variable=self.var_20).grid(row=0, column=1, padx=5, pady=2)
        ttk.Checkbutton(var_frame, text="N=50 ⭐", variable=self.var_50).grid(row=0, column=2, padx=5, pady=2)
        ttk.Checkbutton(var_frame, text="N=100 ⭐", variable=self.var_100).grid(row=0, column=3, padx=5, pady=2)
        ttk.Checkbutton(var_frame, text="N=250 ⭐", variable=self.var_250).grid(row=1, column=0, padx=5, pady=2)
        ttk.Checkbutton(var_frame, text="N=500", variable=self.var_500).grid(row=1, column=1, padx=5, pady=2)
        ttk.Checkbutton(var_frame, text="N=1000", variable=self.var_1000).grid(row=1, column=2, padx=5, pady=2)

        ttk.Label(subset_frame, text="💡 More subsets = more comprehensive results but longer runtime",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=7, column=0, columnspan=4, sticky=tk.W, pady=(10, 0))

        # === Variable Selection Methods ===
        self._create_section_header(content_frame, "Variable Selection Methods 🆕", row=row, columnspan=2)
        row += 1

        # Create card for variable selection
        varsel_card_outer, varsel_card = self._create_card(content_frame, title="Advanced Variable Selection",
                                                             subtitle="Sophisticated wavelength selection techniques")
        varsel_card_outer.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1
        # Inner frame for grid layout within card
        varsel_frame = tk.Frame(varsel_card, bg=self.colors['card_bg'])
        varsel_frame.pack(fill='both', expand=True)

        # Method selection (checkboxes - multiple selection enabled)
        ttk.Label(varsel_frame, text="Selection Methods (select one or more):", style='Subheading.TLabel').grid(row=0, column=0, columnspan=2, sticky=tk.W, pady=(0, 8))

        ttk.Checkbutton(varsel_frame, text="Feature Importance (default)",
                       variable=self.varsel_importance).grid(row=1, column=0, sticky=tk.W, pady=2)
        ttk.Label(varsel_frame, text="Uses model-specific importance scores",
                 style='Caption.TLabel').grid(row=1, column=1, sticky=tk.W, padx=15)

        ttk.Checkbutton(varsel_frame, text="✓ SPA (Successive Projections)",
                       variable=self.varsel_spa).grid(row=2, column=0, sticky=tk.W, pady=2)
        ttk.Label(varsel_frame, text="Collinearity-aware selection",
                 style='Caption.TLabel').grid(row=2, column=1, sticky=tk.W, padx=15)

        ttk.Checkbutton(varsel_frame, text="✓ UVE (Uninformative Variable Elimination)",
                       variable=self.varsel_uve).grid(row=3, column=0, sticky=tk.W, pady=2)
        ttk.Label(varsel_frame, text="Filters noisy variables",
                 style='Caption.TLabel').grid(row=3, column=1, sticky=tk.W, padx=15)

        ttk.Checkbutton(varsel_frame, text="✓ UVE-SPA Hybrid",
                       variable=self.varsel_uve_spa).grid(row=4, column=0, sticky=tk.W, pady=2)
        ttk.Label(varsel_frame, text="Combines noise filtering + collinearity reduction",
                 style='Caption.TLabel').grid(row=4, column=1, sticky=tk.W, padx=15)

        ttk.Checkbutton(varsel_frame, text="✓ iPLS (Interval PLS)",
                       variable=self.varsel_ipls).grid(row=5, column=0, sticky=tk.W, pady=2)
        ttk.Label(varsel_frame, text="Region-based analysis",
                 style='Caption.TLabel').grid(row=5, column=1, sticky=tk.W, padx=15)

        # UVE Prefilter option
        ttk.Checkbutton(varsel_frame, text="Apply UVE Pre-filter (removes noisy variables first)",
                       variable=self.apply_uve_prefilter).grid(row=6, column=0, columnspan=2, sticky=tk.W, pady=(15, 5))

        # Method parameters
        ttk.Label(varsel_frame, text="Method Parameters:", style='Subheading.TLabel').grid(row=7, column=0, columnspan=2, sticky=tk.W, pady=(15, 8))

        params_frame = ttk.Frame(varsel_frame)
        params_frame.grid(row=8, column=0, columnspan=2, sticky=tk.W, pady=5)

        # UVE parameters
        ttk.Label(params_frame, text="UVE Cutoff:").grid(row=0, column=0, sticky=tk.W, padx=(0, 5))
        ttk.Entry(params_frame, textvariable=self.uve_cutoff_multiplier, width=8).grid(row=0, column=1, sticky=tk.W, padx=5)
        ttk.Label(params_frame, text="(0.7-1.5, default: 1.0)", style='Caption.TLabel').grid(row=0, column=2, sticky=tk.W, padx=10)

        ttk.Label(params_frame, text="UVE Components:").grid(row=1, column=0, sticky=tk.W, padx=(0, 5), pady=5)
        ttk.Entry(params_frame, textvariable=self.uve_n_components, width=8).grid(row=1, column=1, sticky=tk.W, padx=5, pady=5)
        ttk.Label(params_frame, text="(leave empty for auto)", style='Caption.TLabel').grid(row=1, column=2, sticky=tk.W, padx=10)

        ttk.Label(params_frame, text="SPA Random Starts:").grid(row=2, column=0, sticky=tk.W, padx=(0, 5), pady=5)
        ttk.Spinbox(params_frame, from_=1, to=50, textvariable=self.spa_n_random_starts, width=8).grid(row=2, column=1, sticky=tk.W, padx=5, pady=5)
        ttk.Label(params_frame, text="(default: 10)", style='Caption.TLabel').grid(row=2, column=2, sticky=tk.W, padx=10)

        ttk.Label(params_frame, text="iPLS Intervals:").grid(row=3, column=0, sticky=tk.W, padx=(0, 5), pady=5)
        ttk.Spinbox(params_frame, from_=5, to=50, textvariable=self.ipls_n_intervals, width=8).grid(row=3, column=1, sticky=tk.W, padx=5, pady=5)
        ttk.Label(params_frame, text="(default: 20)", style='Caption.TLabel').grid(row=3, column=2, sticky=tk.W, padx=10)

        ttk.Label(varsel_frame, text="📚 See VARIABLE_SELECTION_IMPLEMENTATION.md for method details",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=9, column=0, columnspan=2, sticky=tk.W, pady=(10, 0))

    def _create_tab4c_model_configuration(self):
        """Subtab 4C: Model Configuration - Model selection and advanced model options."""
        tab4c = ttk.Frame(self.config_notebook, style='TFrame')
        self.config_notebook.add(tab4c, text='  🤖 Model Config  ')

        # Create scrollable content
        canvas = tk.Canvas(tab4c, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab4c, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab4c", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Run Analysis button at the top
        run_frame = tk.Frame(content_frame, bg=self.colors['bg'])
        run_frame.grid(row=row, column=0, columnspan=2, sticky='ew', pady=(0, 20))
        self._create_accent_button(run_frame, "▶ Run Analysis", self._run_analysis).pack(side='left')
        row += 1

        # === Model Selection ===
        self._create_section_header(content_frame, "Models to Test", row=row, columnspan=2)
        row += 1

        # Tier Selection - use modern card
        tier_card_outer, tier_card = self._create_card(content_frame, title="Model Tier (Quick Presets) 🆕",
                                                         subtitle="Select a preset tier to auto-populate model selections")
        tier_card_outer.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1
        # Inner frame for grid layout within card
        tier_frame = tk.Frame(tier_card, bg=self.colors['card_bg'])
        tier_frame.pack(fill='both', expand=True)

        tier_options_frame = ttk.Frame(tier_frame)
        tier_options_frame.grid(row=0, column=0, columnspan=5, sticky=tk.W, pady=5)

        ttk.Radiobutton(tier_options_frame, text="⚡ Quick", variable=self.model_tier, value="quick").grid(row=0, column=0, sticky=tk.W, padx=5)
        ttk.Radiobutton(tier_options_frame, text="⭐ Standard", variable=self.model_tier, value="standard").grid(row=0, column=1, sticky=tk.W, padx=5)
        ttk.Radiobutton(tier_options_frame, text="🔬 Comprehensive", variable=self.model_tier, value="comprehensive").grid(row=0, column=2, sticky=tk.W, padx=5)
        ttk.Radiobutton(tier_options_frame, text="🧪 Experimental", variable=self.model_tier, value="experimental").grid(row=0, column=3, sticky=tk.W, padx=5)
        ttk.Radiobutton(tier_options_frame, text="⚙️ Custom", variable=self.model_tier, value="custom").grid(row=0, column=4, sticky=tk.W, padx=5)

        ttk.Label(tier_frame, text="💡 Tiers auto-update checkboxes below. Select Custom to manually choose models.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=2, column=0, columnspan=5, sticky=tk.W, pady=(10, 0))

        # Add callback to tier selection to update checkboxes
        self.model_tier.trace_add('write', self._on_tier_changed)

        # Create card for model selection
        models_card_outer, models_card = self._create_card(content_frame, title="Select Models",
                                                            subtitle="Choose machine learning models for analysis")
        models_card_outer.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1
        # Inner frame for grid layout within card
        models_frame = tk.Frame(models_card, bg=self.colors['card_bg'])
        models_frame.pack(fill='both', expand=True)

        # Core Models (Column 1)
        ttk.Label(models_frame, text="Core Models", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))

        self.pls_checkbox = ttk.Checkbutton(models_frame, text="✓ PLS (Partial Least Squares)", variable=self.use_pls)
        self.pls_checkbox.grid(row=1, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="Linear, fast, interpretable", style='Caption.TLabel').grid(row=1, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.pls_checkbox, text=TOOLTIP_CONTENT['models']['PLS'], delay=500)

        self.plsda_checkbox = ttk.Checkbutton(models_frame, text="✓ PLS-DA (Discriminant Analysis)", variable=self.use_plsda)
        self.plsda_checkbox.grid(row=2, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="PLS for classification tasks", style='Caption.TLabel').grid(row=2, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.plsda_checkbox, text=TOOLTIP_CONTENT['models']['PLS-DA'], delay=500)

        self.ridge_checkbox = ttk.Checkbutton(models_frame, text="✓ Ridge Regression", variable=self.use_ridge)
        self.ridge_checkbox.grid(row=3, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="L2 regularized linear", style='Caption.TLabel').grid(row=3, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.ridge_checkbox, text=TOOLTIP_CONTENT['models']['Ridge'], delay=500)

        self.lasso_checkbox = ttk.Checkbutton(models_frame, text="✓ Lasso Regression", variable=self.use_lasso)
        self.lasso_checkbox.grid(row=4, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="L1 regularized, sparse", style='Caption.TLabel').grid(row=4, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.lasso_checkbox, text=TOOLTIP_CONTENT['models']['Lasso'], delay=500)

        self.elasticnet_checkbox = ttk.Checkbutton(models_frame, text="✓ ElasticNet 🆕", variable=self.use_elasticnet)
        self.elasticnet_checkbox.grid(row=5, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="L1+L2 combined regularization", style='Caption.TLabel').grid(row=5, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.elasticnet_checkbox, text=TOOLTIP_CONTENT['models']['ElasticNet'], delay=500)

        self.randomforest_checkbox = ttk.Checkbutton(models_frame, text="✓ Random Forest", variable=self.use_randomforest)
        self.randomforest_checkbox.grid(row=6, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="Nonlinear, robust", style='Caption.TLabel').grid(row=6, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.randomforest_checkbox, text=TOOLTIP_CONTENT['models']['RandomForest'], delay=500)

        # Advanced Models (Column 2)
        ttk.Label(models_frame, text="Advanced Models", style='Subheading.TLabel').grid(row=0, column=2, sticky=tk.W, pady=(0, 5), padx=(40, 0))

        self.mlp_checkbox = ttk.Checkbutton(models_frame, text="✓ MLP (Multi-Layer Perceptron)", variable=self.use_mlp)
        self.mlp_checkbox.grid(row=1, column=2, sticky=tk.W, pady=5, padx=(40, 0))
        ttk.Label(models_frame, text="Deep learning", style='Caption.TLabel').grid(row=1, column=3, sticky=tk.W, padx=15)
        CreateToolTip(self.mlp_checkbox, text=TOOLTIP_CONTENT['models']['MLP'], delay=500)

        self.svr_checkbox = ttk.Checkbutton(models_frame, text="✓ SVR 🆕", variable=self.use_svr)
        self.svr_checkbox.grid(row=2, column=2, sticky=tk.W, pady=5, padx=(40, 0))
        ttk.Label(models_frame, text="Support Vector Regression", style='Caption.TLabel').grid(row=2, column=3, sticky=tk.W, padx=15)
        CreateToolTip(self.svr_checkbox, text=TOOLTIP_CONTENT['models']['SVR'], delay=500)

        self.svm_checkbox = ttk.Checkbutton(models_frame, text="✓ SVM 🆕", variable=self.use_svm)
        self.svm_checkbox.grid(row=3, column=2, sticky=tk.W, pady=5, padx=(40, 0))
        ttk.Label(models_frame, text="Support Vector Machine (classification)", style='Caption.TLabel').grid(row=3, column=3, sticky=tk.W, padx=15)

        # Gradient Boosting Models (Column 3, spanning bottom)
        ttk.Label(models_frame, text="Modern Gradient Boosting 🆕", style='Subheading.TLabel', foreground=self.colors['success']).grid(row=7, column=0, columnspan=4, sticky=tk.W, pady=(15, 5))

        self.xgboost_checkbox = ttk.Checkbutton(models_frame, text="✓ XGBoost", variable=self.use_xgboost)
        self.xgboost_checkbox.grid(row=8, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="Industry-leading gradient boosting", style='Caption.TLabel').grid(row=8, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.xgboost_checkbox, text=TOOLTIP_CONTENT['models']['XGBoost'], delay=500)

        self.lightgbm_checkbox = ttk.Checkbutton(models_frame, text="✓ LightGBM", variable=self.use_lightgbm)
        self.lightgbm_checkbox.grid(row=9, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="Microsoft's fast gradient boosting", style='Caption.TLabel').grid(row=9, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.lightgbm_checkbox, text=TOOLTIP_CONTENT['models']['LightGBM'], delay=500)

        self.catboost_checkbox = ttk.Checkbutton(models_frame, text="✓ CatBoost", variable=self.use_catboost)
        self.catboost_checkbox.grid(row=10, column=0, sticky=tk.W, pady=5)
        if not HAS_CATBOOST:
            self.catboost_checkbox.state(['disabled'])
            ttk.Label(models_frame, text="Requires Visual Studio 2022 Build Tools (not installed)", style='Caption.TLabel', foreground=self.colors['warning']).grid(row=10, column=1, sticky=tk.W, padx=15)
        else:
            ttk.Label(models_frame, text="Yandex's gradient boosting", style='Caption.TLabel').grid(row=10, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.catboost_checkbox, text=TOOLTIP_CONTENT['models']['CatBoost'], delay=500)

        self.neuralboosted_checkbox = ttk.Checkbutton(models_frame, text="✓ Neural Boosted", variable=self.use_neuralboosted)
        self.neuralboosted_checkbox.grid(row=11, column=0, sticky=tk.W, pady=5)
        ttk.Label(models_frame, text="Gradient boosting with neural networks", style='Caption.TLabel').grid(row=11, column=1, sticky=tk.W, padx=15)
        CreateToolTip(self.neuralboosted_checkbox, text=TOOLTIP_CONTENT['models']['NeuralBoosted'], delay=500)

        # Store checkbox widget references for enable/disable control based on task type
        self.model_checkbox_widgets = {
            'PLS': self.pls_checkbox,
            'PLS-DA': self.plsda_checkbox,
            'Ridge': self.ridge_checkbox,
            'Lasso': self.lasso_checkbox,
            'ElasticNet': self.elasticnet_checkbox,
            'RandomForest': self.randomforest_checkbox,
            'XGBoost': self.xgboost_checkbox,
            'LightGBM': self.lightgbm_checkbox,
            'CatBoost': self.catboost_checkbox,
            'SVR': self.svr_checkbox,
            'SVM': self.svm_checkbox,
            'MLP': self.mlp_checkbox,
            'NeuralBoosted': self.neuralboosted_checkbox
        }

        # === Advanced Model Options ===
        self._create_section_header(content_frame, "Advanced Model Options", row=row, columnspan=2)
        row += 1

        # Create collapsible section for Neural Boosted hyperparameters
        neuralboosted_section, neuralboosted_content = self._create_collapsible_section(content_frame,
                                                                                        "Neural Boosted Hyperparameters",
                                                                                        expanded=False)
        neuralboosted_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        advanced_card_outer, advanced_frame = self._create_card(neuralboosted_content,
                                                                subtitle="Configure n_estimators and learning rates")
        advanced_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout (card uses pack internally)
        advanced_content = ttk.Frame(advanced_frame)
        advanced_content.pack(fill='both', expand=True)

        # n_estimators options
        nb_n_est_label = ttk.Label(advanced_content, text="n_estimators (boosting rounds):", style='Subheading.TLabel')
        nb_n_est_label.grid(row=0, column=0, columnspan=4, sticky=tk.W, pady=(0, 5))
        CreateToolTip(nb_n_est_label, text=TOOLTIP_CONTENT['hyperparameters']['neuralboosted_n_estimators'], delay=500)
        nest_frame = ttk.Frame(advanced_content)
        nest_frame.grid(row=1, column=0, columnspan=4, sticky=tk.W, pady=5)
        ttk.Checkbutton(nest_frame, text="50", variable=self.n_estimators_50).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(nest_frame, text="100 ⭐", variable=self.n_estimators_100).grid(row=0, column=1, padx=5)
        ttk.Label(nest_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(nest_frame, textvariable=self.n_estimators_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(nest_frame, text="(default: 100 only)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        # Learning rate options
        nb_lr_label = ttk.Label(advanced_content, text="Learning rates:", style='Subheading.TLabel')
        nb_lr_label.grid(row=2, column=0, columnspan=4, sticky=tk.W, pady=(15, 5))
        CreateToolTip(nb_lr_label, text=TOOLTIP_CONTENT['hyperparameters']['neuralboosted_learning_rate'], delay=500)
        lr_frame = ttk.Frame(advanced_content)
        lr_frame.grid(row=3, column=0, columnspan=4, sticky=tk.W, pady=5)
        ttk.Checkbutton(lr_frame, text="0.05", variable=self.lr_005).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lr_frame, text="0.1 ⭐", variable=self.lr_01).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lr_frame, text="0.2 ⭐", variable=self.lr_02).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(lr_frame, text="0.3 ⭐", variable=self.lr_03).grid(row=0, column=3, padx=5)
        ttk.Label(lr_frame, text="(default: 0.1, 0.2, 0.3)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        # Hidden Layer Size options
        nb_hidden_label = ttk.Label(advanced_content, text="Hidden Layer Size (neurons per weak learner):", style='Subheading.TLabel')
        nb_hidden_label.grid(row=4, column=0, columnspan=4, sticky=tk.W, pady=(15, 5))
        CreateToolTip(nb_hidden_label, text=TOOLTIP_CONTENT['hyperparameters']['neuralboosted_hidden_layer_size'], delay=500)
        hidden_frame = ttk.Frame(advanced_content)
        hidden_frame.grid(row=5, column=0, columnspan=4, sticky=tk.W, pady=5)
        ttk.Checkbutton(hidden_frame, text="3 ⭐", variable=self.neuralboosted_hidden_3).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(hidden_frame, text="5 ⭐", variable=self.neuralboosted_hidden_5).grid(row=0, column=1, padx=5)
        ttk.Label(hidden_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(hidden_frame, textvariable=self.neuralboosted_hidden_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(hidden_frame, text="(default: 3, 5)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        # Activation Function options
        nb_activation_label = ttk.Label(advanced_content, text="Activation Function:", style='Subheading.TLabel')
        nb_activation_label.grid(row=6, column=0, columnspan=4, sticky=tk.W, pady=(15, 5))
        CreateToolTip(nb_activation_label, text=TOOLTIP_CONTENT['hyperparameters']['neuralboosted_activation'], delay=500)
        activation_frame = ttk.Frame(advanced_content)
        activation_frame.grid(row=7, column=0, columnspan=4, sticky=tk.W, pady=5)
        ttk.Checkbutton(activation_frame, text="tanh ⭐", variable=self.neuralboosted_activation_tanh).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(activation_frame, text="identity ⭐", variable=self.neuralboosted_activation_identity).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(activation_frame, text="relu", variable=self.neuralboosted_activation_relu).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(activation_frame, text="logistic", variable=self.neuralboosted_activation_logistic).grid(row=0, column=3, padx=5)
        ttk.Label(activation_frame, text="(default: tanh, identity)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        # Max Iterations (for neural base learner)
        nb_maxiter_label = ttk.Label(advanced_content, text="Max Iterations (neural base learner):", style='Subheading.TLabel')
        nb_maxiter_label.grid(row=8, column=0, columnspan=4, sticky=tk.W, pady=(15, 5))
        CreateToolTip(nb_maxiter_label, text=TOOLTIP_CONTENT['hyperparameters']['neuralboosted_max_iter'], delay=500)
        nb_maxiter_frame = ttk.Frame(advanced_content)
        nb_maxiter_frame.grid(row=9, column=0, columnspan=4, sticky=tk.W, pady=5)

        ttk.Label(nb_maxiter_frame, text="Max iterations:", style='TLabel').grid(row=0, column=0, padx=(0, 5))
        ttk.Spinbox(nb_maxiter_frame, from_=100, to=5000, increment=100, textvariable=self.max_iter, width=10).grid(row=0, column=1, padx=5)
        ttk.Label(nb_maxiter_frame, text="(default: 100)", style='Caption.TLabel').grid(row=0, column=2, padx=10)

        # Info labels
        ttk.Label(advanced_content, text="💡 Selecting more options = more comprehensive analysis but longer runtime",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=8, column=0, columnspan=4, sticky=tk.W, pady=(10, 0))
        ttk.Label(advanced_content, text="💡 Subsample < 1.0 adds randomness (stochastic gradient boosting) to prevent overfitting",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=9, column=0, columnspan=4, sticky=tk.W, pady=(5, 0))

        # Random Forest Hyperparameters - collapsible
        rf_section, rf_content = self._create_collapsible_section(content_frame,
                                                                   "Random Forest Hyperparameters",
                                                                   expanded=False)
        rf_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        rf_card_outer, rf_frame = self._create_card(rf_content,
                                                    subtitle="Configure number of trees and tree depth")
        rf_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout (card uses pack internally)
        rf_content_frame = ttk.Frame(rf_frame)
        rf_content_frame.pack(fill='both', expand=True)

        # n_estimators (number of trees) options
        rf_n_est_label = ttk.Label(rf_content_frame, text="Number of Trees (n_estimators):", style='Subheading.TLabel')
        rf_n_est_label.grid(row=0, column=0, columnspan=4, sticky=tk.W, pady=(0, 5))
        CreateToolTip(rf_n_est_label, text=TOOLTIP_CONTENT['hyperparameters']['rf_n_estimators'], delay=500)
        rf_trees_frame = ttk.Frame(rf_content_frame)
        rf_trees_frame.grid(row=1, column=0, columnspan=4, sticky=tk.W, pady=5)

        ttk.Checkbutton(rf_trees_frame, text="100 ⭐", variable=self.rf_n_trees_100).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(rf_trees_frame, text="200", variable=self.rf_n_trees_200).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(rf_trees_frame, text="500", variable=self.rf_n_trees_500).grid(row=0, column=2, padx=5)
        ttk.Label(rf_trees_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(rf_trees_frame, textvariable=self.rf_n_trees_custom, width=8).grid(row=0, column=4, padx=5)
        ttk.Label(rf_trees_frame, text="(default: 100)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        # Info label
        ttk.Label(rf_content_frame, text="💡 More trees = better performance but slower training (e.g., 1000, 2000)",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=2, column=0, columnspan=4, sticky=tk.W, pady=(10, 0))

        # Maximum Tree Depth (max_depth) options
        rf_maxdepth_label = ttk.Label(rf_content_frame, text="Maximum Tree Depth (max_depth):", style='Subheading.TLabel')
        rf_maxdepth_label.grid(row=3, column=0, columnspan=4, sticky=tk.W, pady=(15, 5))
        CreateToolTip(rf_maxdepth_label, text=TOOLTIP_CONTENT['hyperparameters']['rf_max_depth'], delay=500)
        rf_depth_frame = ttk.Frame(rf_content_frame)
        rf_depth_frame.grid(row=4, column=0, columnspan=4, sticky=tk.W, pady=5)

        ttk.Checkbutton(rf_depth_frame, text="None (unlimited) ⭐", variable=self.rf_max_depth_none).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(rf_depth_frame, text="30 ⭐", variable=self.rf_max_depth_30).grid(row=0, column=1, padx=5)
        ttk.Label(rf_depth_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(rf_depth_frame, textvariable=self.rf_max_depth_custom, width=8).grid(row=0, column=3, padx=5)
        ttk.Label(rf_depth_frame, text="(default: None, 30)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        # Info label for max_depth
        ttk.Label(rf_content_frame, text="💡 None = trees grow as deep as needed (unlimited). Lower values prevent overfitting.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=5, column=0, columnspan=4, sticky=tk.W, pady=(10, 0))

        # === NEW PHASE 1 HYPERPARAMETERS ===
        # Create wrapper frame for pack-based controls (rf_content_frame uses grid)
        rf_advanced_params_wrapper = ttk.Frame(rf_content_frame)
        rf_advanced_params_wrapper.grid(row=6, column=0, columnspan=4, sticky=tk.W, pady=(15, 0))

        # min_samples_split
        self.rf_min_samples_split_control = self._create_parameter_grid_control(
            rf_advanced_params_wrapper,
            param_name='min_samples_split',
            param_label='Minimum Samples to Split (min_samples_split):',
            checkbox_values=[2, 5, 10, 20],
            default_checked=[2],
            is_float=False,
            allow_string_values=False,
            help_text="Minimum samples required to split an internal node"
        )

        # min_samples_leaf
        self.rf_min_samples_leaf_control = self._create_parameter_grid_control(
            rf_advanced_params_wrapper,
            param_name='min_samples_leaf',
            param_label='Minimum Samples in Leaf (min_samples_leaf):',
            checkbox_values=[1, 2, 4, 8],
            default_checked=[1],
            is_float=False,
            allow_string_values=False,
            help_text="Minimum samples required in each leaf node"
        )

        # max_features
        self.rf_max_features_control = self._create_parameter_grid_control(
            rf_advanced_params_wrapper,
            param_name='max_features',
            param_label='Max Features for Splits (max_features):',
            checkbox_values=['sqrt', 'log2', None],
            default_checked=['sqrt'],
            is_float=False,
            allow_string_values=True,
            help_text="sqrt = square root of features, log2 = log2 of features, None = all features"
        )

        # bootstrap
        self.rf_bootstrap_control = self._create_parameter_grid_control(
            rf_advanced_params_wrapper,
            param_name='bootstrap',
            param_label='Bootstrap Sampling (bootstrap):',
            checkbox_values=[True, False],
            default_checked=[True],
            is_float=False,
            allow_string_values=True,
            help_text="Whether to use bootstrap samples when building trees"
        )

        # max_leaf_nodes
        self.rf_max_leaf_nodes_control = self._create_parameter_grid_control(
            rf_advanced_params_wrapper,
            param_name='max_leaf_nodes',
            param_label='Max Leaf Nodes (max_leaf_nodes):',
            checkbox_values=[None, 10, 50, 100, 500],
            default_checked=[None],
            is_float=False,
            allow_string_values=False,
            help_text="Limits tree growth, None = unlimited"
        )

        # min_impurity_decrease
        self.rf_min_impurity_decrease_control = self._create_parameter_grid_control(
            rf_advanced_params_wrapper,
            param_name='min_impurity_decrease',
            param_label='Min Impurity Decrease (min_impurity_decrease):',
            checkbox_values=[0.0, 0.01, 0.05, 0.1],
            default_checked=[0.0],
            is_float=True,
            allow_string_values=False,
            help_text="Minimum impurity decrease required to split a node"
        )

        # Ridge Regression Hyperparameters - collapsible
        ridge_section, ridge_content = self._create_collapsible_section(content_frame,
                                                                        "Ridge Regression Hyperparameters",
                                                                        expanded=False)
        ridge_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        ridge_card_outer, ridge_frame = self._create_card(ridge_content,
                                                          subtitle="Configure alpha (regularization strength)")
        ridge_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout (card uses pack internally)
        ridge_content_frame = ttk.Frame(ridge_frame)
        ridge_content_frame.pack(fill='both', expand=True)

        # Alpha (regularization strength) options
        ridge_alpha_label = ttk.Label(ridge_content_frame, text="Alpha (Regularization Strength):", style='Subheading.TLabel')
        ridge_alpha_label.grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        # CreateToolTip(ridge_alpha_label, text="Controls the strength of L2 regularization", delay=500)
        ridge_alpha_frame = ttk.Frame(ridge_content_frame)
        ridge_alpha_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(ridge_alpha_frame, text="0.001 ⭐", variable=self.ridge_alpha_0001).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(ridge_alpha_frame, text="0.01 ⭐", variable=self.ridge_alpha_001).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(ridge_alpha_frame, text="0.1 ⭐", variable=self.ridge_alpha_01).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(ridge_alpha_frame, text="1.0 ⭐", variable=self.ridge_alpha_1).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(ridge_alpha_frame, text="10.0 ⭐", variable=self.ridge_alpha_10).grid(row=0, column=4, padx=5)
        ttk.Label(ridge_alpha_frame, text="Custom:", style='TLabel').grid(row=0, column=5, padx=(15, 5))
        ttk.Entry(ridge_alpha_frame, textvariable=self.ridge_alpha_custom, width=10).grid(row=0, column=6, padx=5)
        ttk.Label(ridge_alpha_frame, text="(default: all checked)", style='Caption.TLabel').grid(row=0, column=7, padx=10)

        # Solver (optimization algorithm)
        ridge_solver_label = ttk.Label(ridge_content_frame, text="Solver (Optimization Algorithm):", style='Subheading.TLabel')
        ridge_solver_label.grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        ridge_solver_frame = ttk.Frame(ridge_content_frame)
        ridge_solver_frame.grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(ridge_solver_frame, text="auto ⭐", variable=self.ridge_solver_auto).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(ridge_solver_frame, text="svd", variable=self.ridge_solver_svd).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(ridge_solver_frame, text="cholesky", variable=self.ridge_solver_cholesky).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(ridge_solver_frame, text="lsqr", variable=self.ridge_solver_lsqr).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(ridge_solver_frame, text="sag", variable=self.ridge_solver_sag).grid(row=0, column=4, padx=5)
        ttk.Label(ridge_solver_frame, text="(default: auto)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        # Tolerance (convergence criterion)
        ridge_tol_label = ttk.Label(ridge_content_frame, text="Tolerance (Convergence Criterion):", style='Subheading.TLabel')
        ridge_tol_label.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        ridge_tol_frame = ttk.Frame(ridge_content_frame)
        ridge_tol_frame.grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(ridge_tol_frame, text="1e-4 ⭐", variable=self.ridge_tol_1e4).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(ridge_tol_frame, text="1e-3", variable=self.ridge_tol_1e3).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(ridge_tol_frame, text="1e-5", variable=self.ridge_tol_1e5).grid(row=0, column=2, padx=5)
        ttk.Label(ridge_tol_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(ridge_tol_frame, textvariable=self.ridge_tol_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(ridge_tol_frame, text="(default: 1e-4)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        # Info label
        ttk.Label(ridge_content_frame, text="💡 Lower alpha = less regularization (closer to OLS). Higher alpha = more shrinkage.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=(10, 0))

        # Lasso Regression Hyperparameters - collapsible
        lasso_section, lasso_content = self._create_collapsible_section(content_frame,
                                                                        "Lasso Regression Hyperparameters",
                                                                        expanded=False)
        lasso_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        lasso_card_outer, lasso_frame = self._create_card(lasso_content,
                                                          subtitle="Configure alpha for sparse solutions")
        lasso_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout (card uses pack internally)
        lasso_content_frame = ttk.Frame(lasso_frame)
        lasso_content_frame.pack(fill='both', expand=True)

        # Alpha (regularization strength) options
        lasso_alpha_label = ttk.Label(lasso_content_frame, text="Alpha (Regularization Strength):", style='Subheading.TLabel')
        lasso_alpha_label.grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        # CreateToolTip() - missing parameters
        lasso_alpha_frame = ttk.Frame(lasso_content_frame)
        lasso_alpha_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lasso_alpha_frame, text="0.001 ⭐", variable=self.lasso_alpha_0001).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lasso_alpha_frame, text="0.01 ⭐", variable=self.lasso_alpha_001).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lasso_alpha_frame, text="0.1 ⭐", variable=self.lasso_alpha_01).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(lasso_alpha_frame, text="1.0 ⭐", variable=self.lasso_alpha_1).grid(row=0, column=3, padx=5)
        ttk.Label(lasso_alpha_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5))
        ttk.Entry(lasso_alpha_frame, textvariable=self.lasso_alpha_custom, width=10).grid(row=0, column=5, padx=5)
        ttk.Label(lasso_alpha_frame, text="(default: all checked)", style='Caption.TLabel').grid(row=0, column=6, padx=10)

        # Selection (coordinate descent algorithm)
        lasso_selection_label = ttk.Label(lasso_content_frame, text="Selection (Coordinate Descent Algorithm):", style='Subheading.TLabel')
        lasso_selection_label.grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lasso_selection_frame = ttk.Frame(lasso_content_frame)
        lasso_selection_frame.grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lasso_selection_frame, text="cyclic ⭐", variable=self.lasso_selection_cyclic).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lasso_selection_frame, text="random", variable=self.lasso_selection_random).grid(row=0, column=1, padx=5)
        ttk.Label(lasso_selection_frame, text="(default: cyclic)", style='Caption.TLabel').grid(row=0, column=2, padx=10)

        # Tolerance (convergence criterion)
        lasso_tol_label = ttk.Label(lasso_content_frame, text="Tolerance (Convergence Criterion):", style='Subheading.TLabel')
        lasso_tol_label.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lasso_tol_frame = ttk.Frame(lasso_content_frame)
        lasso_tol_frame.grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lasso_tol_frame, text="1e-4 ⭐", variable=self.lasso_tol_1e4).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lasso_tol_frame, text="1e-3", variable=self.lasso_tol_1e3).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lasso_tol_frame, text="1e-5", variable=self.lasso_tol_1e5).grid(row=0, column=2, padx=5)
        ttk.Label(lasso_tol_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(lasso_tol_frame, textvariable=self.lasso_tol_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(lasso_tol_frame, text="(default: 1e-4)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        # Info label
        ttk.Label(lasso_content_frame, text="💡 Lasso encourages sparse solutions (sets weak coefficients to zero). Higher alpha = more sparsity.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=(10, 0))

        # ElasticNet Hyperparameters - collapsible
        elasticnet_section, elasticnet_content = self._create_collapsible_section(content_frame,
                                                                                   "ElasticNet Hyperparameters",
                                                                                   expanded=False)
        elasticnet_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        elasticnet_card_outer, elasticnet_frame = self._create_card(elasticnet_content,
                                                                     subtitle="Configure alpha and L1/L2 ratio")
        elasticnet_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout
        elasticnet_content_frame = ttk.Frame(elasticnet_frame)
        elasticnet_content_frame.pack(fill='both', expand=True)

        # Alpha (regularization strength) options
        elasticnet_alpha_label = ttk.Label(elasticnet_content_frame, text="Alpha (Regularization Strength):", style='Subheading.TLabel')
        elasticnet_alpha_label.grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        # CreateToolTip() - missing parameters
        elasticnet_alpha_frame = ttk.Frame(elasticnet_content_frame)
        elasticnet_alpha_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(elasticnet_alpha_frame, text="0.01 ⭐", variable=self.elasticnet_alpha_001).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(elasticnet_alpha_frame, text="0.1 ⭐", variable=self.elasticnet_alpha_01).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(elasticnet_alpha_frame, text="1.0 ⭐", variable=self.elasticnet_alpha_10).grid(row=0, column=2, padx=5)
        ttk.Label(elasticnet_alpha_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(elasticnet_alpha_frame, textvariable=self.elasticnet_alpha_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(elasticnet_alpha_frame, text="(default: all checked)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        # L1 ratio options
        elasticnet_l1_label = ttk.Label(elasticnet_content_frame, text="L1 Ratio (L1 vs L2 mix):", style='Subheading.TLabel')
        elasticnet_l1_label.grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(10, 5))
        # CreateToolTip() - missing parameters
        elasticnet_l1_frame = ttk.Frame(elasticnet_content_frame)
        elasticnet_l1_frame.grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(elasticnet_l1_frame, text="0.3 ⭐", variable=self.elasticnet_l1_ratio_03).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(elasticnet_l1_frame, text="0.5 ⭐", variable=self.elasticnet_l1_ratio_05).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(elasticnet_l1_frame, text="0.7 ⭐", variable=self.elasticnet_l1_ratio_07).grid(row=0, column=2, padx=5)
        ttk.Label(elasticnet_l1_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(elasticnet_l1_frame, textvariable=self.elasticnet_l1_ratio_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(elasticnet_l1_frame, text="(default: all checked)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        # Selection (coordinate descent algorithm)
        elasticnet_selection_label = ttk.Label(elasticnet_content_frame, text="Selection (Coordinate Descent Algorithm):", style='Subheading.TLabel')
        elasticnet_selection_label.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        elasticnet_selection_frame = ttk.Frame(elasticnet_content_frame)
        elasticnet_selection_frame.grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(elasticnet_selection_frame, text="cyclic ⭐", variable=self.elasticnet_selection_cyclic).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(elasticnet_selection_frame, text="random", variable=self.elasticnet_selection_random).grid(row=0, column=1, padx=5)
        ttk.Label(elasticnet_selection_frame, text="(default: cyclic)", style='Caption.TLabel').grid(row=0, column=2, padx=10)

        # Tolerance (convergence criterion)
        elasticnet_tol_label = ttk.Label(elasticnet_content_frame, text="Tolerance (Convergence Criterion):", style='Subheading.TLabel')
        elasticnet_tol_label.grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        elasticnet_tol_frame = ttk.Frame(elasticnet_content_frame)
        elasticnet_tol_frame.grid(row=7, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(elasticnet_tol_frame, text="1e-4 ⭐", variable=self.elasticnet_tol_1e4).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(elasticnet_tol_frame, text="1e-3", variable=self.elasticnet_tol_1e3).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(elasticnet_tol_frame, text="1e-5", variable=self.elasticnet_tol_1e5).grid(row=0, column=2, padx=5)
        ttk.Label(elasticnet_tol_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(elasticnet_tol_frame, textvariable=self.elasticnet_tol_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(elasticnet_tol_frame, text="(default: 1e-4)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        # Info labels
        ttk.Label(elasticnet_content_frame, text="💡 ElasticNet combines Ridge (L2) and Lasso (L1) regularization.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=8, column=0, columnspan=6, sticky=tk.W, pady=(10, 0))
        ttk.Label(elasticnet_content_frame, text="   L1 ratio: 0.0=Ridge only, 0.5=balanced, 1.0=Lasso only",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=9, column=0, columnspan=6, sticky=tk.W, pady=(2, 0))

        # PLS Hyperparameters - collapsible
        pls_section, pls_content = self._create_collapsible_section(content_frame,
                                                                     "PLS Hyperparameters",
                                                                     expanded=False)
        pls_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        pls_card_outer, pls_frame = self._create_card(pls_content,
                                                       subtitle="Configure convergence parameters")
        pls_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout
        pls_content_frame = ttk.Frame(pls_frame)
        pls_content_frame.pack(fill='both', expand=True)

        # Max Latent Variables (n_components)
        pls_ncomp_label = ttk.Label(pls_content_frame, text="Max Latent Variables (n_components):", style='Subheading.TLabel')
        pls_ncomp_label.grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        # CreateToolTip() - missing parameters
        max_comp_frame = ttk.Frame(pls_content_frame)
        max_comp_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Label(max_comp_frame, text="Max components:", style='TLabel').grid(row=0, column=0, padx=(0, 5))
        ttk.Spinbox(max_comp_frame, from_=2, to=100, textvariable=self.max_n_components, width=10).grid(row=0, column=1, padx=5)
        ttk.Label(max_comp_frame, text="(default: 8, determines search range)", style='Caption.TLabel').grid(row=0, column=2, padx=10)

        ttk.Label(pls_content_frame, text="💡 Sets the maximum number of PLS/PCR components to evaluate during optimization.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(5, 10))

        # max_iter options
        pls_maxiter_label = ttk.Label(pls_content_frame, text="Max Iterations:", style='Subheading.TLabel')
        pls_maxiter_label.grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=(10, 5))
        # CreateToolTip() - missing parameters
        pls_maxiter_frame = ttk.Frame(pls_content_frame)
        pls_maxiter_frame.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(pls_maxiter_frame, text="500 ⭐", variable=self.pls_max_iter_500).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(pls_maxiter_frame, text="1000", variable=self.pls_max_iter_1000).grid(row=0, column=1, padx=5)
        ttk.Label(pls_maxiter_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(pls_maxiter_frame, textvariable=self.pls_max_iter_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(pls_maxiter_frame, text="(default: 500)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        # tol options
        pls_tol_label = ttk.Label(pls_content_frame, text="Tolerance:", style='Subheading.TLabel')
        pls_tol_label.grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=(10, 5))
        # CreateToolTip() - missing parameters
        pls_tol_frame = ttk.Frame(pls_content_frame)
        pls_tol_frame.grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(pls_tol_frame, text="1e-7", variable=self.pls_tol_1e7).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(pls_tol_frame, text="1e-6 ⭐", variable=self.pls_tol_1e6).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(pls_tol_frame, text="1e-5", variable=self.pls_tol_1e5).grid(row=0, column=2, padx=5)
        ttk.Label(pls_tol_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(pls_tol_frame, textvariable=self.pls_tol_custom, width=15).grid(row=0, column=4, padx=5)
        ttk.Label(pls_tol_frame, text="(default: 1e-6)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        # Info labels
        ttk.Label(pls_content_frame, text="💡 max_iter controls convergence speed, tol controls precision.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=7, column=0, columnspan=6, sticky=tk.W, pady=(10, 0))
        ttk.Label(pls_content_frame, text="   Standard defaults (500, 1e-6) work well for most spectral data.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=8, column=0, columnspan=6, sticky=tk.W, pady=(2, 0))

        # XGBoost Hyperparameters - collapsible
        xgb_section, xgb_content = self._create_collapsible_section(content_frame,
                                                                     "XGBoost Hyperparameters (Spectroscopy-Optimized)",
                                                                     expanded=False)
        xgb_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        xgb_card_outer, xgb_frame = self._create_card(xgb_content,
                                                      subtitle="Comprehensive hyperparameter tuning for XGBoost")
        xgb_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout (card uses pack internally)
        xgb_content_frame = ttk.Frame(xgb_frame)
        xgb_content_frame.pack(fill='both', expand=True)

        # n_estimators (number of boosting rounds)
        ttk.Label(xgb_content_frame, text="Number of Boosting Rounds (n_estimators):", style='Subheading.TLabel').grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        xgb_n_est_frame = ttk.Frame(xgb_content_frame)
        xgb_n_est_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_n_est_frame, text="100 ⭐", variable=self.xgb_n_estimators_100).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_n_est_frame, text="200 ⭐", variable=self.xgb_n_estimators_200).grid(row=0, column=1, padx=5)
        ttk.Label(xgb_n_est_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(xgb_n_est_frame, textvariable=self.xgb_n_estimators_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(xgb_n_est_frame, text="(default: 100, 200)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(xgb_content_frame, text="💡 More rounds = better fit but risk overfitting. 100-200 typical for spectral data.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # learning_rate (step size shrinkage)
        ttk.Label(xgb_content_frame, text="Learning Rate (shrinkage):", style='Subheading.TLabel').grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        xgb_lr_frame = ttk.Frame(xgb_content_frame)
        xgb_lr_frame.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_lr_frame, text="0.05 ⭐", variable=self.xgb_lr_005).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_lr_frame, text="0.1 ⭐", variable=self.xgb_lr_01).grid(row=0, column=1, padx=5)
        ttk.Label(xgb_lr_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(xgb_lr_frame, textvariable=self.xgb_lr_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(xgb_lr_frame, text="(default: 0.05, 0.1)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(xgb_content_frame, text="💡 Lower learning rate = more conservative updates (less overfitting). Range: 0.01-0.3.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # max_depth (maximum tree depth)
        ttk.Label(xgb_content_frame, text="Maximum Tree Depth:", style='Subheading.TLabel').grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        xgb_depth_frame = ttk.Frame(xgb_content_frame)
        xgb_depth_frame.grid(row=7, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_depth_frame, text="3 ⭐", variable=self.xgb_max_depth_3).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_depth_frame, text="6 ⭐", variable=self.xgb_max_depth_6).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(xgb_depth_frame, text="9", variable=self.xgb_max_depth_9).grid(row=0, column=2, padx=5)
        ttk.Label(xgb_depth_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(xgb_depth_frame, textvariable=self.xgb_max_depth_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(xgb_depth_frame, text="(default: 3, 6)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        ttk.Label(xgb_content_frame, text="💡 Shallow trees (3-6) reduce overfitting. Deeper trees capture complex patterns.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=8, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # subsample (row sampling ratio)
        ttk.Label(xgb_content_frame, text="Row Sampling Ratio (subsample):", style='Subheading.TLabel').grid(row=9, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        xgb_subsample_frame = ttk.Frame(xgb_content_frame)
        xgb_subsample_frame.grid(row=10, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_subsample_frame, text="0.8 ⭐", variable=self.xgb_subsample_08).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_subsample_frame, text="1.0 ⭐", variable=self.xgb_subsample_10).grid(row=0, column=1, padx=5)
        ttk.Label(xgb_subsample_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(xgb_subsample_frame, textvariable=self.xgb_subsample_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(xgb_subsample_frame, text="(default: 0.8, 1.0)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(xgb_content_frame, text="💡 0.8 = use 80% of samples per tree (reduces overfitting). 1.0 = use all samples.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=11, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # colsample_bytree (column sampling ratio)
        ttk.Label(xgb_content_frame, text="Feature Sampling Ratio (colsample_bytree):", style='Subheading.TLabel').grid(row=12, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        xgb_colsample_frame = ttk.Frame(xgb_content_frame)
        xgb_colsample_frame.grid(row=13, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_colsample_frame, text="0.8 ⭐", variable=self.xgb_colsample_08).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_colsample_frame, text="1.0 ⭐", variable=self.xgb_colsample_10).grid(row=0, column=1, padx=5)
        ttk.Label(xgb_colsample_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(xgb_colsample_frame, textvariable=self.xgb_colsample_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(xgb_colsample_frame, text="(default: 0.8, 1.0)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(xgb_content_frame, text="💡 0.8 = use 80% of wavelengths per tree (increases diversity). Critical for 2000+ features.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=14, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # reg_alpha (L1 regularization)
        ttk.Label(xgb_content_frame, text="L1 Regularization (reg_alpha):", style='Subheading.TLabel').grid(row=15, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        xgb_reg_alpha_frame = ttk.Frame(xgb_content_frame)
        xgb_reg_alpha_frame.grid(row=16, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_reg_alpha_frame, text="0 ⭐", variable=self.xgb_reg_alpha_0).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_reg_alpha_frame, text="0.1 ⭐", variable=self.xgb_reg_alpha_01).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(xgb_reg_alpha_frame, text="0.5", variable=self.xgb_reg_alpha_05).grid(row=0, column=2, padx=5)
        ttk.Label(xgb_reg_alpha_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(xgb_reg_alpha_frame, textvariable=self.xgb_reg_alpha_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(xgb_reg_alpha_frame, text="(default: 0, 0.1)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        ttk.Label(xgb_content_frame, text="💡 L1 penalty for feature selection. Higher = sparser model. 0.1-0.5 recommended for high-dim data.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=17, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # reg_lambda (L2 regularization) - comprehensive tier
        ttk.Label(xgb_content_frame, text="L2 Regularization (reg_lambda) - Comprehensive:", style='Subheading.TLabel').grid(row=18, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        xgb_reg_lambda_frame = ttk.Frame(xgb_content_frame)
        xgb_reg_lambda_frame.grid(row=19, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_reg_lambda_frame, text="1.0", variable=self.xgb_reg_lambda_10).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_reg_lambda_frame, text="5.0", variable=self.xgb_reg_lambda_50).grid(row=0, column=1, padx=5)
        ttk.Label(xgb_reg_lambda_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(xgb_reg_lambda_frame, textvariable=self.xgb_reg_lambda_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(xgb_reg_lambda_frame, text="(comprehensive tier only)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(xgb_content_frame, text="💡 L2 penalty reduces weight magnitudes. Use with L1 for combined regularization (ElasticNet-style).",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=20, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # min_child_weight (minimum sum of instance weight in child)
        ttk.Label(xgb_content_frame, text="Minimum Child Weight (min_child_weight):", style='Subheading.TLabel').grid(row=21, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        xgb_min_child_weight_frame = ttk.Frame(xgb_content_frame)
        xgb_min_child_weight_frame.grid(row=22, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_min_child_weight_frame, text="1 ⭐", variable=self.xgb_min_child_weight_1).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_min_child_weight_frame, text="3", variable=self.xgb_min_child_weight_3).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(xgb_min_child_weight_frame, text="5", variable=self.xgb_min_child_weight_5).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(xgb_min_child_weight_frame, text="7", variable=self.xgb_min_child_weight_7).grid(row=0, column=3, padx=5)
        ttk.Label(xgb_min_child_weight_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5))
        ttk.Entry(xgb_min_child_weight_frame, textvariable=self.xgb_min_child_weight_custom, width=10).grid(row=0, column=5, padx=5)
        ttk.Label(xgb_min_child_weight_frame, text="(default: 1)", style='Caption.TLabel').grid(row=0, column=6, padx=10)

        ttk.Label(xgb_content_frame, text="💡 Larger values prevent overfitting by requiring more instances per leaf",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=23, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # gamma (minimum loss reduction required for split)
        ttk.Label(xgb_content_frame, text="Gamma (minimum loss reduction):", style='Subheading.TLabel').grid(row=24, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        xgb_gamma_frame = ttk.Frame(xgb_content_frame)
        xgb_gamma_frame.grid(row=25, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(xgb_gamma_frame, text="0 ⭐", variable=self.xgb_gamma_0).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(xgb_gamma_frame, text="0.1", variable=self.xgb_gamma_01).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(xgb_gamma_frame, text="0.3", variable=self.xgb_gamma_03).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(xgb_gamma_frame, text="0.5", variable=self.xgb_gamma_05).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(xgb_gamma_frame, text="1.0", variable=self.xgb_gamma_10).grid(row=0, column=4, padx=5)
        ttk.Label(xgb_gamma_frame, text="Custom:", style='TLabel').grid(row=0, column=5, padx=(15, 5))
        ttk.Entry(xgb_gamma_frame, textvariable=self.xgb_gamma_custom, width=10).grid(row=0, column=6, padx=5)
        ttk.Label(xgb_gamma_frame, text="(default: 0)", style='Caption.TLabel').grid(row=0, column=7, padx=10)

        ttk.Label(xgb_content_frame, text="💡 Regularization parameter, higher = more conservative",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=26, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # LightGBM Hyperparameters - collapsible
        lgbm_section, lgbm_content = self._create_collapsible_section(content_frame,
                                                                       "LightGBM Hyperparameters (Fast Gradient Boosting)",
                                                                       expanded=False)
        lgbm_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        lgbm_card_outer, lgbm_frame = self._create_card(lgbm_content,
                                                         subtitle="Comprehensive hyperparameter tuning for LightGBM")
        lgbm_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout (card uses pack internally)
        lgbm_content_frame = ttk.Frame(lgbm_frame)
        lgbm_content_frame.pack(fill='both', expand=True)

        # max_depth (maximum tree depth)
        ttk.Label(lgbm_content_frame, text="Maximum Tree Depth (max_depth):", style='Subheading.TLabel').grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        lgbm_max_depth_frame = ttk.Frame(lgbm_content_frame)
        lgbm_max_depth_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_max_depth_frame, text="-1 ⭐", variable=self.lightgbm_max_depth_m1).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_max_depth_frame, text="5", variable=self.lightgbm_max_depth_5).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_max_depth_frame, text="10", variable=self.lightgbm_max_depth_10).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(lgbm_max_depth_frame, text="20", variable=self.lightgbm_max_depth_20).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(lgbm_max_depth_frame, text="50", variable=self.lightgbm_max_depth_50).grid(row=0, column=4, padx=5)
        ttk.Label(lgbm_max_depth_frame, text="Custom:", style='TLabel').grid(row=0, column=5, padx=(15, 5))
        ttk.Entry(lgbm_max_depth_frame, textvariable=self.lightgbm_max_depth_custom, width=10).grid(row=0, column=6, padx=5)
        ttk.Label(lgbm_max_depth_frame, text="(default: -1)", style='Caption.TLabel').grid(row=0, column=7, padx=10)

        ttk.Label(lgbm_content_frame, text="💡 -1 means no limit",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # min_child_samples (minimum data in one leaf)
        ttk.Label(lgbm_content_frame, text="Minimum Data in One Leaf (min_child_samples):", style='Subheading.TLabel').grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lgbm_min_child_samples_frame = ttk.Frame(lgbm_content_frame)
        lgbm_min_child_samples_frame.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_min_child_samples_frame, text="5 ⭐", variable=self.lightgbm_min_child_samples_5).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_min_child_samples_frame, text="10", variable=self.lightgbm_min_child_samples_10).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_min_child_samples_frame, text="20", variable=self.lightgbm_min_child_samples_20).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(lgbm_min_child_samples_frame, text="50", variable=self.lightgbm_min_child_samples_50).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(lgbm_min_child_samples_frame, text="100", variable=self.lightgbm_min_child_samples_100).grid(row=0, column=4, padx=5)
        ttk.Label(lgbm_min_child_samples_frame, text="Custom:", style='TLabel').grid(row=0, column=5, padx=(15, 5))
        ttk.Entry(lgbm_min_child_samples_frame, textvariable=self.lightgbm_min_child_samples_custom, width=10).grid(row=0, column=6, padx=5)
        ttk.Label(lgbm_min_child_samples_frame, text="(default: 5 - fixed for small datasets)", style='Caption.TLabel').grid(row=0, column=7, padx=10)

        # subsample (row sampling, aka bagging_fraction)
        ttk.Label(lgbm_content_frame, text="Row Sampling (subsample / bagging_fraction):", style='Subheading.TLabel').grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lgbm_subsample_frame = ttk.Frame(lgbm_content_frame)
        lgbm_subsample_frame.grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_subsample_frame, text="0.5", variable=self.lightgbm_subsample_05).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_subsample_frame, text="0.7", variable=self.lightgbm_subsample_07).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_subsample_frame, text="0.8 ⭐", variable=self.lightgbm_subsample_08).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(lgbm_subsample_frame, text="0.85", variable=self.lightgbm_subsample_085).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(lgbm_subsample_frame, text="1.0", variable=self.lightgbm_subsample_10).grid(row=0, column=4, padx=5)
        ttk.Label(lgbm_subsample_frame, text="Custom:", style='TLabel').grid(row=0, column=5, padx=(15, 5))
        ttk.Entry(lgbm_subsample_frame, textvariable=self.lightgbm_subsample_custom, width=10).grid(row=0, column=6, padx=5)
        ttk.Label(lgbm_subsample_frame, text="(default: 0.8)", style='Caption.TLabel').grid(row=0, column=7, padx=10)

        ttk.Label(lgbm_content_frame, text="💡 Also called bagging_fraction",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=7, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # colsample_bytree (feature sampling, aka feature_fraction)
        ttk.Label(lgbm_content_frame, text="Feature Sampling (colsample_bytree / feature_fraction):", style='Subheading.TLabel').grid(row=8, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lgbm_colsample_frame = ttk.Frame(lgbm_content_frame)
        lgbm_colsample_frame.grid(row=9, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_colsample_frame, text="0.5", variable=self.lightgbm_colsample_bytree_05).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_colsample_frame, text="0.7", variable=self.lightgbm_colsample_bytree_07).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_colsample_frame, text="0.8 ⭐", variable=self.lightgbm_colsample_bytree_08).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(lgbm_colsample_frame, text="0.85", variable=self.lightgbm_colsample_bytree_085).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(lgbm_colsample_frame, text="1.0", variable=self.lightgbm_colsample_bytree_10).grid(row=0, column=4, padx=5)
        ttk.Label(lgbm_colsample_frame, text="Custom:", style='TLabel').grid(row=0, column=5, padx=(15, 5))
        ttk.Entry(lgbm_colsample_frame, textvariable=self.lightgbm_colsample_bytree_custom, width=10).grid(row=0, column=6, padx=5)
        ttk.Label(lgbm_colsample_frame, text="(default: 0.8)", style='Caption.TLabel').grid(row=0, column=7, padx=10)

        ttk.Label(lgbm_content_frame, text="💡 Also called feature_fraction",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=10, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # reg_alpha (L1 regularization)
        ttk.Label(lgbm_content_frame, text="L1 Regularization (reg_alpha):", style='Subheading.TLabel').grid(row=11, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lgbm_reg_alpha_frame = ttk.Frame(lgbm_content_frame)
        lgbm_reg_alpha_frame.grid(row=12, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_reg_alpha_frame, text="0.0", variable=self.lightgbm_reg_alpha_00).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_reg_alpha_frame, text="0.1 ⭐", variable=self.lightgbm_reg_alpha_01).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_reg_alpha_frame, text="0.5", variable=self.lightgbm_reg_alpha_05).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(lgbm_reg_alpha_frame, text="1.0", variable=self.lightgbm_reg_alpha_10).grid(row=0, column=3, padx=5)
        ttk.Label(lgbm_reg_alpha_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5))
        ttk.Entry(lgbm_reg_alpha_frame, textvariable=self.lightgbm_reg_alpha_custom, width=10).grid(row=0, column=5, padx=5)
        ttk.Label(lgbm_reg_alpha_frame, text="(default: 0.1)", style='Caption.TLabel').grid(row=0, column=6, padx=10)

        # reg_lambda (L2 regularization)
        ttk.Label(lgbm_content_frame, text="L2 Regularization (reg_lambda):", style='Subheading.TLabel').grid(row=13, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lgbm_reg_lambda_frame = ttk.Frame(lgbm_content_frame)
        lgbm_reg_lambda_frame.grid(row=14, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_reg_lambda_frame, text="0.0", variable=self.lightgbm_reg_lambda_00).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_reg_lambda_frame, text="0.5", variable=self.lightgbm_reg_lambda_05).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_reg_lambda_frame, text="1.0 ⭐", variable=self.lightgbm_reg_lambda_10).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(lgbm_reg_lambda_frame, text="2.0", variable=self.lightgbm_reg_lambda_20).grid(row=0, column=3, padx=5)
        ttk.Label(lgbm_reg_lambda_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5))
        ttk.Entry(lgbm_reg_lambda_frame, textvariable=self.lightgbm_reg_lambda_custom, width=10).grid(row=0, column=5, padx=5)
        ttk.Label(lgbm_reg_lambda_frame, text="(default: 1.0)", style='Caption.TLabel').grid(row=0, column=6, padx=10)

        # n_estimators (number of boosting rounds)
        ttk.Label(lgbm_content_frame, text="Number of Boosting Rounds (n_estimators):", style='Subheading.TLabel').grid(row=15, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lgbm_n_estimators_frame = ttk.Frame(lgbm_content_frame)
        lgbm_n_estimators_frame.grid(row=16, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_n_estimators_frame, text="50", variable=self.lightgbm_n_estimators_50).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_n_estimators_frame, text="100 ⭐", variable=self.lightgbm_n_estimators_100).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_n_estimators_frame, text="200 ⭐", variable=self.lightgbm_n_estimators_200).grid(row=0, column=2, padx=5)
        ttk.Label(lgbm_n_estimators_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(lgbm_n_estimators_frame, textvariable=self.lightgbm_n_estimators_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(lgbm_n_estimators_frame, text="(default: 100, 200)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        ttk.Label(lgbm_content_frame, text="💡 More rounds = better fit but slower training",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=17, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # learning_rate (step size shrinkage)
        ttk.Label(lgbm_content_frame, text="Learning Rate (learning_rate):", style='Subheading.TLabel').grid(row=18, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lgbm_lr_frame = ttk.Frame(lgbm_content_frame)
        lgbm_lr_frame.grid(row=19, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_lr_frame, text="0.05", variable=self.lightgbm_lr_005).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_lr_frame, text="0.1 ⭐", variable=self.lightgbm_lr_01).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_lr_frame, text="0.2", variable=self.lightgbm_lr_02).grid(row=0, column=2, padx=5)
        ttk.Label(lgbm_lr_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(lgbm_lr_frame, textvariable=self.lightgbm_lr_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(lgbm_lr_frame, text="(default: 0.1)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        ttk.Label(lgbm_content_frame, text="💡 Lower = more conservative but needs more rounds",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=20, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # num_leaves (max number of leaves in one tree)
        ttk.Label(lgbm_content_frame, text="Maximum Number of Leaves (num_leaves):", style='Subheading.TLabel').grid(row=21, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        lgbm_num_leaves_frame = ttk.Frame(lgbm_content_frame)
        lgbm_num_leaves_frame.grid(row=22, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(lgbm_num_leaves_frame, text="31 ⭐", variable=self.lightgbm_num_leaves_31).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(lgbm_num_leaves_frame, text="50 ⭐", variable=self.lightgbm_num_leaves_50).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(lgbm_num_leaves_frame, text="70", variable=self.lightgbm_num_leaves_70).grid(row=0, column=2, padx=5)
        ttk.Label(lgbm_num_leaves_frame, text="Custom:", style='TLabel').grid(row=0, column=3, padx=(15, 5))
        ttk.Entry(lgbm_num_leaves_frame, textvariable=self.lightgbm_num_leaves_custom, width=10).grid(row=0, column=4, padx=5)
        ttk.Label(lgbm_num_leaves_frame, text="(default: 31, 50)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        ttk.Label(lgbm_content_frame, text="💡 Controls tree complexity (2^max_depth in XGBoost terms)",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=23, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # CatBoost Hyperparameters - collapsible
        catboost_section, catboost_content = self._create_collapsible_section(content_frame,
                                                                               "CatBoost Hyperparameters (Yandex Gradient Boosting)",
                                                                               expanded=False)
        catboost_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        catboost_card_outer, catboost_frame = self._create_card(catboost_content,
                                                                  subtitle="Comprehensive hyperparameter tuning for CatBoost")
        catboost_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout (card uses pack internally)
        catboost_content_frame = ttk.Frame(catboost_frame)
        catboost_content_frame.pack(fill='both', expand=True)

        # iterations (number of boosting rounds, equivalent to n_estimators)
        ttk.Label(catboost_content_frame, text="Number of Boosting Rounds (iterations):", style='Subheading.TLabel').grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        catboost_iterations_frame = ttk.Frame(catboost_content_frame)
        catboost_iterations_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(catboost_iterations_frame, text="100 ⭐", variable=self.catboost_iterations_100).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(catboost_iterations_frame, text="200 ⭐", variable=self.catboost_iterations_200).grid(row=0, column=1, padx=5)
        ttk.Label(catboost_iterations_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(catboost_iterations_frame, textvariable=self.catboost_iterations_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(catboost_iterations_frame, text="(default: 100, 200)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(catboost_content_frame, text="💡 More iterations = better fit but slower training",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # learning_rate (step size shrinkage)
        ttk.Label(catboost_content_frame, text="Learning Rate (learning_rate):", style='Subheading.TLabel').grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        catboost_lr_frame = ttk.Frame(catboost_content_frame)
        catboost_lr_frame.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(catboost_lr_frame, text="0.05 ⭐", variable=self.catboost_lr_005).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(catboost_lr_frame, text="0.1 ⭐", variable=self.catboost_lr_01).grid(row=0, column=1, padx=5)
        ttk.Label(catboost_lr_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(catboost_lr_frame, textvariable=self.catboost_lr_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(catboost_lr_frame, text="(default: 0.05, 0.1)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(catboost_content_frame, text="💡 Lower = more conservative, higher = faster learning",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # depth (tree depth)
        ttk.Label(catboost_content_frame, text="Tree Depth (depth):", style='Subheading.TLabel').grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        catboost_depth_frame = ttk.Frame(catboost_content_frame)
        catboost_depth_frame.grid(row=7, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(catboost_depth_frame, text="4 ⭐", variable=self.catboost_depth_4).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(catboost_depth_frame, text="6 ⭐", variable=self.catboost_depth_6).grid(row=0, column=1, padx=5)
        ttk.Label(catboost_depth_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(catboost_depth_frame, textvariable=self.catboost_depth_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(catboost_depth_frame, text="(default: 4, 6)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(catboost_content_frame, text="💡 Deeper trees = more complex model, may overfit",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=8, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # l2_leaf_reg (L2 regularization coefficient)
        ttk.Label(catboost_content_frame, text="L2 Leaf Regularization (l2_leaf_reg):", style='Subheading.TLabel').grid(row=9, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        catboost_l2_leaf_reg_frame = ttk.Frame(catboost_content_frame)
        catboost_l2_leaf_reg_frame.grid(row=10, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(catboost_l2_leaf_reg_frame, text="1.0", variable=self.catboost_l2_leaf_reg_10).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(catboost_l2_leaf_reg_frame, text="3.0 ⭐", variable=self.catboost_l2_leaf_reg_30).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(catboost_l2_leaf_reg_frame, text="10.0", variable=self.catboost_l2_leaf_reg_100).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(catboost_l2_leaf_reg_frame, text="30.0", variable=self.catboost_l2_leaf_reg_300).grid(row=0, column=3, padx=5)
        ttk.Label(catboost_l2_leaf_reg_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5))
        ttk.Entry(catboost_l2_leaf_reg_frame, textvariable=self.catboost_l2_leaf_reg_custom, width=10).grid(row=0, column=5, padx=5)
        ttk.Label(catboost_l2_leaf_reg_frame, text="(default: 3.0)", style='Caption.TLabel').grid(row=0, column=6, padx=10)

        # border_count (number of splits for numerical features)
        ttk.Label(catboost_content_frame, text="Border Count (border_count):", style='Subheading.TLabel').grid(row=11, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        catboost_border_count_frame = ttk.Frame(catboost_content_frame)
        catboost_border_count_frame.grid(row=12, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(catboost_border_count_frame, text="32", variable=self.catboost_border_count_32).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(catboost_border_count_frame, text="64", variable=self.catboost_border_count_64).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(catboost_border_count_frame, text="128", variable=self.catboost_border_count_128).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(catboost_border_count_frame, text="254 ⭐", variable=self.catboost_border_count_254).grid(row=0, column=3, padx=5)
        ttk.Label(catboost_border_count_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5))
        ttk.Entry(catboost_border_count_frame, textvariable=self.catboost_border_count_custom, width=10).grid(row=0, column=5, padx=5)
        ttk.Label(catboost_border_count_frame, text="(default: 254)", style='Caption.TLabel').grid(row=0, column=6, padx=10)

        ttk.Label(catboost_content_frame, text="💡 Higher = more precise but slower",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=13, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # bagging_temperature (Bayesian bagging intensity)
        ttk.Label(catboost_content_frame, text="Bagging Temperature (bagging_temperature):", style='Subheading.TLabel').grid(row=14, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        catboost_bagging_temp_frame = ttk.Frame(catboost_content_frame)
        catboost_bagging_temp_frame.grid(row=15, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(catboost_bagging_temp_frame, text="0.0", variable=self.catboost_bagging_temperature_00).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(catboost_bagging_temp_frame, text="0.5", variable=self.catboost_bagging_temperature_05).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(catboost_bagging_temp_frame, text="1.0 ⭐", variable=self.catboost_bagging_temperature_10).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(catboost_bagging_temp_frame, text="3.0", variable=self.catboost_bagging_temperature_30).grid(row=0, column=3, padx=5)
        ttk.Label(catboost_bagging_temp_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5))
        ttk.Entry(catboost_bagging_temp_frame, textvariable=self.catboost_bagging_temperature_custom, width=10).grid(row=0, column=5, padx=5)
        ttk.Label(catboost_bagging_temp_frame, text="(default: 1.0)", style='Caption.TLabel').grid(row=0, column=6, padx=10)

        ttk.Label(catboost_content_frame, text="💡 Higher = more aggressive sampling",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=16, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # random_strength (randomness for scoring splits)
        ttk.Label(catboost_content_frame, text="Random Strength (random_strength):", style='Subheading.TLabel').grid(row=17, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        catboost_random_strength_frame = ttk.Frame(catboost_content_frame)
        catboost_random_strength_frame.grid(row=18, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(catboost_random_strength_frame, text="0.5", variable=self.catboost_random_strength_05).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(catboost_random_strength_frame, text="1.0 ⭐", variable=self.catboost_random_strength_10).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(catboost_random_strength_frame, text="2.0", variable=self.catboost_random_strength_20).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(catboost_random_strength_frame, text="5.0", variable=self.catboost_random_strength_50).grid(row=0, column=3, padx=5)
        ttk.Label(catboost_random_strength_frame, text="Custom:", style='TLabel').grid(row=0, column=4, padx=(15, 5))
        ttk.Entry(catboost_random_strength_frame, textvariable=self.catboost_random_strength_custom, width=10).grid(row=0, column=5, padx=5)
        ttk.Label(catboost_random_strength_frame, text="(default: 1.0)", style='Caption.TLabel').grid(row=0, column=6, padx=10)

        # MLP Hyperparameters - collapsible
        mlp_section, mlp_content = self._create_collapsible_section(content_frame,
                                                                    "MLP (Multi-Layer Perceptron) Hyperparameters",
                                                                    expanded=False)
        mlp_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        mlp_card_outer, mlp_frame = self._create_card(mlp_content,
                                                      subtitle="Configure neural network activation, solver, and training parameters")
        mlp_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout
        mlp_content_frame = ttk.Frame(mlp_frame)
        mlp_content_frame.pack(fill='both', expand=True)

        # Hidden layer sizes (network architecture)
        ttk.Label(mlp_content_frame, text="Hidden Layer Sizes (hidden_layer_sizes):", style='Subheading.TLabel').grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        mlp_hidden_frame = ttk.Frame(mlp_content_frame)
        mlp_hidden_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(mlp_hidden_frame, text="(64,) ⭐", variable=self.mlp_hidden_64).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(mlp_hidden_frame, text="(128,64) ⭐", variable=self.mlp_hidden_128_64).grid(row=0, column=1, padx=5)
        ttk.Label(mlp_hidden_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(mlp_hidden_frame, textvariable=self.mlp_hidden_custom, width=15).grid(row=0, column=3, padx=5)
        ttk.Label(mlp_hidden_frame, text='(e.g., "100,50,25")', style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(mlp_content_frame, text="💡 Tuple of hidden layer sizes, e.g., (64,) = 1 layer with 64 neurons",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Alpha (L2 regularization penalty)
        ttk.Label(mlp_content_frame, text="L2 Regularization (alpha):", style='Subheading.TLabel').grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        mlp_alpha_frame = ttk.Frame(mlp_content_frame)
        mlp_alpha_frame.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(mlp_alpha_frame, text="0.001 ⭐", variable=self.mlp_alpha_1e3).grid(row=0, column=0, padx=5)
        ttk.Label(mlp_alpha_frame, text="Custom:", style='TLabel').grid(row=0, column=1, padx=(15, 5))
        ttk.Entry(mlp_alpha_frame, textvariable=self.mlp_alpha_custom, width=10).grid(row=0, column=2, padx=5)
        ttk.Label(mlp_alpha_frame, text="(default: 0.001)", style='Caption.TLabel').grid(row=0, column=3, padx=10)

        ttk.Label(mlp_content_frame, text="💡 Higher values = more regularization (prevents overfitting)",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Learning rate initial value
        ttk.Label(mlp_content_frame, text="Initial Learning Rate (learning_rate_init):", style='Subheading.TLabel').grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        mlp_lr_init_frame = ttk.Frame(mlp_content_frame)
        mlp_lr_init_frame.grid(row=7, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(mlp_lr_init_frame, text="0.001 ⭐", variable=self.mlp_lr_init_1e3).grid(row=0, column=0, padx=5)
        ttk.Label(mlp_lr_init_frame, text="Custom:", style='TLabel').grid(row=0, column=1, padx=(15, 5))
        ttk.Entry(mlp_lr_init_frame, textvariable=self.mlp_lr_init_custom, width=10).grid(row=0, column=2, padx=5)
        ttk.Label(mlp_lr_init_frame, text="(default: 0.001)", style='Caption.TLabel').grid(row=0, column=3, padx=10)

        ttk.Label(mlp_content_frame, text="💡 Controls step size for weight updates during training",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=8, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Activation function
        ttk.Label(mlp_content_frame, text="Activation Function:", style='Subheading.TLabel').grid(row=9, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        mlp_activation_frame = ttk.Frame(mlp_content_frame)
        mlp_activation_frame.grid(row=10, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(mlp_activation_frame, text="relu ⭐", variable=self.mlp_activation_relu).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(mlp_activation_frame, text="tanh", variable=self.mlp_activation_tanh).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(mlp_activation_frame, text="logistic", variable=self.mlp_activation_logistic).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(mlp_activation_frame, text="identity", variable=self.mlp_activation_identity).grid(row=0, column=3, padx=5)
        ttk.Label(mlp_activation_frame, text="(default: relu)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(mlp_content_frame, text="💡 relu = rectified linear, tanh = hyperbolic tangent",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=11, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Solver
        ttk.Label(mlp_content_frame, text="Weight Optimization Solver:", style='Subheading.TLabel').grid(row=12, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        mlp_solver_frame = ttk.Frame(mlp_content_frame)
        mlp_solver_frame.grid(row=13, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(mlp_solver_frame, text="adam ⭐", variable=self.mlp_solver_adam).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(mlp_solver_frame, text="sgd", variable=self.mlp_solver_sgd).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(mlp_solver_frame, text="lbfgs", variable=self.mlp_solver_lbfgs).grid(row=0, column=2, padx=5)
        ttk.Label(mlp_solver_frame, text="(default: adam)", style='Caption.TLabel').grid(row=0, column=3, padx=10)

        ttk.Label(mlp_content_frame, text="💡 adam is usually best, sgd allows momentum parameter",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=14, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Batch size
        ttk.Label(mlp_content_frame, text="Batch Size (mini-batches):", style='Subheading.TLabel').grid(row=15, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        mlp_batch_frame = ttk.Frame(mlp_content_frame)
        mlp_batch_frame.grid(row=16, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(mlp_batch_frame, text="auto ⭐", variable=self.mlp_batch_auto).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(mlp_batch_frame, text="32", variable=self.mlp_batch_32).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(mlp_batch_frame, text="64", variable=self.mlp_batch_64).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(mlp_batch_frame, text="128", variable=self.mlp_batch_128).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(mlp_batch_frame, text="256", variable=self.mlp_batch_256).grid(row=0, column=4, padx=5)
        ttk.Label(mlp_batch_frame, text="(default: auto)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        ttk.Label(mlp_content_frame, text="💡 auto = min(200, n_samples)",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=17, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Learning rate schedule
        ttk.Label(mlp_content_frame, text="Learning Rate Schedule:", style='Subheading.TLabel').grid(row=18, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        mlp_lr_sched_frame = ttk.Frame(mlp_content_frame)
        mlp_lr_sched_frame.grid(row=19, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(mlp_lr_sched_frame, text="constant ⭐", variable=self.mlp_lr_schedule_constant).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(mlp_lr_sched_frame, text="invscaling", variable=self.mlp_lr_schedule_invscaling).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(mlp_lr_sched_frame, text="adaptive", variable=self.mlp_lr_schedule_adaptive).grid(row=0, column=2, padx=5)
        ttk.Label(mlp_lr_sched_frame, text="(default: constant)", style='Caption.TLabel').grid(row=0, column=3, padx=10)

        ttk.Label(mlp_content_frame, text="💡 constant = fixed rate, adaptive = reduces when loss plateaus",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=20, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Momentum (for SGD solver)
        ttk.Label(mlp_content_frame, text="Momentum (for SGD solver only):", style='Subheading.TLabel').grid(row=21, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        mlp_momentum_frame = ttk.Frame(mlp_content_frame)
        mlp_momentum_frame.grid(row=22, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(mlp_momentum_frame, text="0.7", variable=self.mlp_momentum_07).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(mlp_momentum_frame, text="0.8", variable=self.mlp_momentum_08).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(mlp_momentum_frame, text="0.9 ⭐", variable=self.mlp_momentum_09).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(mlp_momentum_frame, text="0.95", variable=self.mlp_momentum_095).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(mlp_momentum_frame, text="0.99", variable=self.mlp_momentum_099).grid(row=0, column=4, padx=5)
        ttk.Label(mlp_momentum_frame, text="(default: 0.9)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        ttk.Label(mlp_content_frame, text="💡 Only used when solver='sgd', should be in [0, 1]",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=23, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Max Iterations
        ttk.Label(mlp_content_frame, text="Max Iterations:", style='Subheading.TLabel').grid(row=24, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        mlp_maxiter_frame = ttk.Frame(mlp_content_frame)
        mlp_maxiter_frame.grid(row=25, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Label(mlp_maxiter_frame, text="Max iterations:", style='TLabel').grid(row=0, column=0, padx=(0, 5))
        ttk.Spinbox(mlp_maxiter_frame, from_=100, to=5000, increment=100, textvariable=self.max_iter, width=10).grid(row=0, column=1, padx=5)
        ttk.Label(mlp_maxiter_frame, text="(default: 100)", style='Caption.TLabel').grid(row=0, column=2, padx=10)

        ttk.Label(mlp_content_frame, text="💡 Maximum number of training iterations. Higher values may improve convergence but increase training time.",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=26, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # SVR/SVM Hyperparameters - collapsible
        svr_section, svr_content = self._create_collapsible_section(content_frame,
                                                                    "SVR/SVM (Support Vector) Hyperparameters",
                                                                    expanded=False)
        svr_section.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        row += 1

        # Create card inside collapsible section
        svr_card_outer, svr_frame = self._create_card(svr_content,
                                                      subtitle="Configure epsilon, kernel parameters, and regularization")
        svr_card_outer.pack(fill='both', expand=True, padx=5, pady=5)

        # Create content frame for grid layout
        svr_content_frame = ttk.Frame(svr_frame)
        svr_content_frame.pack(fill='both', expand=True)

        # Kernel type
        ttk.Label(svr_content_frame, text="Kernel Type (kernel):", style='Subheading.TLabel').grid(row=0, column=0, columnspan=6, sticky=tk.W, pady=(0, 5))
        svr_kernel_frame = ttk.Frame(svr_content_frame)
        svr_kernel_frame.grid(row=1, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(svr_kernel_frame, text="rbf ⭐", variable=self.svr_kernel_rbf).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(svr_kernel_frame, text="linear ⭐", variable=self.svr_kernel_linear).grid(row=0, column=1, padx=5)
        ttk.Label(svr_kernel_frame, text="(default: rbf, linear)", style='Caption.TLabel').grid(row=0, column=2, padx=10)

        ttk.Label(svr_content_frame, text="💡 rbf = radial basis function (non-linear), linear = linear kernel",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=2, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # C (regularization parameter)
        ttk.Label(svr_content_frame, text="Regularization Parameter (C):", style='Subheading.TLabel').grid(row=3, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        svr_C_frame = ttk.Frame(svr_content_frame)
        svr_C_frame.grid(row=4, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(svr_C_frame, text="1.0 ⭐", variable=self.svr_C_10).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(svr_C_frame, text="10.0 ⭐", variable=self.svr_C_100).grid(row=0, column=1, padx=5)
        ttk.Label(svr_C_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(svr_C_frame, textvariable=self.svr_C_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(svr_C_frame, text="(default: 1.0, 10.0)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(svr_content_frame, text="💡 Higher C = less regularization (may overfit)",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=5, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Gamma (kernel coefficient)
        ttk.Label(svr_content_frame, text="Gamma (kernel coefficient):", style='Subheading.TLabel').grid(row=6, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        svr_gamma_frame = ttk.Frame(svr_content_frame)
        svr_gamma_frame.grid(row=7, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(svr_gamma_frame, text="scale ⭐", variable=self.svr_gamma_scale).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(svr_gamma_frame, text="auto", variable=self.svr_gamma_auto).grid(row=0, column=1, padx=5)
        ttk.Label(svr_gamma_frame, text="Custom:", style='TLabel').grid(row=0, column=2, padx=(15, 5))
        ttk.Entry(svr_gamma_frame, textvariable=self.svr_gamma_custom, width=10).grid(row=0, column=3, padx=5)
        ttk.Label(svr_gamma_frame, text="(default: scale)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(svr_content_frame, text="💡 scale = 1/(n_features * X.var()), auto = 1/n_features",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=8, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Epsilon
        ttk.Label(svr_content_frame, text="Epsilon (epsilon-insensitive tube width):", style='Subheading.TLabel').grid(row=9, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        svr_epsilon_frame = ttk.Frame(svr_content_frame)
        svr_epsilon_frame.grid(row=10, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(svr_epsilon_frame, text="0.01", variable=self.svr_epsilon_001).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(svr_epsilon_frame, text="0.05", variable=self.svr_epsilon_005).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(svr_epsilon_frame, text="0.1 ⭐", variable=self.svr_epsilon_01).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(svr_epsilon_frame, text="0.2", variable=self.svr_epsilon_02).grid(row=0, column=3, padx=5)
        ttk.Checkbutton(svr_epsilon_frame, text="0.5", variable=self.svr_epsilon_05).grid(row=0, column=4, padx=5)
        ttk.Label(svr_epsilon_frame, text="(default: 0.1)", style='Caption.TLabel').grid(row=0, column=5, padx=10)

        ttk.Label(svr_content_frame, text="💡 Width of epsilon-insensitive tube",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=11, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Degree (for polynomial kernel)
        ttk.Label(svr_content_frame, text="Degree (for polynomial kernel only):", style='Subheading.TLabel').grid(row=12, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        svr_degree_frame = ttk.Frame(svr_content_frame)
        svr_degree_frame.grid(row=13, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(svr_degree_frame, text="2", variable=self.svr_degree_2).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(svr_degree_frame, text="3 ⭐", variable=self.svr_degree_3).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(svr_degree_frame, text="4", variable=self.svr_degree_4).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(svr_degree_frame, text="5", variable=self.svr_degree_5).grid(row=0, column=3, padx=5)
        ttk.Label(svr_degree_frame, text="(default: 3)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(svr_content_frame, text="💡 Only used with kernel='poly'",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=14, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Coef0 (independent term in kernel)
        ttk.Label(svr_content_frame, text="Coef0 (independent term in kernel function):", style='Subheading.TLabel').grid(row=15, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        svr_coef0_frame = ttk.Frame(svr_content_frame)
        svr_coef0_frame.grid(row=16, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(svr_coef0_frame, text="0.0 ⭐", variable=self.svr_coef0_00).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(svr_coef0_frame, text="0.5", variable=self.svr_coef0_05).grid(row=0, column=1, padx=5)
        ttk.Checkbutton(svr_coef0_frame, text="1.0", variable=self.svr_coef0_10).grid(row=0, column=2, padx=5)
        ttk.Checkbutton(svr_coef0_frame, text="2.0", variable=self.svr_coef0_20).grid(row=0, column=3, padx=5)
        ttk.Label(svr_coef0_frame, text="(default: 0.0)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        ttk.Label(svr_content_frame, text="💡 Used with poly and sigmoid kernels",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=17, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # Shrinking heuristic
        ttk.Label(svr_content_frame, text="Shrinking Heuristic:", style='Subheading.TLabel').grid(row=18, column=0, columnspan=6, sticky=tk.W, pady=(15, 5))
        svr_shrinking_frame = ttk.Frame(svr_content_frame)
        svr_shrinking_frame.grid(row=19, column=0, columnspan=6, sticky=tk.W, pady=5)

        ttk.Checkbutton(svr_shrinking_frame, text="True ⭐", variable=self.svr_shrinking_true).grid(row=0, column=0, padx=5)
        ttk.Checkbutton(svr_shrinking_frame, text="False", variable=self.svr_shrinking_false).grid(row=0, column=1, padx=5)
        ttk.Label(svr_shrinking_frame, text="(default: True)", style='Caption.TLabel').grid(row=0, column=2, padx=10)

        ttk.Label(svr_content_frame, text="💡 Can speed up training",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=20, column=0, columnspan=6, sticky=tk.W, pady=(5, 0))

        # CSV export checkbox
        ttk.Checkbutton(content_frame, text="Export preprocessed data CSV (2nd derivative)",
                       variable=self.export_preprocessed_csv).grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(20, 5))
        row += 1

    def _create_tab4d_ensemble_methods(self):
        """Subtab 4D: Ensemble Methods - Intelligent model combination."""
        tab4d = ttk.Frame(self.config_notebook, style='TFrame')
        self.config_notebook.add(tab4d, text='  🎯 Ensemble Methods  ')

        # Create scrollable content
        canvas = tk.Canvas(tab4d, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab4d, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab4d", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Run Analysis button at the top
        run_frame = tk.Frame(content_frame, bg=self.colors['bg'])
        run_frame.grid(row=row, column=0, columnspan=2, sticky='ew', pady=(0, 20))
        self._create_accent_button(run_frame, "▶ Run Analysis", self._run_analysis).pack(side='left')
        row += 1

        # === Ensemble Methods Configuration ===
        self._create_section_header(content_frame, "Ensemble Methods", row=row, columnspan=2)
        row += 1

        # Create card for ensemble methods
        ensemble_card_outer, ensemble_card = self._create_card(content_frame, title="Intelligent Model Combination",
                                                                 subtitle="Combine top models for improved predictions")
        ensemble_card_outer.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1
        # Inner frame for grid layout within card
        ensemble_frame = tk.Frame(ensemble_card, bg=self.colors['card_bg'])
        ensemble_frame.pack(fill='both', expand=True)

        # Enable ensemble checkbox
        ttk.Checkbutton(ensemble_frame, text="Enable Ensemble Methods (combine top models for better predictions)",
                       variable=self.enable_ensembles).grid(row=0, column=0, columnspan=4, sticky=tk.W, pady=(0, 15))

        # Ensemble method selection
        ttk.Label(ensemble_frame, text="Select Ensemble Methods:", style='Subheading.TLabel').grid(row=1, column=0, columnspan=4, sticky=tk.W, pady=(0, 10))

        # Simple Average (baseline)
        ttk.Checkbutton(ensemble_frame, text="Simple Average", variable=self.ensemble_simple_average).grid(row=2, column=0, sticky=tk.W, pady=5, padx=(20, 0))
        ttk.Label(ensemble_frame, text="Equal weight to all models (baseline)",
                 style='Caption.TLabel').grid(row=2, column=1, sticky=tk.W, padx=10)

        # Region-Aware Weighted Ensemble
        ttk.Checkbutton(ensemble_frame, text="Region-Aware Weighted ⭐", variable=self.ensemble_region_weighted).grid(row=3, column=0, sticky=tk.W, pady=5, padx=(20, 0))
        ttk.Label(ensemble_frame, text="Dynamic weights based on prediction region",
                 style='Caption.TLabel').grid(row=3, column=1, sticky=tk.W, padx=10)

        # Mixture of Experts
        ttk.Checkbutton(ensemble_frame, text="Mixture of Experts ⭐", variable=self.ensemble_mixture_experts).grid(row=4, column=0, sticky=tk.W, pady=5, padx=(20, 0))
        ttk.Label(ensemble_frame, text="Select best model per region",
                 style='Caption.TLabel').grid(row=4, column=1, sticky=tk.W, padx=10)

        # Stacking Ensemble
        ttk.Checkbutton(ensemble_frame, text="Stacking Ensemble ⭐", variable=self.ensemble_stacking).grid(row=5, column=0, sticky=tk.W, pady=5, padx=(20, 0))
        ttk.Label(ensemble_frame, text="Meta-learner combines predictions",
                 style='Caption.TLabel').grid(row=5, column=1, sticky=tk.W, padx=10)

        # Stacking with Region Features
        ttk.Checkbutton(ensemble_frame, text="Stacking + Region Features", variable=self.ensemble_stacking_region).grid(row=6, column=0, sticky=tk.W, pady=5, padx=(20, 0))
        ttk.Label(ensemble_frame, text="Stacking with region-aware meta-features",
                 style='Caption.TLabel').grid(row=6, column=1, sticky=tk.W, padx=10)

        # Number of regions parameter
        ttk.Label(ensemble_frame, text="Number of Regions:", style='Subheading.TLabel').grid(row=7, column=0, sticky=tk.W, pady=(15, 5), padx=(20, 0))

        regions_frame = ttk.Frame(ensemble_frame)
        regions_frame.grid(row=8, column=0, columnspan=4, sticky=tk.W, pady=5, padx=(20, 0))

        ttk.Radiobutton(regions_frame, text="3", variable=self.ensemble_n_regions, value=3).grid(row=0, column=0, padx=5)
        ttk.Radiobutton(regions_frame, text="5 ⭐", variable=self.ensemble_n_regions, value=5).grid(row=0, column=1, padx=5)
        ttk.Radiobutton(regions_frame, text="7", variable=self.ensemble_n_regions, value=7).grid(row=0, column=2, padx=5)
        ttk.Radiobutton(regions_frame, text="10", variable=self.ensemble_n_regions, value=10).grid(row=0, column=3, padx=5)

        ttk.Label(regions_frame, text="(default: 5)", style='Caption.TLabel').grid(row=0, column=4, padx=10)

        # Model Selection Configuration
        ttk.Label(ensemble_frame, text="Model Selection:", style='Subheading.TLabel').grid(row=9, column=0, sticky=tk.W, pady=(15, 5), padx=(20, 0))

        selection_frame = ttk.Frame(ensemble_frame)
        selection_frame.grid(row=10, column=0, columnspan=4, sticky=tk.W, pady=5, padx=(20, 0))

        ttk.Label(selection_frame, text="Use top").grid(row=0, column=0, padx=(0, 5))
        ttk.Spinbox(selection_frame, from_=3, to=20, width=5, textvariable=self.ensemble_top_n).grid(row=0, column=1, padx=5)
        ttk.Label(selection_frame, text="models from ranked list").grid(row=0, column=2, padx=(5, 20))

        ttk.Label(selection_frame, text="(Auto-trains with top N. Use 🔄 Train Ensemble button for custom selection)",
                 font=('TkDefaultFont', 8, 'italic')).grid(row=0, column=3, padx=10, sticky='w')

        # Info labels
        ttk.Label(ensemble_frame, text="💡 Ensembles combine multiple models - more models capture diverse strengths across prediction ranges",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=11, column=0, columnspan=4, sticky=tk.W, pady=(15, 5))
        ttk.Label(ensemble_frame, text="💡 Region-based methods identify which models excel at low/mid/high predictions",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=12, column=0, columnspan=4, sticky=tk.W, pady=(5, 0))

        # Run button - use modern gradient button
        run_btn = self._create_button_with_gradient(content_frame, text="▶ Run Analysis",
                                                     command=self._run_analysis, style='accent')
        run_btn.grid(row=row, column=0, columnspan=2, pady=30)
        row += 1

        self.tab3_status = ttk.Label(content_frame, text="Configure analysis settings above", style='Caption.TLabel')
        self.tab3_status.grid(row=row, column=0, columnspan=2)

    def _create_tab4e_validation(self):
        """Subtab 4E: Validation - Holdout validation set configuration."""
        tab4e = ttk.Frame(self.config_notebook, style='TFrame')
        self.config_notebook.add(tab4e, text='  ✓ Validation  ')

        # Create scrollable content
        canvas = tk.Canvas(tab4e, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab4e, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab4e", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Run Analysis button at the top
        run_frame = tk.Frame(content_frame, bg=self.colors['bg'])
        run_frame.grid(row=row, column=0, columnspan=2, sticky='ew', pady=(0, 20))
        self._create_accent_button(run_frame, "▶ Run Analysis", self._run_analysis).pack(side='left')
        row += 1

        # === Validation Set Configuration ===
        self._create_section_header(content_frame, "Validation Set Configuration", row=row, columnspan=2)
        row += 1

        # Create card for validation set
        validation_card_outer, validation_card = self._create_card(content_frame, title="Holdout Validation Set",
                                                                     subtitle="Configure independent validation samples")
        validation_card_outer.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10, padx=5)
        row += 1
        # Inner frame for grid layout within card
        validation_frame = tk.Frame(validation_card, bg=self.colors['card_bg'])
        validation_frame.pack(fill='both', expand=True)

        # Enable checkbox
        ttk.Checkbutton(validation_frame, text="Enable Validation Set (holdout samples for independent testing)",
                       variable=self.validation_enabled).grid(row=0, column=0, columnspan=3, sticky=tk.W, pady=(0, 10))

        # Percentage slider
        ttk.Label(validation_frame, text="Validation Set Size:", style='Subheading.TLabel').grid(row=1, column=0, sticky=tk.W, pady=5)

        val_pct_frame = ttk.Frame(validation_frame)
        val_pct_frame.grid(row=2, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=5)

        ttk.Scale(val_pct_frame, from_=5, to=40, variable=self.validation_percentage, orient=tk.HORIZONTAL, length=300).pack(side=tk.LEFT, padx=(0, 10))

        val_pct_display = ttk.Label(val_pct_frame, text="20%")
        val_pct_display.pack(side=tk.LEFT)

        # Update label when slider changes
        def update_pct_label(*args):
            val_pct_display.config(text=f"{int(self.validation_percentage.get())}%")
        self.validation_percentage.trace_add('write', update_pct_label)

        # Algorithm selection
        ttk.Label(validation_frame, text="Selection Algorithm:", style='Subheading.TLabel').grid(row=3, column=0, sticky=tk.W, pady=(15, 5))

        algo_frame = ttk.Frame(validation_frame)
        algo_frame.grid(row=4, column=0, columnspan=3, sticky=tk.W, pady=5)

        ttk.Radiobutton(algo_frame, text="Kennard-Stone",
                       variable=self.validation_algorithm, value="Kennard-Stone").grid(row=0, column=0, sticky=tk.W, padx=(0, 15))
        ttk.Label(algo_frame, text="Maximizes spectral diversity only (ignores y distribution)",
                 style='Caption.TLabel').grid(row=0, column=1, sticky=tk.W)

        ttk.Radiobutton(algo_frame, text="SPXY ⭐",
                       variable=self.validation_algorithm, value="SPXY").grid(row=1, column=0, sticky=tk.W, padx=(0, 15), pady=3)
        ttk.Label(algo_frame, text="Balances spectral and target diversity (recommended)",
                 style='Caption.TLabel').grid(row=1, column=1, sticky=tk.W, pady=3)

        ttk.Radiobutton(algo_frame, text="Random",
                       variable=self.validation_algorithm, value="Random").grid(row=2, column=0, sticky=tk.W, padx=(0, 15), pady=3)
        ttk.Label(algo_frame, text="Simple random selection",
                 style='Caption.TLabel').grid(row=2, column=1, sticky=tk.W, pady=3)

        ttk.Radiobutton(algo_frame, text="Stratified",
                       variable=self.validation_algorithm, value="Stratified").grid(row=3, column=0, sticky=tk.W, padx=(0, 15), pady=3)
        ttk.Label(algo_frame, text="Ensures balanced target variable distribution",
                 style='Caption.TLabel').grid(row=3, column=1, sticky=tk.W, pady=3)

        # Buttons
        button_frame = ttk.Frame(validation_frame)
        button_frame.grid(row=5, column=0, columnspan=3, sticky=tk.W, pady=(15, 5))

        ttk.Button(button_frame, text="Create Validation Set", command=self._create_validation_set, style='Modern.TButton').pack(side=tk.LEFT, padx=(0, 10))
        ttk.Button(button_frame, text="Reset", command=self._reset_validation_set, style='Modern.TButton').pack(side=tk.LEFT)

        # Status label
        self.validation_status_label = ttk.Label(validation_frame, text="No validation set created", style='Caption.TLabel')
        self.validation_status_label.grid(row=6, column=0, columnspan=3, sticky=tk.W, pady=(10, 0))

        ttk.Label(validation_frame, text="💡 Validation set will be held out during model training and used for independent testing",
                 style='Caption.TLabel', foreground=self.colors['accent']).grid(row=7, column=0, columnspan=3, sticky=tk.W, pady=(10, 0))

        # Run button - use modern gradient button
        run_btn = self._create_button_with_gradient(content_frame, text="▶ Run Analysis",
                                                     command=self._run_analysis, style='accent')
        run_btn.grid(row=row, column=0, columnspan=2, pady=30)
        row += 1

        self.tab3_status = ttk.Label(content_frame, text="Configure analysis settings above", style='Caption.TLabel')
        self.tab3_status.grid(row=row, column=0, columnspan=2)

    def _create_tab5_progress(self):
        """Tab 5: Analysis Progress - Live progress monitor."""
        self.tab5 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab5, text='  ⏳ Analysis Progress  ')

        content_frame = ttk.Frame(self.tab5, style='TFrame', padding="30")
        content_frame.pack(fill='both', expand=True)

        # Header with title and best model info side-by-side
        header_frame = ttk.Frame(content_frame)
        header_frame.pack(fill='x', pady=(0, 20))

        # Left side: Animated running figure
        title_container = ttk.Frame(header_frame)
        title_container.pack(side='left', anchor=tk.W)

        # Animated running figure
        self.running_figure = self._create_running_figure(title_container, size=40)
        self.running_figure.pack(side='left', padx=(15, 0))

        # Right side: Best model info
        best_model_frame = ttk.Frame(header_frame)
        best_model_frame.pack(side='right', anchor=tk.E)

        ttk.Label(best_model_frame, text="Best Model So Far:", style='Heading.TLabel').pack(anchor=tk.E)
        self.best_model_info = ttk.Label(best_model_frame, text="(none yet)", style='Caption.TLabel', foreground=self.colors['success'])
        self.best_model_info.pack(anchor=tk.E)

        # Progress info with time estimate
        progress_info_frame = ttk.Frame(content_frame)
        progress_info_frame.pack(fill='x', pady=10)

        self.progress_info = ttk.Label(progress_info_frame, text="No analysis running", style='Heading.TLabel')
        self.progress_info.pack(side='left', anchor=tk.W)

        self.time_estimate_label = ttk.Label(progress_info_frame, text="", style='Caption.TLabel')
        self.time_estimate_label.pack(side='right', anchor=tk.E)

        # Progress text area
        self.progress_text = tk.Text(content_frame, height=30, width=120, font=('Consolas', 10),
                                     bg=self.colors['panel'], fg=self.colors['text'],
                                     relief='flat', borderwidth=0,
                                     selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        self.progress_text.pack(fill='both', expand=True, pady=10)

        scrollbar = ttk.Scrollbar(content_frame, command=self.progress_text.yview)
        scrollbar.pack(side='right', fill='y')
        self.progress_text.config(yscrollcommand=scrollbar.set)

        # Status
        self.progress_status = ttk.Label(content_frame, text="Waiting for analysis to start...", style='Caption.TLabel')
        self.progress_status.pack(pady=10)

    def _create_tab6_results(self):
        """Tab 6: Results - Display analysis results in a table."""
        self.tab6 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab6, text='  📊 Results  ')

        content_frame = ttk.Frame(self.tab6, style='TFrame', padding="30")
        content_frame.pack(fill='both', expand=True)

        # Create card for results table
        results_card_outer, results_card = self._create_card(content_frame, title="Model Performance Results",
                                                              subtitle="All tested models ranked by performance. Click on any result row to load it into the 'Model Development' tab for further tuning.")
        results_card_outer.pack(fill='both', expand=True, pady=10, padx=5)

        # Create frame for treeview and scrollbars
        tree_frame = ttk.Frame(results_card)
        tree_frame.pack(fill='both', expand=True)

        # Create Treeview with scrollbars
        self.results_tree = ttk.Treeview(tree_frame, show='headings', selectmode='browse')

        vsb = ttk.Scrollbar(tree_frame, orient="vertical", command=self.results_tree.yview)
        hsb = ttk.Scrollbar(tree_frame, orient="horizontal", command=self.results_tree.xview)
        self.results_tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)

        self.results_tree.grid(row=0, column=0, sticky='nsew')
        vsb.grid(row=0, column=1, sticky='ns')
        hsb.grid(row=1, column=0, sticky='ew')

        tree_frame.grid_rowconfigure(0, weight=1)
        tree_frame.grid_columnconfigure(0, weight=1)

        # Bind double-click event
        self.results_tree.bind('<Double-Button-1>', self._on_result_double_click)
        # Bind single-click event for checkbox toggling
        self.results_tree.bind('<Button-1>', self._on_result_click)

        # Button frame for actions (inside card)
        button_frame = ttk.Frame(results_card)
        button_frame.pack(pady=10, fill='x')

        # Use modern gradient button for export
        export_btn = self._create_button_with_gradient(button_frame, text="📥 Export Results to CSV",
                                                        command=self._export_results_table, style='secondary')
        export_btn.pack(side='left', padx=5)

        # Status label (inside card)
        self.results_status = ttk.Label(results_card, text="No results yet. Run an analysis to see results here.",
                                       style='Caption.TLabel')
        self.results_status.pack(pady=5)

        # === Ensemble Results Section ===
        ensemble_separator = ttk.Separator(content_frame, orient='horizontal')
        ensemble_separator.pack(fill='x', pady=20)

        # Create card for ensemble results
        ensemble_card_outer, ensemble_card = self._create_card(content_frame, title="Ensemble Model Results",
                                                                subtitle="Ensemble methods combine multiple models to improve predictions. Results appear here after running analysis with ensembles enabled.")
        ensemble_card_outer.pack(fill='both', expand=True, pady=10, padx=5)

        # Ensemble results frame
        self.ensemble_frame = ttk.Frame(ensemble_card)
        self.ensemble_frame.pack(fill='both', expand=False, pady=10)

        # Ensemble table
        ensemble_tree_frame = ttk.Frame(self.ensemble_frame)
        ensemble_tree_frame.pack(fill='both', expand=True)

        self.ensemble_tree = ttk.Treeview(ensemble_tree_frame, show='headings', selectmode='browse', height=5)

        ensemble_vsb = ttk.Scrollbar(ensemble_tree_frame, orient="vertical", command=self.ensemble_tree.yview)
        ensemble_hsb = ttk.Scrollbar(ensemble_tree_frame, orient="horizontal", command=self.ensemble_tree.xview)
        self.ensemble_tree.configure(yscrollcommand=ensemble_vsb.set, xscrollcommand=ensemble_hsb.set)

        self.ensemble_tree.grid(row=0, column=0, sticky='nsew')
        ensemble_vsb.grid(row=0, column=1, sticky='ns')
        ensemble_hsb.grid(row=1, column=0, sticky='ew')

        ensemble_tree_frame.grid_rowconfigure(0, weight=1)
        ensemble_tree_frame.grid_columnconfigure(0, weight=1)

        # Ensemble buttons frame
        ensemble_buttons = ttk.Frame(self.ensemble_frame)
        ensemble_buttons.pack(pady=10, fill='x')

        self.btn_save_best_ensemble = ttk.Button(ensemble_buttons, text="💾 Save Selected Ensemble",
                   command=self._save_selected_ensemble, style='Modern.TButton', state='disabled')
        self.btn_save_best_ensemble.pack(side='left', padx=5)

        self.btn_train_ensemble = ttk.Button(ensemble_buttons, text="🔄 Train Ensemble",
                   command=self._on_train_ensemble_click, style='Modern.TButton', state='disabled')
        self.btn_train_ensemble.pack(side='left', padx=5)

        self.btn_show_regional_perf = ttk.Button(ensemble_buttons, text="📊 Show Regional Performance",
                   command=self._show_regional_performance, style='Modern.TButton', state='disabled')
        self.btn_show_regional_perf.pack(side='left', padx=5)

        self.btn_show_weights = ttk.Button(ensemble_buttons, text="⚖️ Show Ensemble Weights",
                   command=self._show_ensemble_weights, style='Modern.TButton', state='disabled')
        self.btn_show_weights.pack(side='left', padx=5)

        self.btn_show_specialization = ttk.Button(ensemble_buttons, text="🎯 Show Specialization",
                   command=self._show_specialization_profile, style='Modern.TButton', state='disabled')
        self.btn_show_specialization.pack(side='left', padx=5)

        # Ensemble status
        self.ensemble_status = ttk.Label(self.ensemble_frame,
            text="No ensemble results yet. Enable ensembles in Analysis Configuration and run analysis.",
            style='Caption.TLabel')
        self.ensemble_status.pack(pady=5)

        # Initialize ensemble storage
        self.ensemble_results = None
        self.trained_ensembles = None

    def _create_tab7_refine_model(self):
        """Tab 7: Model Development - Interactive model parameter refinement with subtabs."""
        self.tab7 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab7, text='  🔧 Model Development  ')

        # Create nested notebook for subtabs (like Analysis Configuration)
        self.model_dev_notebook = ttk.Notebook(self.tab7)
        self.model_dev_notebook.pack(fill='both', expand=True, padx=20, pady=(0, 20))

        # Create the 4 subtabs
        self._create_tab7a_model_selection()
        self._create_tab7b_feature_engineering()
        self._create_tab7c_model_configuration()
        self._create_tab7d_results_diagnostics()

    def _create_tab7a_model_selection(self):
        """Subtab 7A: Model Selection & Loading."""
        tab7a = ttk.Frame(self.model_dev_notebook, style='TFrame')
        self.model_dev_notebook.add(tab7a, text='  📋 Selection  ')

        # Create scrollable content
        canvas = tk.Canvas(tab7a, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab7a, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab7a", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Instructions
        ttk.Label(content_frame,
            text="Double-click a result from the Results tab to load it here for refinement, or click 'Reset to Defaults' for fresh development.",
            style='Caption.TLabel').grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 20))
        row += 1

        # === Mode Control Frame ===
        mode_frame = ttk.LabelFrame(content_frame, text="Development Mode", padding=10)
        mode_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        row += 1

        # Mode status label
        self.refine_mode_label = ttk.Label(mode_frame, text="Mode: Fresh Development", style='Caption.TLabel')
        self.refine_mode_label.pack(side='left', padx=5)

        # Reset button
        ttk.Button(mode_frame, text="Reset to Defaults",
                   command=self._load_default_parameters, style='Modern.TButton').pack(side='right', padx=5)

        # === Selected Model Info ===
        ttk.Label(content_frame, text="Selected Model Configuration", style='Heading.TLabel').grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 15))
        row += 1

        info_frame = ttk.LabelFrame(content_frame, text="Current Configuration", padding="20")
        info_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        self.refine_model_info = tk.Text(info_frame, height=12, width=80, font=('Consolas', 10),
                                         bg=self.colors['panel'], fg=self.colors['text'], wrap=tk.WORD,
                                         relief='flat', borderwidth=0,
                                         selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        self.refine_model_info.pack(fill='both', expand=True)
        self.refine_model_info.insert('1.0', "No model selected. Double-click a result in the Results tab to load it here.")
        self.refine_model_info.config(state='disabled')

        # Quick Run Button (for convenience - same as Configuration tab button)
        row += 1
        button_frame = tk.Frame(content_frame, bg=self.colors['bg'])
        button_frame.grid(row=row, column=0, columnspan=2, pady=(15, 10))

        self.refine_run_button_selection = self._create_accent_button(button_frame, text="▶ Run Model",
                                                                        command=self._run_refined_model, state='disabled')
        self.refine_run_button_selection.pack(side='left', padx=5)

        ttk.Label(button_frame, text="Tip: Adjust settings in Configuration tab before running",
                 style='Caption.TLabel').pack(side='left', padx=15)

        # Status
        row += 1
        self.refine_status = ttk.Label(content_frame, text="No model loaded", style='Caption.TLabel')
        self.refine_status.grid(row=row, column=0, columnspan=2, pady=10)

        # === Wavelength Selection === (moved from Features tab)
        row += 1
        wl_frame = ttk.LabelFrame(content_frame, text="Wavelength Selection", padding="20")
        wl_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(20, 10))
        row += 1

        # Wavelength presets
        ttk.Label(wl_frame, text="Quick presets:", style='Caption.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        preset_frame = ttk.Frame(wl_frame)
        preset_frame.grid(row=1, column=0, columnspan=2, sticky=tk.W, pady=5)

        ttk.Button(preset_frame, text="All", command=lambda: self._apply_wl_preset('all'),
                   style='Modern.TButton', width=10).pack(side='left', padx=2)
        ttk.Button(preset_frame, text="NIR Only", command=lambda: self._apply_wl_preset('nir'),
                   style='Modern.TButton', width=10).pack(side='left', padx=2)
        ttk.Button(preset_frame, text="Visible", command=lambda: self._apply_wl_preset('visible'),
                   style='Modern.TButton', width=10).pack(side='left', padx=2)
        ttk.Button(preset_frame, text="Custom Range...", command=self._custom_range_dialog,
                   style='Modern.TButton', width=14).pack(side='left', padx=2)

        # Instructions
        ttk.Label(wl_frame, text="Enter wavelengths as individual values or ranges (e.g., 1920, 1930-1940, 1950)",
                  style='Caption.TLabel').grid(row=2, column=0, columnspan=2, sticky=tk.W, pady=(10, 5))

        # Text box for wavelength specification
        wl_spec_frame = ttk.Frame(wl_frame)
        wl_spec_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)

        self.refine_wl_spec = tk.Text(wl_spec_frame, height=8, width=70, font=('Consolas', 9), wrap=tk.WORD,
                                      bg=self.colors['panel'], fg=self.colors['text'],
                                      relief='flat', borderwidth=0,
                                      selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        self.refine_wl_spec.pack(side='left', fill='both', expand=True)

        wl_spec_scrollbar = ttk.Scrollbar(wl_spec_frame, orient='vertical', command=self.refine_wl_spec.yview)
        wl_spec_scrollbar.pack(side='right', fill='y')
        self.refine_wl_spec.config(yscrollcommand=wl_spec_scrollbar.set)

        # Button row
        wl_button_frame = ttk.Frame(wl_frame)
        wl_button_frame.grid(row=4, column=0, columnspan=2, sticky=tk.W, pady=(10, 0))

        ttk.Button(wl_button_frame, text="Preview Selected Wavelengths",
                   command=self._preview_wavelength_selection, style='Modern.TButton').pack(side='left', padx=(0, 10))

        # Wavelength count display (real-time)
        self.refine_wl_count_label = ttk.Label(wl_button_frame, text="Wavelengths: 0 selected",
                                                style='Caption.TLabel')
        self.refine_wl_count_label.pack(side='left')

        # Bind update to text widget for real-time feedback
        self.refine_wl_spec.bind('<KeyRelease>', self._update_wavelength_count)

    def _create_tab7b_feature_engineering(self):
        """Subtab 7B: Feature Engineering (wavelengths, preprocessing)."""
        # REMOVED: Features subtab content moved to Selection and Configuration tabs
        return
        tab7b = ttk.Frame(self.model_dev_notebook, style='TFrame')
        self.model_dev_notebook.add(tab7b, text='  🔬 Features  ')

        # Create scrollable content
        canvas = tk.Canvas(tab7b, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab7b, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab7b", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Instructions
        ttk.Label(content_frame,
            text="Configure which wavelengths and preprocessing methods to use for model development.",
            style='Caption.TLabel').grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 20))
        row += 1

        # === Wavelength Selection ===
        wl_frame = ttk.LabelFrame(content_frame, text="Wavelength Selection", padding="20")
        wl_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Wavelength presets
        ttk.Label(wl_frame, text="Quick presets:", style='Caption.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        preset_frame = ttk.Frame(wl_frame)
        preset_frame.grid(row=1, column=0, columnspan=2, sticky=tk.W, pady=5)

        ttk.Button(preset_frame, text="All", command=lambda: self._apply_wl_preset('all'),
                   style='Modern.TButton', width=10).pack(side='left', padx=2)
        ttk.Button(preset_frame, text="NIR Only", command=lambda: self._apply_wl_preset('nir'),
                   style='Modern.TButton', width=10).pack(side='left', padx=2)
        ttk.Button(preset_frame, text="Visible", command=lambda: self._apply_wl_preset('visible'),
                   style='Modern.TButton', width=10).pack(side='left', padx=2)
        ttk.Button(preset_frame, text="Custom Range...", command=self._custom_range_dialog,
                   style='Modern.TButton', width=14).pack(side='left', padx=2)

        # Instructions
        ttk.Label(wl_frame, text="Enter wavelengths as individual values or ranges (e.g., 1920, 1930-1940, 1950)",
                  style='Caption.TLabel').grid(row=2, column=0, columnspan=2, sticky=tk.W, pady=(10, 5))

        # Text box for wavelength specification
        wl_spec_frame = ttk.Frame(wl_frame)
        wl_spec_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)

        self.refine_wl_spec = tk.Text(wl_spec_frame, height=8, width=70, font=('Consolas', 9), wrap=tk.WORD,
                                      bg=self.colors['panel'], fg=self.colors['text'],
                                      relief='flat', borderwidth=0,
                                      selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        self.refine_wl_spec.pack(side='left', fill='both', expand=True)

        wl_spec_scrollbar = ttk.Scrollbar(wl_spec_frame, orient='vertical', command=self.refine_wl_spec.yview)
        wl_spec_scrollbar.pack(side='right', fill='y')
        self.refine_wl_spec.config(yscrollcommand=wl_spec_scrollbar.set)

        # Button row
        wl_button_frame = ttk.Frame(wl_frame)
        wl_button_frame.grid(row=4, column=0, columnspan=2, sticky=tk.W, pady=(10, 0))

        ttk.Button(wl_button_frame, text="Preview Selected Wavelengths",
                   command=self._preview_wavelength_selection, style='Modern.TButton').pack(side='left', padx=(0, 10))

        # Wavelength count display (real-time)
        self.refine_wl_count_label = ttk.Label(wl_button_frame, text="Wavelengths: 0 selected",
                                                style='Caption.TLabel')
        self.refine_wl_count_label.pack(side='left')

        # Bind update to text widget for real-time feedback
        self.refine_wl_spec.bind('<KeyRelease>', self._update_wavelength_count)

        # === Preprocessing Configuration ===
        preprocess_frame = ttk.LabelFrame(content_frame, text="Preprocessing Configuration", padding="20")
        preprocess_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Preprocessing Method
        ttk.Label(preprocess_frame, text="Preprocessing Method:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        self.refine_preprocess = tk.StringVar(value='raw')
        preprocess_combo = ttk.Combobox(preprocess_frame, textvariable=self.refine_preprocess, width=25, state='readonly')
        preprocess_combo['values'] = ['raw', 'snv', 'sg1', 'sg2', 'snv_sg1', 'snv_sg2', 'deriv_snv']
        preprocess_combo.grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(preprocess_frame, text="Raw: No preprocessing | SNV: Standard Normal Variate | SG1/SG2: Savitzky-Golay derivatives",
                  style='Caption.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(5, 10))

        # Window size (for derivatives)
        ttk.Label(preprocess_frame, text="Window Size (for derivatives):", style='Subheading.TLabel').grid(row=3, column=0, sticky=tk.W, pady=(10, 5))
        self.refine_window = tk.IntVar(value=17)
        self.refine_window_custom = tk.StringVar(value="")
        window_frame = ttk.Frame(preprocess_frame)
        window_frame.grid(row=4, column=0, sticky=tk.W, pady=5)
        for w in [7, 11, 17, 19]:
            ttk.Radiobutton(window_frame, text=f"{w}", variable=self.refine_window, value=w).pack(side='left', padx=5)
        ttk.Label(window_frame, text="Custom:", style='TLabel').pack(side='left', padx=(15, 5))
        ttk.Entry(window_frame, textvariable=self.refine_window_custom, width=8).pack(side='left', padx=5)

    def _create_tab7c_model_configuration(self):
        """Subtab 7C: Model Configuration (model type, hyperparameters, execution)."""
        tab7c = ttk.Frame(self.model_dev_notebook, style='TFrame')
        self.model_dev_notebook.add(tab7c, text='  ⚙️ Configuration  ')

        # Create scrollable content
        canvas = tk.Canvas(tab7c, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab7c, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab7c", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Instructions
        ttk.Label(content_frame,
            text="Configure model type, task, and training parameters before running your refined model.",
            style='Caption.TLabel').grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 20))
        row += 1

        # === Preprocessing Configuration === (moved from Features tab)
        preprocess_frame = ttk.LabelFrame(content_frame, text="Preprocessing Configuration", padding="20")
        preprocess_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Preprocessing Method
        ttk.Label(preprocess_frame, text="Preprocessing Method:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        self.refine_preprocess = tk.StringVar(value='raw')
        preprocess_combo = ttk.Combobox(preprocess_frame, textvariable=self.refine_preprocess, width=25, state='readonly')
        preprocess_combo['values'] = ['raw', 'snv', 'sg1', 'sg2', 'snv_sg1', 'snv_sg2', 'deriv_snv']
        preprocess_combo.grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(preprocess_frame, text="Raw: No preprocessing | SNV: Standard Normal Variate | SG1/SG2: Savitzky-Golay derivatives",
                  style='Caption.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(5, 10))

        # Window size (for derivatives)
        ttk.Label(preprocess_frame, text="Window Size (for derivatives):", style='Subheading.TLabel').grid(row=3, column=0, sticky=tk.W, pady=(10, 5))
        self.refine_window = tk.IntVar(value=17)
        self.refine_window_custom = tk.StringVar(value="")
        window_frame = ttk.Frame(preprocess_frame)
        window_frame.grid(row=4, column=0, sticky=tk.W, pady=5)
        for w in [7, 11, 17, 19]:
            ttk.Radiobutton(window_frame, text=f"{w}", variable=self.refine_window, value=w).pack(side='left', padx=5)
        ttk.Label(window_frame, text="Custom:", style='TLabel').pack(side='left', padx=(15, 5))
        ttk.Entry(window_frame, textvariable=self.refine_window_custom, width=8).pack(side='left', padx=5)

        # === Model Selection ===
        model_frame = ttk.LabelFrame(content_frame, text="Model Selection", padding="20")
        model_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Model Type
        ttk.Label(model_frame, text="Model Type:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        self.refine_model_type = tk.StringVar(value='PLS')
        self.refine_model_type.trace_add('write', self._on_refine_model_changed)  # Bind callback for dynamic hyperparameter display
        self.refine_model_combo = ttk.Combobox(model_frame, textvariable=self.refine_model_type, width=25, state='readonly')
        # Use central model registry for consistency
        if get_supported_models is not None:
            self.refine_model_combo['values'] = get_supported_models('regression')
        else:
            # Fallback if registry import failed
            self.refine_model_combo['values'] = ['PLS', 'Ridge', 'Lasso', 'ElasticNet', 'RandomForest', 'MLP', 'NeuralBoosted', 'SVR', 'XGBoost', 'LightGBM', 'CatBoost']
        self.refine_model_combo.grid(row=1, column=0, sticky=tk.W, pady=5)

        # Task Type
        ttk.Label(model_frame, text="Task Type:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        self.refine_task_type = tk.StringVar(value='regression')
        self.refine_task_type.trace_add('write', lambda *args: self._on_refine_task_type_changed())
        task_frame = ttk.Frame(model_frame)
        task_frame.grid(row=3, column=0, sticky=tk.W, pady=5)
        ttk.Radiobutton(task_frame, text="Regression", variable=self.refine_task_type, value='regression').pack(side='left', padx=5)
        ttk.Radiobutton(task_frame, text="Classification", variable=self.refine_task_type, value='classification').pack(side='left', padx=5)

        # === Model Hyperparameters (Dynamic per Model) ===
        hyperparams_outer_frame = ttk.LabelFrame(content_frame, text="Model Hyperparameters", padding="20")
        hyperparams_outer_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Store all hyperparameter frames for show/hide logic
        self.refine_hyperparam_frames = {}

        # === PLS Hyperparameters ===
        self.refine_hyperparam_frames['PLS'] = ttk.Frame(hyperparams_outer_frame)
        pls_frame = self.refine_hyperparam_frames['PLS']

        ttk.Label(pls_frame, text="Number of Components:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        ttk.Spinbox(pls_frame, from_=1, to=30, textvariable=self.refine_pls_n_components, width=12).grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(pls_frame, text="Max Iterations:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(pls_frame, from_=100, to=5000, increment=100, textvariable=self.refine_pls_max_iter, width=12).grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(pls_frame, text="Tolerance:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        tol_combo = ttk.Combobox(pls_frame, textvariable=self.refine_pls_tol, width=12, state='readonly')
        tol_combo['values'] = ['1e-7', '1e-6', '1e-5', '1e-4']
        tol_combo.grid(row=5, column=0, sticky=tk.W, pady=5)

        # === Ridge Hyperparameters ===
        self.refine_hyperparam_frames['Ridge'] = ttk.Frame(hyperparams_outer_frame)
        ridge_frame = self.refine_hyperparam_frames['Ridge']

        ttk.Label(ridge_frame, text="Alpha (Regularization Strength):", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        alpha_combo = ttk.Combobox(ridge_frame, textvariable=self.refine_ridge_alpha, width=12, state='readonly')
        alpha_combo['values'] = ['0.001', '0.01', '0.1', '1.0', '10.0', '100.0']
        alpha_combo.grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(ridge_frame, text="Solver:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        solver_combo = ttk.Combobox(ridge_frame, textvariable=self.refine_ridge_solver, width=12, state='readonly')
        solver_combo['values'] = ['auto', 'svd', 'cholesky', 'lsqr', 'sag']
        solver_combo.grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(ridge_frame, text="Tolerance:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        tol_combo = ttk.Combobox(ridge_frame, textvariable=self.refine_ridge_tol, width=12, state='readonly')
        tol_combo['values'] = ['1e-5', '1e-4', '1e-3']
        tol_combo.grid(row=5, column=0, sticky=tk.W, pady=5)

        # === Lasso Hyperparameters ===
        self.refine_hyperparam_frames['Lasso'] = ttk.Frame(hyperparams_outer_frame)
        lasso_frame = self.refine_hyperparam_frames['Lasso']

        ttk.Label(lasso_frame, text="Alpha (Regularization Strength):", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        alpha_combo = ttk.Combobox(lasso_frame, textvariable=self.refine_lasso_alpha, width=12, state='readonly')
        alpha_combo['values'] = ['0.001', '0.01', '0.1', '1.0', '10.0']
        alpha_combo.grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(lasso_frame, text="Selection Method:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        selection_combo = ttk.Combobox(lasso_frame, textvariable=self.refine_lasso_selection, width=12, state='readonly')
        selection_combo['values'] = ['cyclic', 'random']
        selection_combo.grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(lasso_frame, text="Tolerance:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        tol_combo = ttk.Combobox(lasso_frame, textvariable=self.refine_lasso_tol, width=12, state='readonly')
        tol_combo['values'] = ['1e-5', '1e-4', '1e-3']
        tol_combo.grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(lasso_frame, text="Max Iterations:", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lasso_frame, from_=100, to=5000, increment=100, textvariable=self.refine_lasso_max_iter, width=12).grid(row=7, column=0, sticky=tk.W, pady=5)

        # === ElasticNet Hyperparameters ===
        self.refine_hyperparam_frames['ElasticNet'] = ttk.Frame(hyperparams_outer_frame)
        elasticnet_frame = self.refine_hyperparam_frames['ElasticNet']

        ttk.Label(elasticnet_frame, text="Alpha (Regularization Strength):", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        alpha_combo = ttk.Combobox(elasticnet_frame, textvariable=self.refine_elasticnet_alpha, width=12, state='readonly')
        alpha_combo['values'] = ['0.01', '0.1', '1.0', '10.0']
        alpha_combo.grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(elasticnet_frame, text="L1 Ratio:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        l1_combo = ttk.Combobox(elasticnet_frame, textvariable=self.refine_elasticnet_l1_ratio, width=12, state='readonly')
        l1_combo['values'] = ['0.1', '0.3', '0.5', '0.7', '0.9']
        l1_combo.grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(elasticnet_frame, text="Selection Method:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        selection_combo = ttk.Combobox(elasticnet_frame, textvariable=self.refine_elasticnet_selection, width=12, state='readonly')
        selection_combo['values'] = ['cyclic', 'random']
        selection_combo.grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(elasticnet_frame, text="Tolerance:", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        tol_combo = ttk.Combobox(elasticnet_frame, textvariable=self.refine_elasticnet_tol, width=12, state='readonly')
        tol_combo['values'] = ['1e-5', '1e-4', '1e-3']
        tol_combo.grid(row=7, column=0, sticky=tk.W, pady=5)

        ttk.Label(elasticnet_frame, text="Max Iterations:", style='Subheading.TLabel').grid(row=8, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(elasticnet_frame, from_=100, to=5000, increment=100, textvariable=self.refine_elasticnet_max_iter, width=12).grid(row=9, column=0, sticky=tk.W, pady=5)

        # === RandomForest Hyperparameters ===
        self.refine_hyperparam_frames['RandomForest'] = ttk.Frame(hyperparams_outer_frame)
        rf_frame = self.refine_hyperparam_frames['RandomForest']

        ttk.Label(rf_frame, text="Number of Trees:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        ttk.Spinbox(rf_frame, from_=50, to=1000, increment=50, textvariable=self.refine_rf_n_estimators, width=12).grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(rf_frame, text="Max Depth:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        depth_combo = ttk.Combobox(rf_frame, textvariable=self.refine_rf_max_depth, width=12, state='readonly')
        depth_combo['values'] = ['None', '10', '20', '30', '50']
        depth_combo.grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(rf_frame, text="Min Samples Split:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(rf_frame, from_=2, to=50, textvariable=self.refine_rf_min_samples_split, width=12).grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(rf_frame, text="Min Samples Leaf:", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(rf_frame, from_=1, to=20, textvariable=self.refine_rf_min_samples_leaf, width=12).grid(row=7, column=0, sticky=tk.W, pady=5)

        ttk.Label(rf_frame, text="Max Features:", style='Subheading.TLabel').grid(row=8, column=0, sticky=tk.W, pady=(15, 5))
        features_combo = ttk.Combobox(rf_frame, textvariable=self.refine_rf_max_features, width=12, state='readonly')
        features_combo['values'] = ['sqrt', 'log2', 'None']
        features_combo.grid(row=9, column=0, sticky=tk.W, pady=5)

        ttk.Label(rf_frame, text="Bootstrap Sampling:", style='Subheading.TLabel').grid(row=10, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Checkbutton(rf_frame, text="Enable Bootstrap", variable=self.refine_rf_bootstrap).grid(row=11, column=0, sticky=tk.W, pady=5)

        ttk.Label(rf_frame, text="Max Leaf Nodes:", style='Subheading.TLabel').grid(row=12, column=0, sticky=tk.W, pady=(15, 5))
        leafnodes_combo = ttk.Combobox(rf_frame, textvariable=self.refine_rf_max_leaf_nodes, width=12, state='readonly')
        leafnodes_combo['values'] = ['None', '50', '100', '200', '500']
        leafnodes_combo.grid(row=13, column=0, sticky=tk.W, pady=5)

        ttk.Label(rf_frame, text="Min Impurity Decrease:", style='Subheading.TLabel').grid(row=14, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(rf_frame, from_=0.0, to=1.0, increment=0.01, textvariable=self.refine_rf_min_impurity_decrease, width=12, format="%.3f").grid(row=15, column=0, sticky=tk.W, pady=5)

        # === XGBoost Hyperparameters ===
        self.refine_hyperparam_frames['XGBoost'] = ttk.Frame(hyperparams_outer_frame)
        xgb_frame = self.refine_hyperparam_frames['XGBoost']

        ttk.Label(xgb_frame, text="Number of Estimators:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        ttk.Spinbox(xgb_frame, from_=50, to=1000, increment=50, textvariable=self.refine_xgb_n_estimators, width=12).grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(xgb_frame, text="Learning Rate:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(xgb_frame, from_=0.01, to=1.0, increment=0.01, textvariable=self.refine_xgb_learning_rate, width=12, format="%.2f").grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(xgb_frame, text="Max Depth:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(xgb_frame, from_=1, to=20, textvariable=self.refine_xgb_max_depth, width=12).grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(xgb_frame, text="Subsample Ratio:", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(xgb_frame, from_=0.1, to=1.0, increment=0.1, textvariable=self.refine_xgb_subsample, width=12, format="%.1f").grid(row=7, column=0, sticky=tk.W, pady=5)

        ttk.Label(xgb_frame, text="Column Sample Ratio:", style='Subheading.TLabel').grid(row=8, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(xgb_frame, from_=0.1, to=1.0, increment=0.1, textvariable=self.refine_xgb_colsample_bytree, width=12, format="%.1f").grid(row=9, column=0, sticky=tk.W, pady=5)

        ttk.Label(xgb_frame, text="Reg Alpha (L1):", style='Subheading.TLabel').grid(row=10, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(xgb_frame, from_=0.0, to=10.0, increment=0.1, textvariable=self.refine_xgb_reg_alpha, width=12, format="%.1f").grid(row=11, column=0, sticky=tk.W, pady=5)

        ttk.Label(xgb_frame, text="Reg Lambda (L2):", style='Subheading.TLabel').grid(row=12, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(xgb_frame, from_=0.0, to=10.0, increment=0.1, textvariable=self.refine_xgb_reg_lambda, width=12, format="%.1f").grid(row=13, column=0, sticky=tk.W, pady=5)

        ttk.Label(xgb_frame, text="Min Child Weight:", style='Subheading.TLabel').grid(row=14, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(xgb_frame, from_=1, to=20, textvariable=self.refine_xgb_min_child_weight, width=12).grid(row=15, column=0, sticky=tk.W, pady=5)

        ttk.Label(xgb_frame, text="Gamma (Min Split Loss):", style='Subheading.TLabel').grid(row=16, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(xgb_frame, from_=0.0, to=5.0, increment=0.1, textvariable=self.refine_xgb_gamma, width=12, format="%.1f").grid(row=17, column=0, sticky=tk.W, pady=5)

        # === LightGBM Hyperparameters ===
        self.refine_hyperparam_frames['LightGBM'] = ttk.Frame(hyperparams_outer_frame)
        lgbm_frame = self.refine_hyperparam_frames['LightGBM']

        ttk.Label(lgbm_frame, text="Number of Estimators:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        ttk.Spinbox(lgbm_frame, from_=50, to=1000, increment=50, textvariable=self.refine_lgbm_n_estimators, width=12).grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(lgbm_frame, text="Learning Rate:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lgbm_frame, from_=0.01, to=1.0, increment=0.01, textvariable=self.refine_lgbm_learning_rate, width=12, format="%.2f").grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(lgbm_frame, text="Num Leaves:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lgbm_frame, from_=10, to=200, increment=5, textvariable=self.refine_lgbm_num_leaves, width=12).grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(lgbm_frame, text="Max Depth (-1 = no limit):", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lgbm_frame, from_=-1, to=50, textvariable=self.refine_lgbm_max_depth, width=12).grid(row=7, column=0, sticky=tk.W, pady=5)

        ttk.Label(lgbm_frame, text="Min Child Samples:", style='Subheading.TLabel').grid(row=8, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lgbm_frame, from_=1, to=200, increment=5, textvariable=self.refine_lgbm_min_child_samples, width=12).grid(row=9, column=0, sticky=tk.W, pady=5)

        ttk.Label(lgbm_frame, text="Subsample Ratio:", style='Subheading.TLabel').grid(row=10, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lgbm_frame, from_=0.1, to=1.0, increment=0.1, textvariable=self.refine_lgbm_subsample, width=12, format="%.1f").grid(row=11, column=0, sticky=tk.W, pady=5)

        ttk.Label(lgbm_frame, text="Column Sample Ratio:", style='Subheading.TLabel').grid(row=12, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lgbm_frame, from_=0.1, to=1.0, increment=0.1, textvariable=self.refine_lgbm_colsample_bytree, width=12, format="%.1f").grid(row=13, column=0, sticky=tk.W, pady=5)

        ttk.Label(lgbm_frame, text="Reg Alpha (L1):", style='Subheading.TLabel').grid(row=14, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lgbm_frame, from_=0.0, to=10.0, increment=0.1, textvariable=self.refine_lgbm_reg_alpha, width=12, format="%.1f").grid(row=15, column=0, sticky=tk.W, pady=5)

        ttk.Label(lgbm_frame, text="Reg Lambda (L2):", style='Subheading.TLabel').grid(row=16, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(lgbm_frame, from_=0.0, to=10.0, increment=0.1, textvariable=self.refine_lgbm_reg_lambda, width=12, format="%.1f").grid(row=17, column=0, sticky=tk.W, pady=5)

        # === CatBoost Hyperparameters ===
        self.refine_hyperparam_frames['CatBoost'] = ttk.Frame(hyperparams_outer_frame)
        catboost_frame = self.refine_hyperparam_frames['CatBoost']

        ttk.Label(catboost_frame, text="Iterations:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        ttk.Spinbox(catboost_frame, from_=50, to=1000, increment=50, textvariable=self.refine_catboost_iterations, width=12).grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(catboost_frame, text="Learning Rate:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(catboost_frame, from_=0.01, to=1.0, increment=0.01, textvariable=self.refine_catboost_learning_rate, width=12, format="%.2f").grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(catboost_frame, text="Depth:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(catboost_frame, from_=1, to=16, textvariable=self.refine_catboost_depth, width=12).grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(catboost_frame, text="L2 Leaf Regularization:", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(catboost_frame, from_=0.1, to=30.0, increment=0.5, textvariable=self.refine_catboost_l2_leaf_reg, width=12, format="%.1f").grid(row=7, column=0, sticky=tk.W, pady=5)

        ttk.Label(catboost_frame, text="Border Count:", style='Subheading.TLabel').grid(row=8, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(catboost_frame, from_=1, to=255, increment=1, textvariable=self.refine_catboost_border_count, width=12).grid(row=9, column=0, sticky=tk.W, pady=5)

        ttk.Label(catboost_frame, text="Bagging Temperature:", style='Subheading.TLabel').grid(row=10, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(catboost_frame, from_=0.0, to=10.0, increment=0.1, textvariable=self.refine_catboost_bagging_temperature, width=12, format="%.1f").grid(row=11, column=0, sticky=tk.W, pady=5)

        ttk.Label(catboost_frame, text="Random Strength:", style='Subheading.TLabel').grid(row=12, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(catboost_frame, from_=0.0, to=10.0, increment=0.1, textvariable=self.refine_catboost_random_strength, width=12, format="%.1f").grid(row=13, column=0, sticky=tk.W, pady=5)

        # === SVR Hyperparameters ===
        self.refine_hyperparam_frames['SVR'] = ttk.Frame(hyperparams_outer_frame)
        svr_frame = self.refine_hyperparam_frames['SVR']

        ttk.Label(svr_frame, text="Kernel:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        kernel_combo = ttk.Combobox(svr_frame, textvariable=self.refine_svr_kernel, width=12, state='readonly')
        kernel_combo['values'] = ['rbf', 'linear', 'poly', 'sigmoid']
        kernel_combo.grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(svr_frame, text="C (Regularization):", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(svr_frame, from_=0.01, to=100.0, increment=0.5, textvariable=self.refine_svr_C, width=12, format="%.2f").grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(svr_frame, text="Gamma:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        gamma_combo = ttk.Combobox(svr_frame, textvariable=self.refine_svr_gamma, width=12, state='readonly')
        gamma_combo['values'] = ['scale', 'auto']
        gamma_combo.grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(svr_frame, text="Epsilon:", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(svr_frame, from_=0.01, to=1.0, increment=0.01, textvariable=self.refine_svr_epsilon, width=12, format="%.2f").grid(row=7, column=0, sticky=tk.W, pady=5)

        ttk.Label(svr_frame, text="Degree (for poly kernel):", style='Subheading.TLabel').grid(row=8, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(svr_frame, from_=1, to=10, textvariable=self.refine_svr_degree, width=12).grid(row=9, column=0, sticky=tk.W, pady=5)

        ttk.Label(svr_frame, text="Coef0 (kernel independent term):", style='Subheading.TLabel').grid(row=10, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(svr_frame, from_=0.0, to=10.0, increment=0.1, textvariable=self.refine_svr_coef0, width=12, format="%.1f").grid(row=11, column=0, sticky=tk.W, pady=5)

        ttk.Label(svr_frame, text="Shrinking Heuristic:", style='Subheading.TLabel').grid(row=12, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Checkbutton(svr_frame, text="Enable Shrinking", variable=self.refine_svr_shrinking).grid(row=13, column=0, sticky=tk.W, pady=5)

        # === MLP Hyperparameters ===
        self.refine_hyperparam_frames['MLP'] = ttk.Frame(hyperparams_outer_frame)
        mlp_frame = self.refine_hyperparam_frames['MLP']

        ttk.Label(mlp_frame, text="Hidden Layer Sizes:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        layers_combo = ttk.Combobox(mlp_frame, textvariable=self.refine_mlp_hidden_layer_sizes, width=12, state='readonly')
        layers_combo['values'] = ['(32,)', '(64,)', '(128,)', '(64,32)', '(128,64)', '(256,128)']
        layers_combo.grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(mlp_frame, text="Alpha (L2 Regularization):", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        alpha_combo = ttk.Combobox(mlp_frame, textvariable=self.refine_mlp_alpha, width=12, state='readonly')
        alpha_combo['values'] = ['0.0001', '0.001', '0.01', '0.1']
        alpha_combo.grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(mlp_frame, text="Activation Function:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        activation_combo = ttk.Combobox(mlp_frame, textvariable=self.refine_mlp_activation, width=12, state='readonly')
        activation_combo['values'] = ['relu', 'tanh', 'logistic', 'identity']
        activation_combo.grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(mlp_frame, text="Solver:", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        solver_combo = ttk.Combobox(mlp_frame, textvariable=self.refine_mlp_solver, width=12, state='readonly')
        solver_combo['values'] = ['adam', 'sgd', 'lbfgs']
        solver_combo.grid(row=7, column=0, sticky=tk.W, pady=5)

        ttk.Label(mlp_frame, text="Learning Rate (Initial):", style='Subheading.TLabel').grid(row=8, column=0, sticky=tk.W, pady=(15, 5))
        lr_init_combo = ttk.Combobox(mlp_frame, textvariable=self.refine_mlp_learning_rate_init, width=12, state='readonly')
        lr_init_combo['values'] = ['0.0001', '0.0005', '0.001', '0.005', '0.01']
        lr_init_combo.grid(row=9, column=0, sticky=tk.W, pady=5)

        ttk.Label(mlp_frame, text="Batch Size:", style='Subheading.TLabel').grid(row=10, column=0, sticky=tk.W, pady=(15, 5))
        batch_combo = ttk.Combobox(mlp_frame, textvariable=self.refine_mlp_batch_size, width=12, state='readonly')
        batch_combo['values'] = ['auto', '32', '64', '128', '256']
        batch_combo.grid(row=11, column=0, sticky=tk.W, pady=5)

        ttk.Label(mlp_frame, text="Learning Rate Schedule:", style='Subheading.TLabel').grid(row=12, column=0, sticky=tk.W, pady=(15, 5))
        lr_schedule_combo = ttk.Combobox(mlp_frame, textvariable=self.refine_mlp_learning_rate, width=12, state='readonly')
        lr_schedule_combo['values'] = ['constant', 'invscaling', 'adaptive']
        lr_schedule_combo.grid(row=13, column=0, sticky=tk.W, pady=5)

        ttk.Label(mlp_frame, text="Momentum (for SGD):", style='Subheading.TLabel').grid(row=14, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(mlp_frame, from_=0.1, to=1.0, increment=0.05, textvariable=self.refine_mlp_momentum, width=12, format="%.2f").grid(row=15, column=0, sticky=tk.W, pady=5)

        ttk.Label(mlp_frame, text="Max Iterations:", style='Subheading.TLabel').grid(row=16, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(mlp_frame, from_=100, to=1000, increment=50, textvariable=self.refine_mlp_max_iter, width=12).grid(row=17, column=0, sticky=tk.W, pady=5)

        # === NeuralBoosted Hyperparameters ===
        self.refine_hyperparam_frames['NeuralBoosted'] = ttk.Frame(hyperparams_outer_frame)
        neuralboosted_frame = self.refine_hyperparam_frames['NeuralBoosted']

        ttk.Label(neuralboosted_frame, text="Number of Estimators:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        ttk.Spinbox(neuralboosted_frame, from_=10, to=500, increment=10, textvariable=self.refine_neuralboosted_n_estimators, width=12).grid(row=1, column=0, sticky=tk.W, pady=5)

        ttk.Label(neuralboosted_frame, text="Learning Rate:", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(neuralboosted_frame, from_=0.01, to=1.0, increment=0.01, textvariable=self.refine_neuralboosted_learning_rate, width=12, format="%.2f").grid(row=3, column=0, sticky=tk.W, pady=5)

        ttk.Label(neuralboosted_frame, text="Hidden Layer Size:", style='Subheading.TLabel').grid(row=4, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Spinbox(neuralboosted_frame, from_=1, to=20, textvariable=self.refine_neuralboosted_hidden_layer_size, width=12).grid(row=5, column=0, sticky=tk.W, pady=5)

        ttk.Label(neuralboosted_frame, text="Activation:", style='Subheading.TLabel').grid(row=6, column=0, sticky=tk.W, pady=(15, 5))
        activation_combo = ttk.Combobox(neuralboosted_frame, textvariable=self.refine_neuralboosted_activation, width=12, state='readonly')
        activation_combo['values'] = ['tanh', 'relu', 'identity', 'logistic']
        activation_combo.grid(row=7, column=0, sticky=tk.W, pady=5)

        ttk.Label(neuralboosted_frame, text="Early Stopping:", style='Subheading.TLabel').grid(row=8, column=0, sticky=tk.W, pady=(15, 5))
        ttk.Checkbutton(neuralboosted_frame, text="Enable Early Stopping", variable=self.refine_neuralboosted_early_stopping).grid(row=9, column=0, sticky=tk.W, pady=5)

        # Initially hide all hyperparameter frames - will show the one matching selected model
        for frame in self.refine_hyperparam_frames.values():
            frame.grid_forget()

        # Show hyperparameters for the default model (PLS)
        if 'PLS' in self.refine_hyperparam_frames:
            self.refine_hyperparam_frames['PLS'].grid(row=0, column=0, sticky=(tk.W, tk.E), pady=5)

        # === Training Parameters ===
        training_frame = ttk.LabelFrame(content_frame, text="Training Parameters", padding="20")
        training_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # CV Folds
        ttk.Label(training_frame, text="Cross-Validation Folds:", style='Subheading.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        self.refine_folds = tk.IntVar(value=5)
        cv_frame = ttk.Frame(training_frame)
        cv_frame.grid(row=1, column=0, sticky=tk.W, pady=5)
        ttk.Spinbox(cv_frame, from_=3, to=10, textvariable=self.refine_folds, width=12).pack(side='left', padx=(0, 5))
        ttk.Label(cv_frame, text="(3-10 folds recommended)", style='Caption.TLabel').pack(side='left')

        # Max iterations (for neural models)
        ttk.Label(training_frame, text="Max Iterations (neural models):", style='Subheading.TLabel').grid(row=2, column=0, sticky=tk.W, pady=(15, 5))
        self.refine_max_iter = tk.IntVar(value=100)
        iter_frame = ttk.Frame(training_frame)
        iter_frame.grid(row=3, column=0, sticky=tk.W, pady=5)
        ttk.Spinbox(iter_frame, from_=100, to=5000, increment=100, textvariable=self.refine_max_iter, width=12).pack(side='left', padx=(0, 5))
        ttk.Label(iter_frame, text="(Only used by MLP and NeuralBoosted models)", style='Caption.TLabel').pack(side='left')

        # === Execution ===
        exec_frame = ttk.LabelFrame(content_frame, text="Execution", padding="20")
        exec_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        ttk.Label(exec_frame, text="Ready to run your refined model with the selected configuration.",
                  style='Caption.TLabel').grid(row=0, column=0, sticky=tk.W, pady=(0, 15))

        # Button frame for Run and Save buttons
        button_frame = tk.Frame(exec_frame, bg=self.colors['bg'])
        button_frame.grid(row=1, column=0, sticky='w', pady=10)

        self.refine_run_button = self._create_accent_button(button_frame, text="▶ Run Model",
                                                             command=self._run_refined_model, state='disabled')
        self.refine_run_button.pack(side='left', padx=(0, 10))

        self.refine_save_button = self._create_accent_button(button_frame, text="💾 Save Model",
                                                              command=self._save_refined_model, state='disabled')
        self.refine_save_button.pack(side='left')

        # Status
        self.refine_run_status = ttk.Label(exec_frame, text="Load a model from the Selection tab to enable execution",
                                            style='Caption.TLabel')
        self.refine_run_status.grid(row=2, column=0, pady=(10, 0))

    def _create_tab7d_results_diagnostics(self):
        """Subtab 7D: Results & Diagnostics (performance, plots, diagnostics)."""
        tab7d = ttk.Frame(self.model_dev_notebook, style='TFrame')
        self.model_dev_notebook.add(tab7d, text='  📊 Results  ')

        # Create scrollable content
        canvas = tk.Canvas(tab7d, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab7d, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab7d", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # === Save Model Button ===
        save_button_frame = tk.Frame(content_frame, bg=self.colors['bg'])
        save_button_frame.grid(row=row, column=0, columnspan=2, sticky='w', pady=(0, 20))

        self.refine_save_button_results = self._create_accent_button(save_button_frame, text="💾 Save Model",
                                                                      command=self._save_refined_model, state='disabled')
        self.refine_save_button_results.pack(side='left')
        row += 1

        # === Performance Metrics ===
        ttk.Label(content_frame, text="Performance Metrics", style='Heading.TLabel').grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 15))
        row += 1

        results_frame = ttk.LabelFrame(content_frame, text="Model Performance", padding="20")
        results_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        self.refine_results_text = tk.Text(results_frame, height=12, width=90, font=('Consolas', 10),
                                           bg=self.colors['panel'], fg=self.colors['text'], wrap=tk.WORD,
                                           relief='flat', borderwidth=0,
                                           selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        self.refine_results_text.pack(fill='both', expand=True)
        self.refine_results_text.insert('1.0', "Run a refined model in the Configuration tab to see results here.")
        self.refine_results_text.config(state='disabled')

        # === Prediction Plot ===
        ttk.Label(content_frame, text="Prediction Visualization", style='Heading.TLabel').grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(25, 15))
        row += 1

        plot_frame = ttk.LabelFrame(content_frame, text="Reference vs Predicted", padding="20")
        plot_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        self.refine_plot_frame = ttk.Frame(plot_frame)
        self.refine_plot_frame.pack(fill='both', expand=True)

        # === Residual Diagnostics ===
        ttk.Label(content_frame, text="Residual Diagnostics", style='Heading.TLabel').grid(
            row=row, column=0, columnspan=2, sticky=tk.W, pady=(25, 15))
        row += 1

        residual_diagnostics_frame = ttk.LabelFrame(content_frame, text="Residual Analysis (Regression Only)", padding="20")
        residual_diagnostics_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Add explanatory text for residual diagnostics
        residual_help_frame = ttk.Frame(residual_diagnostics_frame)
        residual_help_frame.pack(fill='x', padx=10, pady=(5, 15))

        residual_help_text = (
            "Residuals Analysis: Good models show randomly scattered residuals around zero with no patterns.\n\n"
            "• Residuals vs Fitted: Look for random scatter. Patterns (curves, funnels) indicate model issues.\n"
            "• Residuals vs Index: Check for systematic trends across samples.\n"
            "• Q-Q Plot: Points should follow the red diagonal line. Deviations suggest non-normal residuals.\n\n"
            "✓ Good: Random scatter, points on diagonal | ⚠ Warning: Patterns, curved Q-Q plot"
        )

        residual_help_label = ttk.Label(residual_help_frame, text=residual_help_text,
                                        style='Caption.TLabel', justify='left', wraplength=1200)
        residual_help_label.pack(anchor='w')

        self.residual_diagnostics_frame = ttk.Frame(residual_diagnostics_frame)
        self.residual_diagnostics_frame.pack(fill='both', expand=True)

        # === Leverage Diagnostics ===
        ttk.Label(content_frame, text="Leverage Analysis", style='Heading.TLabel').grid(
            row=row, column=0, columnspan=2, sticky=tk.W, pady=(25, 15))
        row += 1

        leverage_frame = ttk.LabelFrame(content_frame, text="Influential Samples (Linear Models Only)", padding="20")
        leverage_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Add explanatory text for leverage diagnostics
        leverage_help_frame = ttk.Frame(leverage_frame)
        leverage_help_frame.pack(fill='x', padx=10, pady=(5, 15))

        leverage_help_text = (
            "Leverage Analysis: Identifies influential samples that strongly affect the model.\n\n"
            "• Interpretation: High-leverage points (red, above threshold) have unusual feature values.\n"
            "• Orange line (2p/n): Moderate influence | Red line (3p/n): High influence\n"
            "  where p = number of model parameters, n = number of samples\n\n"
            "✓ Good: Most points below orange line | ⚠ Warning: Many red points may indicate data quality issues"
        )

        leverage_help_label = ttk.Label(leverage_help_frame, text=leverage_help_text,
                                        style='Caption.TLabel', justify='left', wraplength=1200)
        leverage_help_label.pack(anchor='w')

        self.leverage_plot_frame = ttk.Frame(leverage_frame)
        self.leverage_plot_frame.pack(fill='both', expand=True)

        # Status
        self.refine_status = ttk.Label(content_frame, text="No results available yet", style='Caption.TLabel')
        self.refine_status.grid(row=row, column=0, columnspan=2, pady=20)

    # === Helper Methods ===

    def _browse_spectral_data(self):
        """Browse for spectral data and auto-detect type."""
        directory = filedialog.askdirectory(title="Select Spectral Data Directory")

        if not directory:
            return

        # Store path
        self.spectral_data_path.set(directory)
        path = Path(directory)

        # Auto-detect file type
        # Priority: ASD > CSV > SPC

        # Check for ASD files (case insensitive, includes .sig format)
        asd_files = sorted(list(path.glob("*.asd")) + list(path.glob("*.ASD")) +
                     list(path.glob("*.sig")) + list(path.glob("*.SIG")))
        if asd_files:
            self.detected_type = "asd"
            self.detection_status.config(
                text=f"✓ Detected {len(asd_files)} ASD files",
                foreground=self.colors['success']
            )

            # Auto-detect reference CSV/Excel
            ref_files = sorted(list(path.glob("*.csv")) + list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
            if len(ref_files) == 1:
                self.reference_file.set(str(ref_files[0]))
                self._auto_detect_columns()
                # No popup needed - status label shows detection
            elif len(ref_files) > 1:
                # Update status to guide user - no popup needed
                self.detection_status.config(
                    text=f"✓ Detected {len(asd_files)} ASD files - {len(ref_files)} reference files found, select manually",
                    foreground=self.colors['accent']
                )
            return

        # Check for CSV files
        csv_files = sorted(list(path.glob("*.csv")))
        if csv_files:
            if len(csv_files) == 1:
                # Single CSV - use as spectral data
                self.spectral_data_path.set(str(csv_files[0]))
                self.detected_type = "csv"
                self.detection_status.config(
                    text="✓ Detected CSV spectra file - select reference CSV below",
                    foreground=self.colors['success']
                )
                # No popup needed - status label guides user
            else:
                # Multiple CSVs - need user to clarify
                self.detected_type = "csv"
                self.detection_status.config(
                    text=f"⚠ Found {len(csv_files)} CSV files - select files manually",
                    foreground=self.colors['accent']
                )
                # No popup needed - status label guides user
            return

        # Check for SPC files (GRAMS/Thermo Galactic) - case insensitive
        spc_files = sorted(list(path.glob("*.spc")) + list(path.glob("*.SPC")))
        if spc_files:
            self.detected_type = "spc"
            self.detection_status.config(
                text=f"✓ Detected {len(spc_files)} SPC files",
                foreground=self.colors['success']
            )

            # Auto-detect reference CSV/Excel
            ref_files = sorted(list(path.glob("*.csv")) + list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
            if len(ref_files) == 1:
                self.reference_file.set(str(ref_files[0]))
                self._auto_detect_columns()
                # No popup needed - status label shows detection
            elif len(ref_files) > 1:
                # Update status to guide user - no popup needed
                self.detection_status.config(
                    text=f"✓ Detected {len(spc_files)} SPC files - {len(ref_files)} reference files found, select manually",
                    foreground=self.colors['accent']
                )
            return

        # Check for JCAMP-DX files (.jdx, .dx)
        jcamp_files = sorted(list(path.glob("*.jdx")) + list(path.glob("*.dx")) + list(path.glob("*.JDX")) + list(path.glob("*.DX")))
        if jcamp_files:
            self.detected_type = "jcamp"
            self.detection_status.config(
                text=f"✓ Detected {len(jcamp_files)} JCAMP-DX files",
                foreground=self.colors['success']
            )

            # Auto-detect reference CSV/Excel
            ref_files = sorted(list(path.glob("*.csv")) + list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
            if len(ref_files) == 1:
                self.reference_file.set(str(ref_files[0]))
                self._auto_detect_columns()
            elif len(ref_files) > 1:
                self.detection_status.config(
                    text=f"✓ Detected {len(jcamp_files)} JCAMP-DX files - {len(ref_files)} reference files found, select manually",
                    foreground=self.colors['accent']
                )
            return

        # Check for ASCII variant files (.dpt, .dat, .asc)
        ascii_files = sorted(list(path.glob("*.dpt")) + list(path.glob("*.dat")) + list(path.glob("*.asc")) +
                       list(path.glob("*.DPT")) + list(path.glob("*.DAT")) + list(path.glob("*.ASC")))
        if ascii_files:
            self.detected_type = "ascii"
            self.detection_status.config(
                text=f"✓ Detected {len(ascii_files)} ASCII files (.dpt/.dat/.asc)",
                foreground=self.colors['success']
            )

            # Auto-detect reference CSV/Excel
            ref_files = sorted(list(path.glob("*.csv")) + list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
            if len(ref_files) == 1:
                self.reference_file.set(str(ref_files[0]))
                self._auto_detect_columns()
            elif len(ref_files) > 1:
                self.detection_status.config(
                    text=f"✓ Detected {len(ascii_files)} ASCII files - {len(ref_files)} reference files found, select manually",
                    foreground=self.colors['accent']
                )
            return

        # Check for Bruker OPUS files (.0, .1, .2, etc.)
        opus_files = []
        for i in range(100):  # Check .0 through .99
            opus_files.extend(list(path.glob(f"*.{i}")))
        if opus_files:
            self.detected_type = "opus"
            self.detection_status.config(
                text=f"✓ Detected {len(opus_files)} Bruker OPUS files",
                foreground=self.colors['success']
            )

            # Auto-detect reference CSV/Excel
            ref_files = sorted(list(path.glob("*.csv")) + list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
            if len(ref_files) == 1:
                self.reference_file.set(str(ref_files[0]))
                self._auto_detect_columns()
            elif len(ref_files) > 1:
                self.detection_status.config(
                    text=f"✓ Detected {len(opus_files)} OPUS files - {len(ref_files)} reference files found, select manually",
                    foreground=self.colors['accent']
                )
            return

        # Check for PerkinElmer files (.sp)
        sp_files = sorted(list(path.glob("*.sp")) + list(path.glob("*.SP")))
        if sp_files:
            self.detected_type = "perkinelmer"
            self.detection_status.config(
                text=f"✓ Detected {len(sp_files)} PerkinElmer files",
                foreground=self.colors['success']
            )

            # Auto-detect reference CSV/Excel
            ref_files = sorted(list(path.glob("*.csv")) + list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
            if len(ref_files) == 1:
                self.reference_file.set(str(ref_files[0]))
                self._auto_detect_columns()
            elif len(ref_files) > 1:
                self.detection_status.config(
                    text=f"✓ Detected {len(sp_files)} PerkinElmer files - {len(ref_files)} reference files found, select manually",
                    foreground=self.colors['accent']
                )
            return

        # Check for combined format (single CSV/TXT/Excel with all data)
        from src.spectral_predict.io import (
            detect_combined_format,
            detect_combined_excel_format,
            read_combined_csv,
            read_combined_excel
        )

        # Check for combined CSV/TXT first
        is_combined, combined_file = detect_combined_format(directory)
        is_excel = False

        # If not combined CSV, check for combined Excel
        if not is_combined:
            is_combined, combined_file, sheet_name = detect_combined_excel_format(directory)
            is_excel = True if is_combined else False
        else:
            sheet_name = None

        if is_combined:
            self.detected_type = 'combined_excel' if is_excel else 'combined'
            self.combined_file_path = combined_file
            self.combined_sheet_name = sheet_name if is_excel else None

            # Try to read and detect columns
            try:
                if is_excel:
                    X, y, metadata_df, metadata = read_combined_excel(combined_file, sheet_name=sheet_name)
                else:
                    X, y, metadata_df, metadata = read_combined_csv(combined_file)

                # Store metadata for later use
                self.combined_metadata = metadata
                self.combined_metadata_df = metadata_df

                # Populate column mapping dropdowns with all non-wavelength columns
                # This allows manual override of auto-detected columns
                available_cols = []

                # Add metadata columns if present
                if metadata.get('metadata_cols'):
                    available_cols.extend(metadata['metadata_cols'])

                # Add target column
                if metadata.get('y_col'):
                    available_cols.append(metadata['y_col'])

                # Add specimen ID column if not auto-generated
                if not metadata.get('generated_ids') and metadata.get('specimen_id_col'):
                    if metadata['specimen_id_col'] not in available_cols:
                        available_cols.append(metadata['specimen_id_col'])

                # Sort for consistent display
                available_cols = sorted(available_cols)

                # Populate dropdowns
                if available_cols:
                    self.spectral_file_combo['values'] = available_cols  # Not used for combined files but populate anyway
                    self.id_combo['values'] = ['N/A - Use Filename'] + available_cols
                    self.target_combo['values'] = available_cols

                    # Pre-select auto-detected values
                    if metadata.get('generated_ids'):
                        self.id_column.set('N/A - Use Filename')
                    elif metadata.get('specimen_id_col'):
                        self.id_column.set(metadata['specimen_id_col'])

                    if metadata.get('y_col'):
                        self.target_column.set(metadata['y_col'])

                # Update status with detailed info
                if metadata['generated_ids']:
                    id_info = "Generated IDs (Sample_1, Sample_2, ...)"
                else:
                    id_info = f"Specimen ID: {metadata['specimen_id_col']}"

                format_type = "Excel" if is_excel else "CSV/TXT"
                self.detection_status.config(
                    text=f"✓ Combined {format_type}: {metadata['n_spectra']} spectra, "
                         f"{len(metadata['wavelength_cols'])} wavelengths ({metadata['wavelength_range'][0]:.0f}-{metadata['wavelength_range'][1]:.0f} nm)",
                    foreground=self.colors['success']
                )

                # Show info message with detected columns
                info_msg = (
                    f"Detected combined {format_type} format!\n\n"
                    f"Auto-detected:\n"
                    f"  • {id_info}\n"
                    f"  • Target: {metadata['y_col']}\n"
                    f"  • Wavelengths: {metadata['wavelength_range'][0]:.1f} - {metadata['wavelength_range'][1]:.1f} nm\n"
                    f"  • Spectra: {metadata['n_spectra']}\n\n"
                    f"No reference file needed - all data is in one file.\n"
                    f"Click 'Load Data' to proceed."
                )
                messagebox.showinfo("Combined Format Detected", info_msg)

            except Exception as e:
                self.detection_status.config(
                    text=f"⚠ Error reading combined file: {str(e)}",
                    foreground=self.colors['accent']
                )
                messagebox.showerror("Error", f"Could not parse combined file:\n{str(e)}")

            return

        # Check for Excel files (fallback if not combined format)
        xlsx_files = sorted(list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
        if xlsx_files:
            if len(xlsx_files) == 1:
                # Single Excel file - use as spectral data (requires separate reference)
                self.spectral_data_path.set(str(xlsx_files[0]))
                self.detected_type = "excel"
                self.detection_status.config(
                    text="✓ Detected Excel spectra file - select reference file below",
                    foreground=self.colors['success']
                )

                # Auto-detect reference file (CSV or Excel)
                ref_files = sorted(list(path.glob("*.csv")) + [f for f in (list(path.glob("*.xlsx")) + list(path.glob("*.xls"))) if f not in xlsx_files])
                if len(ref_files) == 1:
                    self.reference_file.set(str(ref_files[0]))
                    self._auto_detect_columns()
                elif len(ref_files) > 1:
                    self.detection_status.config(
                        text=f"✓ Detected Excel spectra - {len(ref_files)} reference files found, select manually",
                        foreground=self.colors['accent']
                    )
            else:
                # Multiple Excel files - need user to clarify
                self.detected_type = "excel"
                self.detection_status.config(
                    text=f"⚠ Found {len(xlsx_files)} Excel files - select files manually",
                    foreground=self.colors['accent']
                )
            return

        # No supported files found
        self.detected_type = None
        self.detection_status.config(
            text="✗ No supported spectral files found",
            foreground=self.colors['warning']
        )
        messagebox.showwarning("No Spectral Data",
            "No supported spectral files found in this directory.\n\nSupported formats:\n• .asd (ASD files)\n• .csv (CSV spectral data)\n• .xlsx/.xls (Excel files)\n• .spc (GRAMS/Thermo Galactic)\n• .jdx/.dx (JCAMP-DX files)\n• .dpt/.dat/.asc (ASCII text files)\n• .0/.1/.2/etc (Bruker OPUS files)\n• .sp (PerkinElmer files)\n• Combined CSV/TXT/Excel (single file with all spectra + targets)")

    def _browse_reference_file(self):
        """Browse for reference file (CSV or Excel)."""
        filename = filedialog.askopenfilename(
            title="Select Reference File",
            filetypes=[
                ("Supported files", "*.csv;*.xlsx;*.xls"),
                ("CSV files", "*.csv"),
                ("Excel files", "*.xlsx;*.xls"),
                ("All files", "*.*")
            ]
        )
        if filename:
            self.reference_file.set(filename)
            self._auto_detect_columns()

    def _auto_detect_columns(self):
        """Auto-detect column names from reference file (CSV or Excel)."""
        if not self.reference_file.get():
            return

        try:
            # Detect file type and read accordingly
            ref_path = self.reference_file.get()
            if ref_path.lower().endswith(('.xlsx', '.xls')):
                df = pd.read_excel(ref_path, nrows=5)
            else:
                df = pd.read_csv(ref_path, nrows=5)

            columns = list(df.columns)

            # Update comboboxes
            self.spectral_file_combo['values'] = columns
            # Add "N/A - Use Filename" option for Specimen ID
            self.id_combo['values'] = ['N/A - Use Filename'] + columns
            self.target_combo['values'] = columns

            # Auto-select if possible
            if len(columns) >= 3:
                self.spectral_file_column.set(columns[0])
                self.id_column.set(columns[1])
                self.target_column.set(columns[2])

            self.tab1_status.config(text=f"✓ Detected {len(columns)} columns")
        except Exception as e:
            messagebox.showerror("Error", f"Could not read reference file:\n{e}")

    def _add_plot_export_button(self, parent_frame, figure, default_filename="plot"):
        """Add a small export button to save a matplotlib figure as an image.

        Parameters
        ----------
        parent_frame : tk.Frame
            Frame to add the export button to
        figure : matplotlib.figure.Figure
            The figure to export
        default_filename : str
            Default filename (without extension) for the saved image
        """
        def export_plot():
            from tkinter import filedialog
            import os

            # Ask user where to save
            filepath = filedialog.asksaveasfilename(
                defaultextension=".png",
                filetypes=[
                    ("PNG Image", "*.png"),
                    ("PDF Document", "*.pdf"),
                    ("SVG Vector", "*.svg"),
                    ("JPEG Image", "*.jpg"),
                    ("All Files", "*.*")
                ],
                initialfile=f"{default_filename}.png",
                title="Export Plot"
            )

            if filepath:
                try:
                    # Save the figure
                    figure.savefig(filepath, dpi=300, bbox_inches='tight')
                    # Success - file saved (no popup needed)
                except Exception as e:
                    messagebox.showerror("Export Error",
                        f"Failed to export plot:\n{str(e)}")

        # Create a small button frame
        button_frame = ttk.Frame(parent_frame)
        button_frame.pack(fill='x', padx=10, pady=(5, 10))

        export_btn = ttk.Button(button_frame, text="💾 Export Plot",
                               command=export_plot, style='Modern.TButton')
        export_btn.pack(side='right')

    def _on_tier_changed(self, *args):
        """Update model checkboxes based on selected tier."""
        tier = self.model_tier.get()
        task_type = self.task_type.get()

        if tier == "custom":
            # Don't auto-update checkboxes for custom tier
            return

        # Determine actual task type for tier lookup
        if task_type == "auto":
            # If auto and data is loaded, try to detect
            if hasattr(self, 'y') and self.y is not None:
                if self.y.nunique() == 2 or self.y.dtype == 'object' or self.y.nunique() < 10:
                    actual_task = "classification"
                else:
                    actual_task = "regression"
            else:
                # Default to regression if no data loaded
                actual_task = "regression"
        else:
            actual_task = task_type

        # Get models for this tier and task type
        try:
            tier_models = set(get_tier_models(tier, actual_task))
        except ValueError:
            # If tier not found, keep current selection
            return

        # Get all supported models for this task type
        if actual_task == "classification":
            supported_models = set(get_supported_models("classification"))
        else:
            supported_models = set(get_supported_models("regression"))

        # Set flag to prevent triggering custom tier switch
        self._updating_from_tier = True

        # Update checkboxes
        for model_name, checkbox_var in self.model_checkboxes.items():
            # Check if model should be selected for this tier AND is supported for task type
            should_select = model_name in tier_models and model_name in supported_models
            # Don't enable CatBoost if it's not available
            if model_name == 'CatBoost' and not HAS_CATBOOST:
                should_select = False
            checkbox_var.set(1 if should_select else 0)

        # Reset flag
        self._updating_from_tier = False

    def _on_task_type_changed(self):
        """Handle task type changes - filter models and update tier selection."""
        task_type = self.task_type.get()

        # Determine actual task type (for auto-detect, check the data)
        if task_type == "auto":
            # Auto-detect from loaded data
            if self.y is not None:
                # Use same logic as in analysis to detect task type
                if self.y.nunique() == 2 or self.y.dtype == 'object' or self.y.nunique() < 10:
                    actual_task = "classification"
                else:
                    actual_task = "regression"
            else:
                # No data loaded yet - enable all models
                for model_name, checkbox_widget in self.model_checkbox_widgets.items():
                    # Don't enable CatBoost if not available
                    if model_name == 'CatBoost' and not HAS_CATBOOST:
                        continue
                    checkbox_widget.state(['!disabled'])
                # Refresh tier selection for auto mode
                self._on_tier_changed()
                return
        else:
            # User explicitly selected task type
            actual_task = task_type

        # Get supported models for this task type
        supported_models = set(get_supported_models(actual_task))

        # Enable/disable checkboxes based on compatibility
        for model_name, checkbox_var in self.model_checkboxes.items():
            checkbox_widget = self.model_checkbox_widgets[model_name]

            if model_name not in supported_models:
                # Model not supported for this task type
                if checkbox_var.get():
                    checkbox_var.set(False)  # Uncheck if currently selected
                checkbox_widget.state(['disabled'])  # Disable visually
            else:
                # Model is supported for this task type
                # Don't enable CatBoost if it's not installed
                if model_name == 'CatBoost' and not HAS_CATBOOST:
                    continue
                checkbox_widget.state(['!disabled'])  # Enable

        # Refresh tier selection to use correct model set
        self._on_tier_changed()

    def _on_refine_model_changed(self, *args):
        """Handle model type changes in Model Development tab - show/hide appropriate hyperparameter section."""
        selected_model = self.refine_model_type.get()

        # Hide all hyperparameter frames
        if hasattr(self, 'refine_hyperparam_frames'):
            for frame in self.refine_hyperparam_frames.values():
                frame.grid_forget()

            # Show the frame for the selected model
            if selected_model in self.refine_hyperparam_frames:
                self.refine_hyperparam_frames[selected_model].grid(row=0, column=0, sticky=(tk.W, tk.E), pady=5)

    def _on_refine_task_type_changed(self):
        """Handle task type changes in Refine tab - update model dropdown to show only appropriate models."""
        task_type = self.refine_task_type.get()

        # Get current model selection before updating
        current_model = self.refine_model_type.get()

        # Get supported models for this task type
        if get_supported_models is not None:
            supported_models = get_supported_models(task_type)
        else:
            # Fallback if registry import failed
            if task_type == 'regression':
                supported_models = ['PLS', 'Ridge', 'Lasso', 'ElasticNet', 'RandomForest', 'MLP', 'NeuralBoosted', 'SVR', 'XGBoost', 'LightGBM', 'CatBoost']
            else:  # classification
                supported_models = ['PLS-DA', 'PLS', 'RandomForest', 'MLP', 'NeuralBoosted', 'SVM', 'XGBoost', 'LightGBM', 'CatBoost']

        # Update combobox values
        self.refine_model_combo['values'] = supported_models

        # If current model is not supported for new task type, switch to default
        if current_model not in supported_models:
            if task_type == 'classification':
                self.refine_model_type.set('PLS-DA')
            else:
                self.refine_model_type.set('PLS')

    def _on_model_checkbox_changed(self, *args):
        """Switch to Custom tier when user manually changes a model checkbox."""
        # Only switch to custom if we're not currently updating from tier
        if not self._updating_from_tier and self.model_tier.get() != "custom":
            self.model_tier.set("custom")

    # === TAB 0 HELPER METHODS: Data Management ===

    def _add_data_source(self):
        """Open dialog to add a new data source."""
        # This will be handled by the load source button
        messagebox.showinfo("Add Source", "Use the 'Add New Source' section to load data")

    def _remove_data_source(self):
        """Remove selected data source."""
        if not self.data_source_manager:
            return

        selected = self.data_sources_tree.selection()
        if not selected:
            messagebox.showwarning("No Selection", "Please select a source to remove")
            return

        source_id = self.data_sources_tree.item(selected[0])['text']

        if messagebox.askyesno("Confirm", f"Remove source {source_id}?"):
            self.data_source_manager.remove_source(source_id)
            self._refresh_sources_tree()

    def _refresh_sources_tree(self):
        """Refresh the data sources TreeView."""
        if not self.data_source_manager:
            return

        # Clear tree
        for item in self.data_sources_tree.get_children():
            self.data_sources_tree.delete(item)

        # Populate with sources
        for source in self.data_source_manager.sources:
            has_y = "✓" if source.y is not None else "✗"
            has_ref = "✓" if source.ref is not None else "✗"
            wl_range = f"{source.wavelength_range[0]:.1f}-{source.wavelength_range[1]:.1f}"

            self.data_sources_tree.insert('', 'end',
                text=source.source_id,
                values=(source.name, source.format_type, source.n_samples,
                       len(source.wavelengths), wl_range, has_y, has_ref))

        # Update merge checkboxes
        self._update_merge_checkboxes()

    def _update_merge_checkboxes(self):
        """Update checkboxes in merge tab based on available sources."""
        if not self.data_source_manager:
            return

        # Clear existing checkboxes
        for widget in self.merge_sources_frame.winfo_children():
            widget.destroy()

        # Create new checkboxes
        self.merge_source_vars = {}
        for source in self.data_source_manager.sources:
            var = tk.BooleanVar(value=True)
            self.merge_source_vars[source.source_id] = var
            ttk.Checkbutton(self.merge_sources_frame,
                          text=f"{source.name} ({source.n_samples} samples)",
                          variable=var).pack(anchor='w', pady=2)

    def _save_source_config(self):
        """Save data source configuration to file."""
        if not self.data_source_manager:
            return

        filename = filedialog.asksaveasfilename(
            title="Save Configuration",
            defaultextension=".json",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")]
        )

        if filename:
            self.data_source_manager.save_configuration(filename)
            messagebox.showinfo("Success", f"Configuration saved to {filename}")

    def _load_source_config(self):
        """Load data source configuration from file."""
        if not self.data_source_manager:
            return

        filename = filedialog.askopenfilename(
            title="Load Configuration",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")]
        )

        if filename:
            config = self.data_source_manager.load_configuration(filename)
            messagebox.showinfo("Loaded", f"Loaded configuration with {config['n_sources']} sources.\nNote: Spectral data must be reloaded.")

    def _clear_all_sources(self):
        """Clear all data sources."""
        if not self.data_source_manager:
            return

        if self.data_source_manager.sources:
            if messagebox.askyesno("Confirm", "Clear all data sources?"):
                self.data_source_manager.clear_sources()
                self._refresh_sources_tree()

    def _browse_dm_spectra(self):
        """Browse for spectral data directory or file (Data Management tab)."""
        # Create a custom dialog that allows both files and directories
        from tkinter import simpledialog

        # First, ask user what they want to load
        choice = messagebox.askquestion("Load Type",
            "Load a directory of spectral files?\n\n"
            "Yes = Directory (ASD, CSV, SPC files)\n"
            "No = Single combined file (CSV or Excel)",
            icon='question')

        if choice == 'yes':
            # Load directory
            directory = filedialog.askdirectory(title="Select Spectral Data Directory")
            if directory:
                self.dm_spectra_path_var.set(directory)
                # Auto-generate source name from directory
                if not self.dm_source_name_var.get():
                    self.dm_source_name_var.set(os.path.basename(directory))
        else:
            # Load single file (CSV or Excel)
            filename = filedialog.askopenfilename(
                title="Select Combined Data File",
                filetypes=[
                    ("All Supported", "*.csv *.txt *.xlsx *.xls"),
                    ("CSV/Text files", "*.csv *.txt"),
                    ("Excel files", "*.xlsx *.xls"),
                    ("All files", "*.*")
                ]
            )
            if filename:
                self.dm_spectra_path_var.set(filename)
                # Auto-generate source name from file
                if not self.dm_source_name_var.get():
                    self.dm_source_name_var.set(os.path.splitext(os.path.basename(filename))[0])

    def _browse_dm_reference(self):
        """Browse for reference CSV or Excel file (Data Management tab)."""
        filename = filedialog.askopenfilename(
            title="Select Reference CSV or Excel",
            filetypes=[
                ("All Supported", "*.csv *.xlsx *.xls"),
                ("CSV files", "*.csv"),
                ("Excel files", "*.xlsx *.xls"),
                ("All files", "*.*")
            ]
        )
        if filename:
            self.dm_reference_path_var.set(filename)

    def _load_dm_source(self):
        """Load a data source in Data Management tab."""
        if not self.data_source_manager:
            messagebox.showerror("Error", "Data management module not available")
            return

        spectra_path = self.dm_spectra_path_var.get()
        if not spectra_path:
            messagebox.showwarning("No Data", "Please select spectral data directory or file")
            return

        try:
            import os
            from pathlib import Path
            from spectral_predict.io import (
                read_asd_dir, read_csv_spectra, read_spc_dir,
                read_combined_csv, read_combined_excel, read_reference_csv, align_xy
            )

            # Check if it's a file or directory
            is_file = os.path.isfile(spectra_path)

            # Initialize variables
            y = None
            ref = None

            if is_file:
                # Single file - check extension
                file_ext = os.path.splitext(spectra_path)[1].lower()

                if file_ext in ['.xlsx', '.xls']:
                    # Try to load as Excel combined format
                    # Excel files can have multiple sheets - try each one
                    import openpyxl
                    wb = openpyxl.load_workbook(spectra_path, read_only=True, data_only=True)
                    sheet_names = wb.sheetnames
                    wb.close()

                    print(f"Excel file has {len(sheet_names)} sheet(s): {sheet_names}")

                    loaded = False
                    last_error = None

                    for sheet_idx, sheet_name in enumerate(sheet_names):
                        try:
                            print(f"Trying sheet '{sheet_name}' (index {sheet_idx})...")

                            # First, peek at what columns exist
                            peek_df = pd.read_excel(spectra_path, sheet_name=sheet_idx, nrows=5)
                            print(f"  Sheet has {len(peek_df.columns)} columns")
                            print(f"  First 20 column names: {list(peek_df.columns[:20])}")
                            print(f"  Sample of first row: {peek_df.iloc[0].head(10).to_dict()}")

                            X, y, metadata_df, metadata = read_combined_excel(spectra_path, sheet_name=sheet_idx)
                            format_type = 'combined_excel'

                            # Don't designate target yet - keep ALL non-wavelength columns as metadata
                            # Put the y column back into metadata with its real name
                            if metadata_df is not None and y is not None:
                                y_col_name = metadata.get('y_col', 'target')
                                metadata_df[y_col_name] = y
                            elif y is not None:
                                # No metadata_df, create one with just the y column
                                y_col_name = metadata.get('y_col', 'target')
                                metadata_df = pd.DataFrame({y_col_name: y})

                            ref = metadata_df  # All metadata columns with real names
                            y = None  # Don't pre-designate target

                            print(f"✓ Loaded combined Excel from sheet '{sheet_name}': {len(X)} samples, {len(X.columns)} wavelengths")
                            print(f"  • Specimen ID: {metadata.get('specimen_id_col', 'N/A')}")
                            if ref is not None:
                                print(f"  • Metadata columns: {', '.join(ref.columns)}")
                            loaded = True
                            break  # Success - stop trying other sheets
                        except Exception as e:
                            last_error = e
                            print(f"  Sheet '{sheet_name}' failed: {str(e)[:100]}...")
                            continue

                    if not loaded:
                        # All sheets failed
                        import traceback
                        error_details = traceback.format_exc()
                        print(f"Failed to load any sheet. Last error:\n{error_details}")
                        messagebox.showerror("Excel Format Error",
                            f"Could not load Excel file from any sheet.\n\n"
                            f"Tried {len(sheet_names)} sheet(s): {', '.join(sheet_names)}\n\n"
                            f"Last error: {str(last_error)}\n\n"
                            f"Combined Excel format requires:\n"
                            f"- Sample_ID column (optional)\n"
                            f"- Numeric wavelength columns (at least 100)\n"
                            f"- Target column (e.g., 'target', 'y')\n\n"
                            f"Check console for detailed errors from each sheet.")
                        return
                elif file_ext in ['.csv', '.txt']:
                    # Try to load as combined format CSV (preferred)
                    try:
                        print(f"Attempting to load {os.path.basename(spectra_path)} as combined CSV...")
                        X, y, metadata_df, metadata = read_combined_csv(spectra_path)
                        format_type = 'combined'

                        # Don't designate target yet - keep ALL non-wavelength columns as metadata
                        # Put the y column back into metadata with its real name
                        if metadata_df is not None and y is not None:
                            y_col_name = metadata.get('y_col', 'target')
                            metadata_df[y_col_name] = y
                        elif y is not None:
                            # No metadata_df, create one with just the y column
                            y_col_name = metadata.get('y_col', 'target')
                            metadata_df = pd.DataFrame({y_col_name: y})

                        ref = metadata_df  # All metadata columns with real names
                        y = None  # Don't pre-designate target

                        print(f"✓ Loaded combined CSV: {len(X)} samples, {len(X.columns)} wavelengths")
                        print(f"  • Specimen ID: {metadata.get('specimen_id_col', 'N/A')}")
                        if ref is not None:
                            print(f"  • Metadata columns: {', '.join(ref.columns)}")
                    except Exception as e:
                        print(f"Combined CSV failed: {str(e)}")

                        # Show detailed diagnostics for delimiter errors
                        if "Could not parse file" in str(e) and "delimiters" in str(e):
                            # Try to read first few lines to show user what we see
                            try:
                                with open(spectra_path, 'r') as f:
                                    first_lines = [f.readline().strip() for _ in range(3)]
                                print(f"First 3 lines of file:")
                                for i, line in enumerate(first_lines, 1):
                                    print(f"  Line {i}: {line[:200]}...")  # Show first 200 chars
                            except:
                                pass

                        # Try single spectrum CSV format
                        print(f"Trying single spectrum format...")
                        try:
                            X, metadata = read_csv_spectra(spectra_path)
                            format_type = 'csv'
                            print(f"✓ Loaded as single spectrum CSV: {len(X)} samples, {len(X.columns)} wavelengths")
                        except Exception as e2:
                            # Both formats failed - show helpful error
                            import traceback
                            error_details = traceback.format_exc()
                            print(f"Full error:\n{error_details}")

                            error_msg = (
                                f"Failed to load CSV file.\n\n"
                                f"Combined format error: {str(e)}\n\n"
                                f"Single spectrum error: {str(e2)}\n\n"
                                f"Please ensure your CSV is in one of these formats:\n"
                                f"1. Combined: Sample_ID, metadata, wavelength columns, target\n"
                                f"   - Must have at least 100 numeric wavelength columns\n"
                                f"   - Delimiter: comma, tab, semicolon, or whitespace\n"
                                f"2. Single spectrum: Sample_ID, numeric wavelength columns\n\n"
                                f"Check console output for file preview and detailed errors."
                            )
                            messagebox.showerror("CSV Load Error", error_msg)
                            return
                else:
                    messagebox.showerror("Error", f"Unsupported file type: {file_ext}")
                    return
            else:
                # Directory - detect format by scanning files
                dir_path = Path(spectra_path)
                files = list(dir_path.iterdir())

                # Check for ASD files
                asd_files = [f for f in files if f.suffix.lower() == '.asd']
                if asd_files:
                    format_type = 'asd'
                    X, metadata = read_asd_dir(spectra_path)
                # Check for SPC files
                elif any(f.suffix.lower() == '.spc' for f in files):
                    format_type = 'spc'
                    X, metadata = read_spc_dir(spectra_path)
                # Check for CSV files
                elif any(f.suffix.lower() == '.csv' for f in files):
                    format_type = 'csv'
                    X, metadata = read_csv_spectra(spectra_path)
                else:
                    messagebox.showerror("Error", f"No supported spectral files found in directory")
                    return

            # Load reference CSV or Excel if provided (for directory-based or single-file data that doesn't have y)
            reference_path = self.dm_reference_path_var.get()
            if reference_path and y is None:  # Only load reference if we don't already have y from combined format
                # Read reference file to get column names
                ref_ext = os.path.splitext(reference_path)[1].lower()
                if ref_ext in ['.xlsx', '.xls']:
                    # Read Excel file
                    ref_df = pd.read_excel(reference_path)
                else:
                    # Read CSV file
                    ref_df = pd.read_csv(reference_path)

                # Auto-detect columns from reference file
                columns = list(ref_df.columns)

                # Auto-detect ID column - look for common names first
                id_column = None
                id_candidates = ['specimen_id', 'sample_id', 'id', 'ID', 'Sample_ID', 'Specimen_ID',
                                'SampleID', 'SpecimenID', 'sample', 'specimen', 'filename', 'file']
                for candidate in id_candidates:
                    if candidate in columns:
                        id_column = candidate
                        break

                # If no standard ID column found, look for column with unique string values
                if not id_column:
                    for col in columns:
                        if ref_df[col].nunique() == len(ref_df) and not pd.api.types.is_numeric_dtype(ref_df[col]):
                            id_column = col
                            break

                # Fallback to first column only if nothing else found
                if not id_column:
                    id_column = columns[0]

                # Auto-detect target column - look for common names or numeric columns
                target_column = None
                target_candidates = ['target', 'y', 'Target', 'Y', 'value', 'Value',
                                   'collagen', 'nitrogen', 'protein', 'class', 'label']
                for candidate in target_candidates:
                    if candidate in columns:
                        target_column = candidate
                        break

                # If no standard target found, find last numeric column (excluding ID)
                if not target_column:
                    for col in reversed(columns):
                        if col != id_column and pd.api.types.is_numeric_dtype(ref_df[col]):
                            target_column = col
                            break

                # Fallback to last column if still not found
                if not target_column:
                    target_column = columns[-1]

                print(f"Auto-detected reference columns: ID='{id_column}', Target='{target_column}'")

                # Now read reference with proper ID column
                if ref_ext in ['.xlsx', '.xls']:
                    ref_df = pd.read_excel(reference_path)
                    ref_df = ref_df.set_index(id_column)
                else:
                    ref_df = read_reference_csv(reference_path, id_column)

                # Align data - CORRECTED SIGNATURE
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref_df,
                    id_column,
                    target_column,
                    return_alignment_info=True
                )

                # Validate alignment
                if not X_aligned.index.equals(y_aligned.index):
                    raise ValueError("X and y indices don't match after alignment!")
                if len(X_aligned) != len(y_aligned):
                    raise ValueError(f"X has {len(X_aligned)} samples but y has {len(y_aligned)} samples!")

                X = X_aligned
                y = y_aligned
                ref = ref_df  # Keep full reference dataframe

                # Show alignment info
                if alignment_info:
                    try:
                        matched = alignment_info.get('matched', 'unknown')
                        unmatched_X = alignment_info.get('unmatched_X', 'unknown')
                        unmatched_y = alignment_info.get('unmatched_y', 'unknown')
                        print(f"Alignment: {matched} matched, "
                              f"{unmatched_X} unmatched spectra, "
                              f"{unmatched_y} unmatched references")
                    except Exception as e:
                        print(f"Alignment info available: {alignment_info}")
                        print(f"Warning: Could not display alignment stats: {e}")

            # Validate loaded data
            if X is None or len(X) == 0:
                messagebox.showerror("Error", "No spectral data was loaded. Please check the file format.")
                return

            if y is not None and len(X) != len(y):
                messagebox.showerror("Error",
                    f"Data mismatch: {len(X)} spectra but {len(y)} target values.\n"
                    f"Please check your reference file alignment.")
                return

            # Add source to manager
            source_name = self.dm_source_name_var.get() or os.path.basename(spectra_path)
            source = self.data_source_manager.add_source(
                X=X, y=y, ref=ref,
                path=spectra_path,
                format_type=format_type,
                name=source_name
            )

            # Refresh tree
            self._refresh_sources_tree()

            # Show source details
            self.source_details_text.delete('1.0', tk.END)
            self.source_details_text.insert('1.0',
                f"Source: {source.name}\n"
                f"Format: {source.format_type}\n"
                f"Samples: {source.n_samples}\n"
                f"Wavelengths: {len(source.wavelengths)}\n"
                f"Range: {source.wavelength_range[0]:.1f} - {source.wavelength_range[1]:.1f} nm\n"
                f"Has Y values: {source.y is not None}\n"
                f"Has metadata: {source.ref is not None}\n"
            )

            messagebox.showinfo("Success", f"Loaded {source.n_samples} samples from {source.name}")

            # Clear input fields
            self.dm_spectra_path_var.set("")
            self.dm_reference_path_var.set("")
            self.dm_source_name_var.set("")

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"Full error traceback:\n{error_details}")
            messagebox.showerror("Load Error",
                f"Failed to load data.\n\n"
                f"Error: {str(e)}\n\n"
                f"Check the console for detailed error information.")

    def _use_for_analysis(self):
        """Send selected data source to analysis pipeline."""
        if not self.data_source_manager:
            return

        selected = self.data_sources_tree.selection()
        if not selected:
            # If merged data exists, use that
            if self.data_source_manager.merged_dataset:
                dataset = self.data_source_manager.merged_dataset
                self.X = dataset.X
                self.y = dataset.y
                self.ref = dataset.ref
                messagebox.showinfo("Success", "Merged dataset loaded for analysis")
            else:
                messagebox.showwarning("No Selection", "Please select a source or create a merged dataset")
                return
        else:
            # Use selected source
            source_id = self.data_sources_tree.item(selected[0])['text']
            source = self.data_source_manager.get_source(source_id)
            if source:
                self.X = source.X
                self.y = source.y
                self.ref = source.ref
                messagebox.showinfo("Success", f"Source '{source.name}' loaded for analysis")

        # Update plots in Tab 1 if needed
        if hasattr(self, 'plot_canvas') and self.plot_canvas:
            self._update_spectral_plots()

        # Update Data Viewer tab
        if hasattr(self, 'sheet') and self.sheet:
            self._update_data_viewer()

        # Switch to Import & Preview tab to show the data
        self.notebook.select(1)  # Tab 1 (Import & Preview)

    def _preview_merge(self):
        """Preview merge operation."""
        print("Preview merge button clicked")

        if not self.data_source_manager:
            print("No data source manager")
            messagebox.showerror("Error", "Data source manager not initialized")
            return

        # Get selected sources
        selected_sources = [sid for sid, var in self.merge_source_vars.items() if var.get()]
        print(f"Selected sources: {selected_sources}")

        if len(selected_sources) < 2:
            messagebox.showwarning("Not Enough Sources", "Please select at least 2 sources to merge")
            return

        try:
            # Preview merge without executing
            strategy = self.merge_strategy_var.get()
            handle_duplicates = self.duplicate_handling_var.get()

            # Get source objects
            sources = [self.data_source_manager.get_source(sid) for sid in selected_sources]

            # Generate preview text
            preview = f"Merge Preview\n{'='*50}\n\n"
            preview += f"Sources to merge: {len(sources)}\n"
            for source in sources:
                preview += f"  - {source.name}: {source.n_samples} samples, {len(source.wavelengths)} wavelengths\n"

            preview += f"\nStrategy: {strategy}\n"
            preview += f"Duplicate handling: {handle_duplicates}\n\n"

            # Check for wavelength compatibility
            if strategy == 'intersection':
                common_wl = set(sources[0].wavelengths)
                for source in sources[1:]:
                    common_wl = common_wl.intersection(set(source.wavelengths))
                preview += f"Common wavelengths: {len(common_wl)}\n"
                if len(common_wl) == 0:
                    preview += "WARNING: No common wavelengths found!\n"
            elif strategy == 'union':
                all_wl = set()
                for source in sources:
                    all_wl.update(source.wavelengths)
                preview += f"Total unique wavelengths: {len(all_wl)}\n"

            # Check for duplicate sample IDs
            all_ids = []
            for source in sources:
                all_ids.extend(source.X.index.tolist())
            unique_ids = set(all_ids)
            n_duplicates = len(all_ids) - len(unique_ids)
            preview += f"\nTotal samples: {len(all_ids)}\n"
            preview += f"Unique samples: {len(unique_ids)}\n"
            preview += f"Duplicates: {n_duplicates}\n"

            self.merge_preview_text.delete('1.0', tk.END)
            self.merge_preview_text.insert('1.0', preview)

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"Preview merge error:\n{error_details}")
            messagebox.showerror("Preview Error", f"Failed to preview merge: {str(e)}")

    def _execute_merge(self):
        """Execute merge operation."""
        print("Execute merge button clicked")

        if not self.data_source_manager:
            print("No data source manager")
            messagebox.showerror("Error", "Data source manager not initialized")
            return

        # Get selected sources
        selected_sources = [sid for sid, var in self.merge_source_vars.items() if var.get()]
        print(f"Selected sources for merge: {selected_sources}")

        if len(selected_sources) < 2:
            messagebox.showwarning("Not Enough Sources", "Please select at least 2 sources to merge")
            return

        try:
            strategy = self.merge_strategy_var.get()
            handle_duplicates = self.duplicate_handling_var.get()
            print(f"Merge strategy: {strategy}, duplicate handling: {handle_duplicates}")

            # Execute merge
            print("Calling data_source_manager.merge_sources...")
            merged = self.data_source_manager.merge_sources(
                source_ids=selected_sources,
                strategy=strategy,
                handle_duplicates=handle_duplicates
            )

            print(f"Merge successful! Result: {len(merged.X)} samples, {len(merged.X.columns)} wavelengths")

            # Automatically load merged data to viewer
            self._load_merged_to_viewer()

            # Show results
            report = merged.merge_report
            messagebox.showinfo("Merge Complete",
                f"Merged {len(merged.sources)} sources\n"
                f"Result: {len(merged.X)} samples, {len(merged.X.columns)} wavelengths\n"
                f"Strategy: {strategy}\n\n"
                f"View in the '📊 View Merged Data' tab")

            # Update preview
            self.merge_preview_text.insert(tk.END, f"\n\n{'='*50}\nMERGE COMPLETED\n")
            self.merge_preview_text.insert(tk.END, f"Report: {report}\n")

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"Merge error:\n{error_details}")
            messagebox.showerror("Merge Error", f"Failed to merge: {str(e)}\n\nCheck console for details.")

    def _merge_and_use(self):
        """Execute merge and immediately use for analysis."""
        self._execute_merge()
        if self.data_source_manager and self.data_source_manager.merged_dataset:
            self.X = self.data_source_manager.merged_dataset.X
            self.y = self.data_source_manager.merged_dataset.y
            self.ref = self.data_source_manager.merged_dataset.ref
            messagebox.showinfo("Success", "Merged dataset loaded for analysis")
            # Switch to Import & Preview tab
            self.notebook.select(1)

    def _load_merged_to_viewer(self):
        """Load merged dataset into the spreadsheet viewer."""
        if not self.data_source_manager or not self.data_source_manager.merged_dataset:
            messagebox.showwarning("No Merged Data", "Please execute a merge first")
            return

        merged = self.data_source_manager.merged_dataset

        # Start with wavelength data
        display_df = merged.X.copy()

        # Add metadata columns with their real names (no special "target" designation)
        if merged.ref is not None:
            for col in merged.ref.columns:
                display_df.insert(0, col, merged.ref[col])

        # Add sample ID as first column
        display_df.insert(0, '_sample_id_', display_df.index)

        # Set data in sheet
        headers = list(display_df.columns)
        data = display_df.values.tolist()

        self.merged_data_sheet.headers(headers)
        self.merged_data_sheet.data = data
        self.merged_data_sheet.set_sheet_data(data)

        # Update info label
        n_samples = len(display_df)
        n_cols = len(headers)
        n_metadata = len(merged.ref.columns) if merged.ref is not None else 0
        n_wavelengths = len(headers) - 1 - n_metadata  # Total - sample_id - metadata

        self.merged_info_label.config(
            text=f"Loaded: {n_samples} samples, {n_wavelengths} wavelength columns, {n_metadata} metadata columns"
        )

        print(f"Merged data loaded to viewer: {n_samples} x {n_cols}")
        if merged.ref is not None:
            print(f"  Metadata columns: {', '.join(merged.ref.columns)}")

    def _delete_merged_column(self):
        """Delete selected column from merged data."""
        if not self.data_source_manager or not self.data_source_manager.merged_dataset:
            messagebox.showwarning("No Data", "No merged data loaded")
            return

        # Get selected column
        selected = self.merged_data_sheet.get_currently_selected()
        if not selected or selected.type_ != "column":
            messagebox.showwarning("No Selection", "Please select a column header to delete")
            return

        col_idx = selected.column
        headers = self.merged_data_sheet.headers()

        if col_idx >= len(headers):
            return

        col_name = headers[col_idx]

        # Don't allow deleting sample ID
        if col_name == '_sample_id_':
            messagebox.showerror("Error", "Cannot delete sample ID column")
            return

        # Confirm deletion
        confirm = messagebox.askyesno("Confirm Delete",
            f"Delete column '{col_name}'?\n\nThis cannot be undone.")

        if not confirm:
            return

        # Delete column from sheet
        self.merged_data_sheet.delete_column(col_idx)

        # Update merged dataset
        merged = self.data_source_manager.merged_dataset

        if merged.ref is not None and col_name in merged.ref.columns:
            merged.ref = merged.ref.drop(columns=[col_name])
            print(f"Deleted metadata column: {col_name}")
        elif col_name in merged.X.columns:
            merged.X = merged.X.drop(columns=[col_name])
            print(f"Deleted wavelength column: {col_name}")

        messagebox.showinfo("Success", f"Deleted column '{col_name}'")
        self._load_merged_to_viewer()  # Refresh view

    def _on_merged_sheet_right_click(self, event):
        """Handle right-click on sheet to add column."""
        # Get clicked position
        region = self.merged_data_sheet.identify_region(event)

        if region and region.type_ == "header":
            # Right-clicked on column header
            col_idx = region.column
            headers = self.merged_data_sheet.headers()

            if col_idx < len(headers):
                col_name = headers[col_idx]

                # Show menu to add column
                menu = tk.Menu(self.merged_data_sheet, tearoff=0)
                menu.add_command(label=f"Add column after '{col_name}'",
                               command=lambda: self._add_column_after(col_idx))
                menu.post(event.x_root, event.y_root)

    def _add_column_after(self, col_idx):
        """Add a new categorical column after the specified index."""
        if not self.data_source_manager or not self.data_source_manager.merged_dataset:
            return

        # Get new column name from user
        new_col_name = tk.simpledialog.askstring("Add Column",
            "Enter name for new column:",
            parent=self.merged_data_sheet)

        if not new_col_name:
            return

        merged = self.data_source_manager.merged_dataset

        # Add empty column to ref (metadata)
        if merged.ref is None:
            merged.ref = pd.DataFrame(index=merged.X.index)

        merged.ref[new_col_name] = ""  # Empty string for categorical data

        print(f"Added new column: {new_col_name}")
        messagebox.showinfo("Success", f"Added column '{new_col_name}'")

        # Refresh viewer
        self._load_merged_to_viewer()

    def _save_merged_changes(self):
        """Save changes from sheet back to merged dataset."""
        if not self.data_source_manager or not self.data_source_manager.merged_dataset:
            messagebox.showwarning("No Data", "No merged data to save")
            return

        # Get data from sheet
        data = self.merged_data_sheet.get_sheet_data()
        headers = self.merged_data_sheet.headers()

        if not data or not headers:
            return

        # Convert to dataframe
        df = pd.DataFrame(data, columns=headers)

        # Set index
        if '_sample_id_' in df.columns:
            df = df.set_index('_sample_id_')

        merged = self.data_source_manager.merged_dataset

        # Separate wavelength columns from metadata
        wavelength_cols = [c for c in df.columns if c in merged.X.columns]
        metadata_cols = [c for c in df.columns if c not in wavelength_cols]

        # Update X (wavelength data)
        if wavelength_cols:
            merged.X = df[wavelength_cols]

        # Update ref (all metadata columns with real names)
        if metadata_cols:
            merged.ref = df[metadata_cols]

        messagebox.showinfo("Success", "Changes saved to merged dataset")
        print("Merged dataset updated from viewer")

    def _apply_sample_filter(self):
        """Apply sample filter to current dataset."""
        if not self.data_source_manager:
            return

        if self.X is None:
            messagebox.showwarning("No Data", "Please load or select data first")
            return

        try:
            filter_type = self.filter_type_var.get()
            filter_value = self.filter_value_var.get()
            filter_column = self.filter_column_var.get() if self.filter_column_var.get() else None

            # Apply filter
            X_filtered, y_filtered, ref_filtered = self.data_source_manager.filter_samples(
                self.X, self.y, self.ref,
                filter_type=filter_type,
                filter_value=filter_value,
                column=filter_column
            )

            # Update data
            original_samples = len(self.X)
            self.X = X_filtered
            self.y = y_filtered
            self.ref = ref_filtered

            messagebox.showinfo("Filter Applied",
                f"Filtered from {original_samples} to {len(self.X)} samples")

        except Exception as e:
            messagebox.showerror("Filter Error", f"Failed to apply filter: {str(e)}")

    def _trim_wavelengths(self):
        """Trim wavelength range of current dataset."""
        if not self.data_source_manager:
            return

        if self.X is None:
            messagebox.showwarning("No Data", "Please load or select data first")
            return

        try:
            min_wl = self.min_wavelength_var.get() if self.min_wavelength_var.get() else None
            max_wl = self.max_wavelength_var.get() if self.max_wavelength_var.get() else None

            # Apply trimming
            original_wl = len(self.X.columns)
            self.X = self.data_source_manager.trim_wavelengths(self.X, min_wl, max_wl)

            messagebox.showinfo("Wavelengths Trimmed",
                f"Trimmed from {original_wl} to {len(self.X.columns)} wavelengths")

        except Exception as e:
            messagebox.showerror("Trim Error", f"Failed to trim wavelengths: {str(e)}")

    def _export_to_csv(self):
        """Export current dataset to CSV."""
        if not self.data_source_manager:
            return

        if self.X is None:
            messagebox.showwarning("No Data", "No data to export")
            return

        filename = filedialog.asksaveasfilename(
            title="Export to CSV",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv"), ("All files", "*.*")]
        )

        if filename:
            try:
                self.data_source_manager.export_to_csv(
                    self.X, self.y, self.ref, filename
                )
                messagebox.showinfo("Success", f"Data exported to {filename}")
            except Exception as e:
                messagebox.showerror("Export Error", f"Failed to export: {str(e)}")

    def _export_to_excel(self):
        """Export current dataset to Excel."""
        if not self.data_source_manager:
            return

        if self.X is None:
            messagebox.showwarning("No Data", "No data to export")
            return

        filename = filedialog.asksaveasfilename(
            title="Export to Excel",
            defaultextension=".xlsx",
            filetypes=[("Excel files", "*.xlsx"), ("All files", "*.*")]
        )

        if filename:
            try:
                # Create Excel writer
                with pd.ExcelWriter(filename) as writer:
                    # Export spectral data
                    export_df = self.X.copy()
                    if self.y is not None:
                        export_df.insert(0, 'Target', self.y)
                    # Add metadata columns (from combined file or reference file)
                    if hasattr(self, 'combined_metadata_df') and self.combined_metadata_df is not None:
                        for col in self.combined_metadata_df.columns:
                            export_df.insert(0, col, self.combined_metadata_df[col])
                    elif self.ref is not None:
                        for col in self.ref.columns:
                            export_df.insert(0, col, self.ref[col])

                    export_df.to_excel(writer, sheet_name='Spectral_Data')

                    # Add metadata sheet
                    metadata = pd.DataFrame({
                        'Property': ['Samples', 'Wavelengths', 'Has_Targets', 'Has_Metadata'],
                        'Value': [len(self.X), len(self.X.columns),
                                self.y is not None, self.ref is not None]
                    })
                    metadata.to_excel(writer, sheet_name='Metadata', index=False)

                messagebox.showinfo("Success", f"Data exported to {filename}")
            except Exception as e:
                messagebox.showerror("Export Error", f"Failed to export: {str(e)}")

    def _update_spectral_plots(self):
        """Update spectral plots when data changes."""
        # This will be called when data is loaded from Data Management tab
        # The existing plot update logic can be reused
        pass

    def _update_data_viewer(self):
        """Update Data Viewer tab when data changes."""
        # This will be called when data is loaded from Data Management tab
        # The existing data viewer update logic can be reused
        pass

    # === END OF TAB 0 HELPER METHODS ===

    def _load_and_plot_data(self):
        """Load data and generate spectral plots."""
        try:
            from spectral_predict.io import read_csv_spectra, read_reference_csv, align_xy, read_asd_dir, read_spc_dir

            self.tab1_status.config(text="Loading data...")
            self.root.update()

            # Check if spectral data has been selected and detected
            if not self.spectral_data_path.get():
                messagebox.showwarning("Missing Input", "Please select spectral data directory")
                return

            if not self.detected_type:
                messagebox.showwarning("No Data Detected",
                    "Could not detect spectral data type.\n\nPlease ensure the directory contains:\n• .asd files\n• .csv files\n• .spc files (GRAMS)")
                return

            # Load spectral data based on detected type
            if self.detected_type in ["combined", "combined_excel"]:
                # Combined format - all data in one file (CSV/TXT or Excel)
                from spectral_predict.io import read_combined_csv, read_combined_excel

                if self.detected_type == "combined_excel":
                    X_aligned, y_aligned, metadata_df, metadata = read_combined_excel(
                        self.combined_file_path,
                        y_col=self.target_column.get() if self.target_column.get() else None,
                        sheet_name=self.combined_sheet_name
                    )
                else:
                    X_aligned, y_aligned, metadata_df, metadata = read_combined_csv(
                        self.combined_file_path,
                        y_col=self.target_column.get() if self.target_column.get() else None
                    )

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False
                self.combined_metadata = metadata

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                # Don't mix metadata with reference data - keep them separate
                self.ref = None  # No separate reference file for combined format
                self.combined_metadata_df = metadata_df  # Store metadata columns separately

                # Show success message
                format_name = "Excel" if self.detected_type == "combined_excel" else "CSV/TXT"
                print(f"\n✓ Loaded combined {format_name} format:")
                print(f"  • Spectra: {metadata['n_spectra']}")
                print(f"  • Wavelengths: {metadata['wavelength_range'][0]:.1f} - {metadata['wavelength_range'][1]:.1f} nm")
                print(f"  • Specimen ID: {metadata['specimen_id_col']}")
                print(f"  • Target: {metadata['y_col']}")
                if metadata.get('metadata_cols'):
                    print(f"  • Metadata columns: {', '.join(metadata['metadata_cols'])}")

            elif self.detected_type == "asd":
                X, metadata = read_asd_dir(self.spectral_data_path.get())

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False

                # Load reference data
                if not self.reference_file.get():
                    messagebox.showwarning("Missing Input", "Please select reference CSV/Excel file")
                    return

                ref = read_reference_csv(self.reference_file.get(), self.spectral_file_column.get())

                # Align data and get alignment info
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref,
                    self.spectral_file_column.get(),
                    self.target_column.get(),
                    return_alignment_info=True
                )

                # Show alignment report to user
                self._show_alignment_report(alignment_info)

                # Apply specimen ID column if not using filename
                # DISABLED: This function can cause data misalignment
                # X_aligned, y_aligned, ref = self._apply_specimen_id_column(X_aligned, y_aligned, ref)

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                self.ref = ref

            elif self.detected_type == "csv":
                X, metadata = read_csv_spectra(self.spectral_data_path.get())

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False

                # Load reference data
                if not self.reference_file.get():
                    messagebox.showwarning("Missing Input", "Please select reference CSV/Excel file")
                    return

                ref = read_reference_csv(self.reference_file.get(), self.spectral_file_column.get())

                # Align data and get alignment info
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref,
                    self.spectral_file_column.get(),
                    self.target_column.get(),
                    return_alignment_info=True
                )

                # Show alignment report to user
                self._show_alignment_report(alignment_info)

                # Apply specimen ID column if not using filename
                # DISABLED: This function can cause data misalignment
                # X_aligned, y_aligned, ref = self._apply_specimen_id_column(X_aligned, y_aligned, ref)

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                self.ref = ref

            elif self.detected_type == "spc":
                X, metadata = read_spc_dir(self.spectral_data_path.get())

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False

                # Load reference data
                if not self.reference_file.get():
                    messagebox.showwarning("Missing Input", "Please select reference CSV/Excel file")
                    return

                ref = read_reference_csv(self.reference_file.get(), self.spectral_file_column.get())

                # Align data and get alignment info
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref,
                    self.spectral_file_column.get(),
                    self.target_column.get(),
                    return_alignment_info=True
                )

                # Show alignment report to user
                self._show_alignment_report(alignment_info)

                # Apply specimen ID column if not using filename
                # DISABLED: This function can cause data misalignment
                # X_aligned, y_aligned, ref = self._apply_specimen_id_column(X_aligned, y_aligned, ref)

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                self.ref = ref

            elif self.detected_type == "jcamp":
                from spectral_predict.io import read_jcamp_dir

                X, metadata = read_jcamp_dir(self.spectral_data_path.get())

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False

                # Load reference data
                if not self.reference_file.get():
                    messagebox.showwarning("Missing Input", "Please select reference CSV/Excel file")
                    return

                ref = read_reference_csv(self.reference_file.get(), self.spectral_file_column.get())

                # Align data and get alignment info
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref,
                    self.spectral_file_column.get(),
                    self.target_column.get(),
                    return_alignment_info=True
                )

                # Show alignment report to user
                self._show_alignment_report(alignment_info)

                # Apply specimen ID column if not using filename
                # DISABLED: This function can cause data misalignment
                # X_aligned, y_aligned, ref = self._apply_specimen_id_column(X_aligned, y_aligned, ref)

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                self.ref = ref

            elif self.detected_type == "ascii":
                from spectral_predict.io import read_ascii_spectra

                X, metadata = read_ascii_spectra(self.spectral_data_path.get())

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False

                # Load reference data
                if not self.reference_file.get():
                    messagebox.showwarning("Missing Input", "Please select reference CSV/Excel file")
                    return

                ref = read_reference_csv(self.reference_file.get(), self.spectral_file_column.get())

                # Align data and get alignment info
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref,
                    self.spectral_file_column.get(),
                    self.target_column.get(),
                    return_alignment_info=True
                )

                # Show alignment report to user
                self._show_alignment_report(alignment_info)

                # Apply specimen ID column if not using filename
                # DISABLED: This function can cause data misalignment
                # X_aligned, y_aligned, ref = self._apply_specimen_id_column(X_aligned, y_aligned, ref)

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                self.ref = ref

            elif self.detected_type == "opus":
                from spectral_predict.io import read_opus_dir

                X, metadata = read_opus_dir(self.spectral_data_path.get())

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False

                # Load reference data
                if not self.reference_file.get():
                    messagebox.showwarning("Missing Input", "Please select reference CSV/Excel file")
                    return

                ref = read_reference_csv(self.reference_file.get(), self.spectral_file_column.get())

                # Align data and get alignment info
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref,
                    self.spectral_file_column.get(),
                    self.target_column.get(),
                    return_alignment_info=True
                )

                # Show alignment report to user
                self._show_alignment_report(alignment_info)

                # Apply specimen ID column if not using filename
                # DISABLED: This function can cause data misalignment
                # X_aligned, y_aligned, ref = self._apply_specimen_id_column(X_aligned, y_aligned, ref)

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                self.ref = ref

            elif self.detected_type == "perkinelmer":
                from spectral_predict.io import read_perkinelmer_dir

                X, metadata = read_perkinelmer_dir(self.spectral_data_path.get())

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False

                # Load reference data
                if not self.reference_file.get():
                    messagebox.showwarning("Missing Input", "Please select reference CSV/Excel file")
                    return

                ref = read_reference_csv(self.reference_file.get(), self.spectral_file_column.get())

                # Align data and get alignment info
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref,
                    self.spectral_file_column.get(),
                    self.target_column.get(),
                    return_alignment_info=True
                )

                # Show alignment report to user
                self._show_alignment_report(alignment_info)

                # Apply specimen ID column if not using filename
                # DISABLED: This function can cause data misalignment
                # X_aligned, y_aligned, ref = self._apply_specimen_id_column(X_aligned, y_aligned, ref)

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                self.ref = ref

            elif self.detected_type == "excel":
                from spectral_predict.io import read_excel_spectra

                X, metadata = read_excel_spectra(self.spectral_data_path.get())

                # Store data type detection results
                self.original_data_type.set(metadata.get('data_type', 'reflectance'))
                self.current_data_type.set(metadata.get('data_type', 'reflectance'))
                self.type_confidence = metadata.get('type_confidence', 0.0)
                self.type_detection_method = metadata.get('detection_method', 'unknown')
                self.data_has_been_converted = False

                # Load reference data
                if not self.reference_file.get():
                    messagebox.showwarning("Missing Input", "Please select reference CSV/Excel file")
                    return

                ref = read_reference_csv(self.reference_file.get(), self.spectral_file_column.get())

                # Align data and get alignment info
                X_aligned, y_aligned, alignment_info = align_xy(
                    X, ref,
                    self.spectral_file_column.get(),
                    self.target_column.get(),
                    return_alignment_info=True
                )

                # Show alignment report to user
                self._show_alignment_report(alignment_info)

                # Apply specimen ID column if not using filename
                # DISABLED: This function can cause data misalignment
                # X_aligned, y_aligned, ref = self._apply_specimen_id_column(X_aligned, y_aligned, ref)

                # Store original unfiltered data
                self.X_original = X_aligned
                self.y = y_aligned
                self.ref = ref

            else:
                messagebox.showerror("Error", f"Unknown data type: {self.detected_type}")
                return

            # CRITICAL: Verify index alignment to prevent data corruption
            if self.X_original is not None and self.y is not None:
                if not (self.X_original.index.equals(self.y.index)):
                    print("ERROR: X and y indices don't match! This would cause data corruption.")
                    print(f"X index sample: {list(self.X_original.index[:5])}")
                    print(f"y index sample: {list(self.y.index[:5])}")
                    messagebox.showerror("Data Alignment Error",
                                       "Critical: X and y indices don't match after loading!\n"
                                       "This would cause incorrect model training.\n"
                                       "Please check your data format.")
                    return

                # Also check ref alignment if it exists
                if self.ref is not None and len(self.ref) > 0:
                    if not (self.X_original.index.equals(self.ref.index)):
                        print("WARNING: X and ref indices don't match!")
                        print("This may affect metadata display but won't corrupt model training.")

            # Auto-populate wavelength range ONLY if empty
            if not self.wavelength_min.get().strip() and not self.wavelength_max.get().strip():
                wavelengths = self.X_original.columns.astype(float)
                self.wavelength_min.set(str(int(wavelengths.min())))
                self.wavelength_max.set(str(int(wavelengths.max())))

            # Apply wavelength filtering
            self._apply_wavelength_filter()

            # Auto-detect and display task type
            if hasattr(self, 'task_type_detection_label') and self.task_type_detection_label:
                if self.task_type.get() == "auto":
                    if self.y.nunique() == 2:
                        detected_type = "classification (binary)"
                    elif self.y.dtype == 'object' or self.y.nunique() < 10:
                        detected_type = "classification"
                    else:
                        detected_type = "regression"
                    self.task_type_detection_label.config(
                        text=f"Detected: {detected_type}",
                        foreground=self.colors.get('success', 'green')
                    )
                else:
                    # User manually selected
                    self.task_type_detection_label.config(
                        text=f"(manual selection)",
                        foreground=self.colors.get('info', 'blue')
                    )

                # Refresh model checkboxes and tier selection based on detected task type
                # This enables/disables checkboxes and selects appropriate models for the task
                self._on_task_type_changed()

            # Generate plots
            self._generate_plots()

            # Update data type detection UI
            self._update_data_type_status_ui()

            self.tab1_status.config(text=f"✓ Loaded {len(self.X)} samples × {self.X.shape[1]} wavelengths")
            # Enable interactive controls
            self.update_wl_button.config(state='normal')
            self.absorbance_checkbox.config(state='normal')
            self.reset_exclusions_button.config(state='normal')
            # No popup needed - status label shows success and plots are visible

            # Populate the data viewer tab
            self._populate_data_viewer()

        except Exception as e:
            import traceback
            traceback.print_exc()
            messagebox.showerror("Error", f"Failed to load data:\n{e}")
            self.tab1_status.config(text="✗ Error loading data")

    def _show_alignment_report(self, alignment_info):
        """Display a report showing which files were matched and which were excluded."""
        matched = alignment_info['matched_ids']
        unmatched_spectra = alignment_info['unmatched_spectra']
        unmatched_ref = alignment_info['unmatched_reference']
        n_nan = alignment_info['n_nan_dropped']
        fuzzy = alignment_info['used_fuzzy_matching']

        # Build report message
        report_lines = [
            "=== DATA ALIGNMENT REPORT ===\n",
            f"✓ Successfully matched: {len(matched)} samples",
        ]

        if fuzzy:
            report_lines.append("  (Used fuzzy filename matching)")

        if n_nan > 0:
            report_lines.append(f"\n⚠️ Dropped {n_nan} samples with missing target values")

        if unmatched_spectra:
            report_lines.append(f"\n❌ Spectral files WITHOUT reference data ({len(unmatched_spectra)}):")
            # Show first 10, then indicate if there are more
            show_count = min(10, len(unmatched_spectra))
            for i in range(show_count):
                report_lines.append(f"   • {unmatched_spectra[i]}")
            if len(unmatched_spectra) > show_count:
                report_lines.append(f"   ... and {len(unmatched_spectra) - show_count} more")

        if unmatched_ref:
            report_lines.append(f"\n⚠️ Reference entries WITHOUT spectral data ({len(unmatched_ref)}):")
            show_count = min(10, len(unmatched_ref))
            for i in range(show_count):
                report_lines.append(f"   • {unmatched_ref[i]}")
            if len(unmatched_ref) > show_count:
                report_lines.append(f"   ... and {len(unmatched_ref) - show_count} more")

        report_lines.append(f"\n{'='*50}")
        report_lines.append(f"\nFinal dataset: {len(matched)} samples ready for analysis")

        report_text = "\n".join(report_lines)

        # Print to console for reference
        print("\n" + report_text)

        # Show dialog only if there are actual alignment issues (mismatches)
        # Don't show for fuzzy matching (that's helpful, not a problem) or NaN drops (just data cleaning)
        # Only show if spectral files are missing reference data (unmatched_ref is common/harmless)
        if unmatched_spectra:
            # Create a custom dialog with scrollable text
            dialog = tk.Toplevel(self.root)
            dialog.title("Data Alignment Report")
            dialog.geometry("600x500")

            # Add text widget with scrollbar
            frame = ttk.Frame(dialog, padding=10)
            frame.pack(fill='both', expand=True)

            text_widget = tk.Text(frame, wrap='word', font=('Courier', 10),
                                  bg=self.colors['panel'], fg=self.colors['text'],
                                  relief='flat', borderwidth=0,
                                  selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
            scrollbar = ttk.Scrollbar(frame, command=text_widget.yview)
            text_widget.config(yscrollcommand=scrollbar.set)

            text_widget.pack(side='left', fill='both', expand=True)
            scrollbar.pack(side='right', fill='y')

            # Insert report text
            text_widget.insert('1.0', report_text)
            text_widget.config(state='disabled')  # Read-only

            # Add OK button
            button_frame = ttk.Frame(dialog, padding=10)
            button_frame.pack(fill='x')
            ok_button = ttk.Button(button_frame, text="OK", command=dialog.destroy, style='Modern.TButton')
            ok_button.pack()

            # Center dialog on parent
            dialog.transient(self.root)
            dialog.grab_set()

    def _apply_specimen_id_column(self, X_aligned, y_aligned, ref):
        """
        Apply the selected specimen ID column to the aligned data.

        If user selected "N/A - Use Filename", keeps filename as ID.
        Otherwise, replaces the index with the selected ID column values.

        Parameters
        ----------
        X_aligned : pd.DataFrame
            Aligned spectral data (indexed by filename)
        y_aligned : pd.Series
            Aligned target values (indexed by filename)
        ref : pd.DataFrame
            Reference dataframe (indexed by filename)

        Returns
        -------
        X_aligned, y_aligned, ref : tuple
            Data with updated indices based on specimen ID column
        """
        id_col = self.id_column.get()

        # If "N/A - Use Filename" is selected, keep current indices (filenames)
        if id_col == "N/A - Use Filename" or not id_col:
            return X_aligned, y_aligned, ref

        # Otherwise, use the selected ID column as the new index
        if id_col not in ref.columns:
            print(f"Warning: ID column '{id_col}' not found in reference data. Using filenames instead.")
            return X_aligned, y_aligned, ref

        # Get the ID values for the aligned samples (in the same order)
        new_ids = ref.loc[X_aligned.index, id_col]

        # Check for duplicate IDs
        if new_ids.duplicated().any():
            dup_ids = new_ids[new_ids.duplicated()].unique()
            print(f"\n⚠️ WARNING: Found {new_ids.duplicated().sum()} duplicate specimen IDs after alignment!")
            print(f"Duplicate IDs: {list(dup_ids[:10])}")
            print("This may cause issues with data tracking. Consider using unique specimen IDs.\n")

        # Update indices
        X_aligned.index = new_ids
        y_aligned.index = new_ids
        ref.index = new_ids

        print(f"✓ Using '{id_col}' column as specimen IDs")

        return X_aligned, y_aligned, ref

    def _apply_wavelength_filter(self):
        """Apply wavelength filtering to X_original and store in self.X."""
        if self.X_original is None:
            return

        # Get wavelength range
        wl_min = self.wavelength_min.get().strip()
        wl_max = self.wavelength_max.get().strip()

        # Start with full data
        self.X = self.X_original.copy()

        # Apply filtering
        if wl_min or wl_max:
            wavelengths = self.X.columns.astype(float)
            if wl_min:
                self.X = self.X.loc[:, wavelengths >= float(wl_min)]
            if wl_max:
                wavelengths = self.X.columns.astype(float)
                self.X = self.X.loc[:, wavelengths <= float(wl_max)]

    def _update_wavelengths(self):
        """Update wavelength filter and regenerate plots."""
        if self.X_original is None:
            messagebox.showwarning("No Data", "Please load data first")
            return

        try:
            # Validate wavelength inputs
            wl_min = self.wavelength_min.get().strip()
            wl_max = self.wavelength_max.get().strip()

            if wl_min:
                float(wl_min)  # Validate it's a number
            if wl_max:
                float(wl_max)  # Validate it's a number

            # Apply new filter
            self._apply_wavelength_filter()

            # Regenerate plots
            self._generate_plots()

            # Update status
            self.tab1_status.config(text=f"✓ Updated to {len(self.X)} samples × {self.X.shape[1]} wavelengths")

            # Update data viewer
            self._populate_data_viewer()

        except ValueError as e:
            messagebox.showerror("Invalid Input", "Wavelength values must be numbers")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to update wavelengths:\n{e}")

    def _set_analysis_wl_preset(self, min_wl, max_wl):
        """Set analysis wavelength restriction preset values."""
        self.analysis_wl_min.set(str(min_wl))
        self.analysis_wl_max.set(str(max_wl))
        self.enable_analysis_wl_restriction.set(True)

    def _toggle_absorbance(self):
        """
        Toggle between reflectance and absorbance display (legacy method).

        This method is kept for backwards compatibility with the old checkbox.
        New code should use _convert_and_replot() instead.
        """
        if self.X is None:
            return

        # Regenerate plots with current transformation state
        self._generate_plots()

    def _on_data_type_override(self):
        """
        Handle manual override of data type via radio buttons.

        Updates the UI to reflect the user's selection without converting data.
        To actually convert, user must click the conversion button.
        """
        if self.X is None:
            return

        # Update the conversion button text based on current selection
        current = self.current_data_type.get()
        if current == "reflectance":
            self.convert_data_button.config(text="Convert to Absorbance")
        else:
            self.convert_data_button.config(text="Convert to Reflectance")

        # Update status label to show user override
        original = self.original_data_type.get()
        if current != original:
            self.data_type_status_label.config(
                text=f"⚠️  User override: Treating as {current.capitalize()} (originally {original.capitalize()})",
                foreground=self.colors['warning']
            )
        else:
            confidence_str = "High" if self.type_confidence >= 70 else "Low"
            self.data_type_status_label.config(
                text=f"Detected: {current.capitalize()} ({confidence_str} confidence)",
                foreground=self.colors['success'] if self.type_confidence >= 70 else self.colors['warning']
            )

    def _convert_and_replot(self):
        """
        Convert spectral data between reflectance and absorbance and regenerate plots.

        This is the main conversion function that:
        1. Determines target type (opposite of current)
        2. Converts the data using bidirectional conversion functions
        3. Updates state variables
        4. Regenerates all plots with new data and labels
        """
        if self.X is None or self.X_original is None:
            messagebox.showwarning("No Data", "Please load data first.")
            return

        # Determine current and target types
        current_type = self.current_data_type.get()
        target_type = "absorbance" if current_type == "reflectance" else "reflectance"

        # Confirm with user if this seems unusual
        if self.type_confidence >= 70 and current_type != self.original_data_type.get():
            response = messagebox.askyesno(
                "Confirm Conversion",
                f"You're converting to {target_type}, but data was detected as {self.original_data_type.get()} "
                f"with {self.type_confidence:.0f}% confidence.\n\nContinue with conversion?"
            )
            if not response:
                return

        # Convert the data
        try:
            # Convert X (filtered wavelength range)
            X_converted = self._convert_data_type(self.X.values, current_type, target_type)
            self.X = pd.DataFrame(X_converted, index=self.X.index, columns=self.X.columns)

            # Convert X_original (full wavelength range)
            X_orig_converted = self._convert_data_type(self.X_original.values, current_type, target_type)
            self.X_original = pd.DataFrame(X_orig_converted,
                                          index=self.X_original.index,
                                          columns=self.X_original.columns)

            # Update state
            self.current_data_type.set(target_type)
            self.data_has_been_converted = True

            # Update legacy variable for backwards compatibility
            self.use_absorbance.set(target_type == "absorbance")

            # Update button text
            next_target = "absorbance" if target_type == "reflectance" else "reflectance"
            self.convert_data_button.config(text=f"Convert to {next_target.capitalize()}")

            # Update status label
            self.data_type_status_label.config(
                text=f"✓ Converted to {target_type.capitalize()} (from {current_type})",
                foreground=self.colors['accent']
            )

            # Regenerate plots with new data and labels
            self._generate_plots()

            print(f"✓ Successfully converted data to {target_type}")

        except Exception as e:
            messagebox.showerror("Conversion Error", f"Failed to convert data:\n{str(e)}")
            print(f"Error during conversion: {e}")

    def _update_data_type_status_ui(self):
        """
        Update the data type status label and UI controls after data loading.

        Called after data is loaded to show detection results.
        """
        if self.X is None:
            self.data_type_status_label.config(text="No data loaded", foreground=self.colors['text_light'])
            self.convert_data_button.config(state='disabled')
            return

        # Get detected type and confidence
        data_type = self.original_data_type.get()
        confidence = self.type_confidence

        # Format confidence level
        if confidence >= 80:
            conf_str = "High"
            color = self.colors['success']
        elif confidence >= 70:
            conf_str = "Medium"
            color = self.colors['success']
        else:
            conf_str = "Low"
            color = self.colors['warning']

        # Update status label
        status_text = f"Detected: {data_type.capitalize()} ({conf_str} confidence: {confidence:.0f}%)"
        if confidence < 70:
            status_text += " ⚠️"

        self.data_type_status_label.config(text=status_text, foreground=color)

        # Update button text
        opposite_type = "Absorbance" if data_type == "reflectance" else "Reflectance"
        self.convert_data_button.config(text=f"Convert to {opposite_type}", state='normal')

        # Enable radio buttons
        for widget in self.convert_data_button.master.winfo_children():
            if isinstance(widget, ttk.Frame):  # radio_frame
                for radio in widget.winfo_children():
                    if isinstance(radio, ttk.Radiobutton):
                        radio.config(state='normal')

    def _reset_exclusions(self):
        """Reset all spectrum exclusions."""
        self.excluded_spectra.clear()
        self._update_exclusion_status()
        # Regenerate plots to restore all spectra
        self._generate_plots()

    def _update_exclusion_status(self):
        """Update the exclusion status label."""
        n_excluded = len(self.excluded_spectra)
        if n_excluded == 0:
            self.exclusion_status.config(text="No spectra excluded")
        elif n_excluded == 1:
            self.exclusion_status.config(text="1 spectrum excluded")
        else:
            self.exclusion_status.config(text=f"{n_excluded} spectra excluded")

    # ==================== Validation Set Selection Methods ====================

    def _validation_kennard_stone(self, X, n_samples):
        """
        Select validation samples using Kennard-Stone algorithm.
        Maximizes Euclidean distance in X-space for spectral diversity.

        Args:
            X: pandas DataFrame of spectral data
            n_samples: number of samples to select

        Returns:
            list of indices to include in validation set
        """
        from scipy.spatial.distance import pdist, squareform

        X_array = X.values
        n_total = len(X_array)

        if n_samples >= n_total:
            raise ValueError(f"Validation set size ({n_samples}) must be less than total samples ({n_total})")

        # Compute pairwise Euclidean distances
        distances = squareform(pdist(X_array, metric='euclidean'))

        # Start with the two samples that are farthest apart
        max_dist_idx = np.unravel_index(distances.argmax(), distances.shape)
        selected = [max_dist_idx[0], max_dist_idx[1]]

        # Iteratively select samples that maximize minimum distance to already selected
        remaining = list(set(range(n_total)) - set(selected))

        while len(selected) < n_samples:
            # For each remaining sample, find minimum distance to selected samples
            min_distances = []
            for idx in remaining:
                min_dist = min(distances[idx, s] for s in selected)
                min_distances.append((min_dist, idx))

            # Select the sample with maximum minimum distance
            _, best_idx = max(min_distances)
            selected.append(best_idx)
            remaining.remove(best_idx)

        # Convert array indices to DataFrame indices
        return X.index[selected].tolist()

    def _validation_spxy(self, X, y, n_samples):
        """
        Select validation samples using SPXY algorithm.
        Maximizes distance in both X-space and Y-space.

        Args:
            X: pandas DataFrame of spectral data
            y: pandas Series of target values
            n_samples: number of samples to select

        Returns:
            list of indices to include in validation set
        """
        from scipy.spatial.distance import pdist, squareform
        from sklearn.preprocessing import LabelEncoder

        X_array = X.values
        y_values = y.values
        n_total = len(X_array)

        if n_samples >= n_total:
            raise ValueError(f"Validation set size ({n_samples}) must be less than total samples ({n_total})")

        # Handle categorical y values by encoding them
        if y_values.dtype == object or not np.issubdtype(y_values.dtype, np.number):
            # Categorical data - encode to numeric
            le = LabelEncoder()
            y_array = le.fit_transform(y_values).reshape(-1, 1).astype(float)
        else:
            # Numeric data - use as is
            y_array = y_values.reshape(-1, 1).astype(float)

        # Normalize X and y to [0, 1]
        X_norm = (X_array - X_array.min(axis=0)) / (X_array.max(axis=0) - X_array.min(axis=0) + 1e-10)
        y_norm = (y_array - y_array.min()) / (y_array.max() - y_array.min() + 1e-10)

        # Compute distances in X-space
        dist_X = squareform(pdist(X_norm, metric='euclidean'))

        # Compute distances in y-space
        dist_y = squareform(pdist(y_norm, metric='euclidean'))

        # Combine distances: d_SPXY = d_X + d_y
        distances = dist_X + dist_y

        # Start with the two samples that are farthest apart
        max_dist_idx = np.unravel_index(distances.argmax(), distances.shape)
        selected = [max_dist_idx[0], max_dist_idx[1]]

        # Iteratively select samples that maximize minimum distance to already selected
        remaining = list(set(range(n_total)) - set(selected))

        while len(selected) < n_samples:
            # For each remaining sample, find minimum distance to selected samples
            min_distances = []
            for idx in remaining:
                min_dist = min(distances[idx, s] for s in selected)
                min_distances.append((min_dist, idx))

            # Select the sample with maximum minimum distance
            _, best_idx = max(min_distances)
            selected.append(best_idx)
            remaining.remove(best_idx)

        # Convert array indices to DataFrame indices
        return X.index[selected].tolist()

    def _validation_random(self, X, y, n_samples):
        """
        Select validation samples using random sampling.

        Args:
            X: pandas DataFrame of spectral data
            y: pandas Series of target values
            n_samples: number of samples to select

        Returns:
            list of indices to include in validation set
        """
        from sklearn.model_selection import train_test_split

        n_total = len(X)

        if n_samples >= n_total:
            raise ValueError(f"Validation set size ({n_samples}) must be less than total samples ({n_total})")

        # Calculate test size as fraction
        test_size = n_samples / n_total

        # Use train_test_split for random selection with fixed random_state
        _, X_val, _, y_val = train_test_split(
            X, y, test_size=test_size, random_state=42
        )

        return X_val.index.tolist()

    def _validation_stratified(self, X, y, n_samples):
        """
        Select validation samples using stratified sampling.
        For classification: stratifies by class.
        For regression: bins y into quartiles and stratifies.

        Args:
            X: pandas DataFrame of spectral data
            y: pandas Series of target values
            n_samples: number of samples to select

        Returns:
            list of indices to include in validation set
        """
        from sklearn.model_selection import train_test_split

        n_total = len(X)

        if n_samples >= n_total:
            raise ValueError(f"Validation set size ({n_samples}) must be less than total samples ({n_total})")

        # Calculate test size as fraction
        test_size = n_samples / n_total

        # Determine if y is continuous or categorical
        if y.dtype in ['object', 'category'] or len(y.unique()) < 10:
            # Classification: use y directly for stratification
            stratify = y
        else:
            # Regression: bin y into quartiles for pseudo-stratification
            stratify = pd.qcut(y, q=4, labels=False, duplicates='drop')

        try:
            _, X_val, _, y_val = train_test_split(
                X, y, test_size=test_size, random_state=42, stratify=stratify
            )
            return X_val.index.tolist()
        except ValueError as e:
            # If stratification fails (e.g., too few samples per bin), fall back to random
            self._log(f"Stratified sampling failed, using random: {e}")
            return self._validation_random(X, y, n_samples)

    def _create_validation_set(self):
        """Create validation set using the selected algorithm."""
        if self.X is None or self.y is None:
            messagebox.showwarning("No Data", "Please load data first")
            return

        try:
            # Get current data (after exclusions)
            X_available = self.X[~self.X.index.isin(self.excluded_spectra)]
            y_available = self.y[~self.y.index.isin(self.excluded_spectra)]

            if len(X_available) < 10:
                messagebox.showwarning("Insufficient Data",
                                     "Need at least 10 samples to create validation set")
                return

            # Calculate number of validation samples
            val_pct = self.validation_percentage.get() / 100.0
            n_val = int(len(X_available) * val_pct)

            if n_val < 3:
                messagebox.showwarning("Validation Set Too Small",
                                     f"Validation set would have only {n_val} samples. Increase percentage or dataset size.")
                return

            if n_val > len(X_available) * 0.4:
                messagebox.showwarning("Validation Set Too Large",
                                     f"Validation set of {n_val} samples is more than 40% of data. Consider reducing percentage.")
                return

            # Select validation samples based on algorithm
            algorithm = self.validation_algorithm.get()

            if algorithm == "Kennard-Stone":
                selected_indices = self._validation_kennard_stone(X_available, n_val)
            elif algorithm == "SPXY":
                selected_indices = self._validation_spxy(X_available, y_available, n_val)
            elif algorithm == "Random":
                selected_indices = self._validation_random(X_available, y_available, n_val)
            elif algorithm == "Stratified":
                selected_indices = self._validation_stratified(X_available, y_available, n_val)
            else:
                messagebox.showerror("Invalid Algorithm", f"Unknown algorithm: {algorithm}")
                return

            # Store validation set
            self.validation_indices = set(selected_indices)
            self.validation_X = self.X.loc[selected_indices]
            self.validation_y = self.y.loc[selected_indices]
            self.validation_enabled.set(True)

            # Update status label
            n_cal = len(X_available) - n_val
            if hasattr(self, 'validation_status_label'):
                self.validation_status_label.config(
                    text=f"✓ {n_val} validation samples selected ({algorithm})\n"
                         f"Calibration: {n_cal} samples | Validation: {n_val} samples"
                )
            # Success - status label already updated

        except Exception as e:
            messagebox.showerror("Error", f"Failed to create validation set:\n{e}")
            import traceback
            traceback.print_exc()

    def _reset_validation_set(self):
        """Reset/clear the validation set."""
        self.validation_indices.clear()
        self.validation_X = None
        self.validation_y = None
        self.validation_enabled.set(False)

        if hasattr(self, 'validation_status_label'):
            self.validation_status_label.config(text="No validation set created")
        # Validation set cleared - status label updated

    # ==================== End Validation Set Methods ====================

    def _on_spectrum_click(self, event):
        """Handle clicking on a spectrum line to toggle its visibility and show info."""
        line = event.artist
        sample_idx = int(line.get_gid())  # Get stored sample index

        # Get Y value for this sample
        y_value = self.y.values[sample_idx] if self.y is not None else None

        # Format and display specimen information
        info_text = self._format_specimen_info(sample_idx, y_value=y_value)

        # Get click coordinates from the line data
        xdata = line.get_xdata()
        ydata = line.get_ydata()
        if len(xdata) > 0:
            # Use middle point of spectrum for annotation
            mid_idx = len(xdata) // 2
            x_coord = xdata[mid_idx]
            y_coord = ydata[mid_idx]

            # Show annotation with specimen info
            self._create_or_update_annotation(event.artist.axes, x_coord, y_coord, info_text, event.canvas)

        # Toggle exclusion (existing behavior)
        if sample_idx in self.excluded_spectra:
            # Re-include the spectrum
            self.excluded_spectra.remove(sample_idx)
            line.set_alpha(0.3)  # Restore normal alpha
            line.set_linewidth(1.0)  # Restore normal linewidth
        else:
            # Exclude the spectrum
            self.excluded_spectra.add(sample_idx)
            line.set_alpha(0.05)  # Make nearly transparent
            line.set_linewidth(0.5)  # Make thinner

        event.canvas.draw()
        self._update_exclusion_status()

    def _format_specimen_info(self, specimen_idx, y_value=None, y_pred=None, extra_info=None):
        """
        Format specimen information for display in plot annotations.

        Args:
            specimen_idx: Index into self.y for specimen lookup
            y_value: Actual Y value (if available)
            y_pred: Predicted Y value (if available, for Model Dev page)
            extra_info: Dict of additional information to display

        Returns:
            Formatted string for annotation display
        """
        lines = []

        # Get specimen ID if available
        specimen_id = None
        if self.y is not None and hasattr(self.y, 'index'):
            specimen_id = self.y.index[specimen_idx]
            lines.append(f"Specimen: {specimen_id}")

        # Add ALL metadata columns if available (from combined file or reference file)
        if specimen_id is not None:
            metadata_source = None
            if hasattr(self, 'combined_metadata_df') and self.combined_metadata_df is not None and len(self.combined_metadata_df.columns) > 0:
                metadata_source = self.combined_metadata_df
            elif self.ref is not None and len(self.ref.columns) > 0:
                metadata_source = self.ref

            if metadata_source is not None:
                try:
                    metadata_row = metadata_source.loc[specimen_id]
                    for col in sorted(metadata_source.columns):
                        value = metadata_row[col]
                        if pd.notna(value):
                            if isinstance(value, (float, np.floating)):
                                lines.append(f"{col}: {value:.4f}")
                            else:
                                lines.append(f"{col}: {value}")
                        else:
                            lines.append(f"{col}: N/A")
                except KeyError:
                    # Specimen not in metadata
                    pass

        # Format Y value (handle both regression and classification)
        if y_value is not None:
            # Check if this is a classification task with text labels
            if self.label_encoder is not None:
                try:
                    # Decode numeric value back to text label
                    if isinstance(y_value, (int, np.integer)):
                        text_label = self.label_encoder.inverse_transform([int(y_value)])[0]
                    else:
                        text_label = str(y_value)
                    lines.append(f"Y: {text_label}")
                except:
                    # Fallback if decoding fails
                    lines.append(f"Y: {y_value:.4f}" if isinstance(y_value, (float, np.floating)) else f"Y: {y_value}")
            else:
                # Regression or classification without encoder
                if isinstance(y_value, (float, np.floating)):
                    lines.append(f"Y: {y_value:.4f}")
                else:
                    lines.append(f"Y: {y_value}")

        # Add predicted value if available
        if y_pred is not None:
            if self.label_encoder is not None:
                try:
                    # Decode predicted value for classification
                    if isinstance(y_pred, (int, np.integer)):
                        text_label = self.label_encoder.inverse_transform([int(y_pred)])[0]
                    else:
                        text_label = str(y_pred)
                    lines.append(f"Predicted: {text_label}")
                except:
                    lines.append(f"Predicted: {y_pred:.4f}" if isinstance(y_pred, (float, np.floating)) else f"Predicted: {y_pred}")
            else:
                if isinstance(y_pred, (float, np.floating)):
                    lines.append(f"Predicted: {y_pred:.4f}")
                else:
                    lines.append(f"Predicted: {y_pred}")

        # Add extra information
        if extra_info:
            for key, value in extra_info.items():
                if isinstance(value, (float, np.floating)):
                    lines.append(f"{key}: {value:.4f}")
                else:
                    lines.append(f"{key}: {value}")

        return "\n".join(lines)

    def _create_or_update_annotation(self, ax, x, y, text, canvas):
        """
        Create or update annotation at the clicked point with smart positioning to stay in bounds.

        Args:
            ax: Matplotlib axis object
            x: X coordinate for annotation
            y: Y coordinate for annotation
            text: Text to display
            canvas: Matplotlib canvas for redrawing
        """
        # Remove existing annotation if any
        if self.active_annotation is not None:
            try:
                self.active_annotation.remove()
            except:
                pass
            self.active_annotation = None

        # Get axis limits to determine position
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()

        # Calculate relative position of point (0-1 range for each axis)
        x_rel = (x - xlim[0]) / (xlim[1] - xlim[0]) if xlim[1] != xlim[0] else 0.5
        y_rel = (y - ylim[0]) / (ylim[1] - ylim[0]) if ylim[1] != ylim[0] else 0.5

        # Determine annotation position based on point location
        # Use larger offsets for points in the middle, smaller for edges
        # Position annotation away from nearest edge

        # Horizontal offset
        if x_rel < 0.3:  # Point on left side
            x_offset = 20
        elif x_rel > 0.7:  # Point on right side
            x_offset = -20
        else:  # Point in middle
            x_offset = 20

        # Vertical offset
        if y_rel < 0.3:  # Point on bottom
            y_offset = 20
        elif y_rel > 0.7:  # Point on top
            y_offset = -20
        else:  # Point in middle
            y_offset = 20

        # Create new annotation with smart positioning
        annotation = ax.annotate(
            text,
            xy=(x, y),
            xytext=(x_offset, y_offset),
            textcoords='offset points',
            bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.8, edgecolor='black'),
            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black'),
            fontsize=9,
            zorder=1000,  # Ensure annotation is on top
            # Horizontal alignment based on position
            ha='left' if x_offset > 0 else 'right',
            va='bottom' if y_offset > 0 else 'top'
        )

        self.active_annotation = annotation
        canvas.draw()

    def _apply_transformation(self, data):
        """
        Apply absorbance transformation if enabled (legacy compatibility).

        This method is deprecated but kept for backwards compatibility.
        New code should use _convert_data_type() instead.
        """
        if self.use_absorbance.get():
            return self._convert_reflectance_to_absorbance(data)
        else:
            return data

    def _convert_reflectance_to_absorbance(self, data):
        """
        Convert reflectance data to absorbance.

        Formula: A = log10(1/R)

        Parameters
        ----------
        data : numpy.ndarray or pandas.DataFrame
            Reflectance data (values typically in [0, 1])

        Returns
        -------
        numpy.ndarray or pandas.DataFrame
            Absorbance data
        """
        # Add small epsilon to avoid log(0)
        epsilon = 1e-10
        data_safe = np.maximum(data, epsilon)

        # Warn if values seem problematic
        if np.any(data > 1.2):
            print("⚠️  Warning: Some reflectance values > 1.2. Data may already be absorbance.")

        return np.log10(1.0 / data_safe)

    def _convert_absorbance_to_reflectance(self, data):
        """
        Convert absorbance data to reflectance.

        Formula: R = 10^(-A)

        Parameters
        ----------
        data : numpy.ndarray or pandas.DataFrame
            Absorbance data (values typically 0-3)

        Returns
        -------
        numpy.ndarray or pandas.DataFrame
            Reflectance data (lower-bounded at 0, values > 1.0 allowed)
        """
        # Convert using inverse formula
        reflectance = np.power(10, -data)

        # Only clip negative values (keep lower bound at 0)
        # Allow values > 1.0 as they can occur in real spectral data
        reflectance = np.maximum(reflectance, 0.0)

        # Warn if negative values were encountered
        n_negative = np.sum(np.power(10, -data) < 0.0)
        if n_negative > 0:
            print(f"⚠️  Warning: {n_negative} negative values clipped to 0 after conversion.")

        # Info message if values exceed 1.0 (this is acceptable)
        n_above_one = np.sum(reflectance > 1.0)
        if n_above_one > 0:
            print(f"ℹ️  Info: {n_above_one} reflectance values > 1.0 (this is acceptable for real spectra).")

        return reflectance

    def _convert_data_type(self, data, from_type, to_type):
        """
        Convert spectral data between reflectance and absorbance.

        Parameters
        ----------
        data : numpy.ndarray or pandas.DataFrame
            Spectral data to convert
        from_type : str
            Current data type: "reflectance" or "absorbance"
        to_type : str
            Target data type: "reflectance" or "absorbance"

        Returns
        -------
        numpy.ndarray or pandas.DataFrame
            Converted data (or unchanged if from_type == to_type)
        """
        if from_type == to_type:
            print(f"Data is already {to_type}, no conversion needed.")
            return data

        if from_type == "reflectance" and to_type == "absorbance":
            print("Converting reflectance → absorbance (A = log10(1/R))")
            return self._convert_reflectance_to_absorbance(data)
        elif from_type == "absorbance" and to_type == "reflectance":
            print("Converting absorbance → reflectance (R = 10^(-A))")
            return self._convert_absorbance_to_reflectance(data)
        else:
            raise ValueError(f"Unknown conversion: {from_type} → {to_type}")

    def _get_spectral_ylabel(self):
        """
        Get the appropriate Y-axis label for spectral plots.

        Returns
        -------
        str
            "Reflectance" or "Absorbance" based on current data type
        """
        return self.current_data_type.get().capitalize()

    def _generate_plots(self):
        """Generate spectral plots in the plot notebook."""
        if not HAS_MATPLOTLIB:
            messagebox.showwarning("Matplotlib Required", "Matplotlib is required for plotting")
            return

        # Clear existing plots
        for widget in self.plot_notebook.winfo_children():
            widget.destroy()

        # Determine y-axis label based on current data type
        ylabel = self._get_spectral_ylabel()

        # Apply transformation to raw data (for backwards compatibility with derivatives)
        data_transformed = self._apply_transformation(self.X.values)

        # Create plot tabs
        self._create_plot_tab("Raw Spectra", data_transformed, ylabel, "blue", is_raw=True)

        # Generate derivative plots if available
        if HAS_DERIVATIVES:
            # 1st derivative
            deriv1 = SavgolDerivative(deriv=1, window=7)
            X_deriv1 = deriv1.transform(self.X.values)
            self._create_plot_tab("1st Derivative", X_deriv1, "First Derivative", "green", is_raw=False)

            # 2nd derivative
            deriv2 = SavgolDerivative(deriv=2, window=7)
            X_deriv2 = deriv2.transform(self.X.values)
            self._create_plot_tab("2nd Derivative", X_deriv2, "Second Derivative", "red", is_raw=False)
        else:
            messagebox.showwarning("Derivatives Unavailable",
                "Could not import SavgolDerivative. Only raw spectra will be plotted.")

    def _create_plot_tab(self, title, data, ylabel, color, is_raw=False):
        """Create an interactive plot tab with click-to-toggle and zoom/pan.

        Args:
            title: Title for the plot tab
            data: Spectral data to plot (samples x wavelengths)
            ylabel: Y-axis label
            color: Line color
            is_raw: Whether this is raw data (enables click interaction only for raw)
        """
        frame = ttk.Frame(self.plot_notebook)
        self.plot_notebook.add(frame, text=title)

        fig = Figure(figsize=(12, 6))
        ax = fig.add_subplot(111)

        wavelengths = self.X.columns.values
        n_samples = len(data)

        # Determine plotting strategy
        if n_samples <= 50:
            alpha = 0.3
            indices = range(n_samples)
        else:
            alpha = 0.5
            indices = np.random.choice(n_samples, size=50, replace=False)

        # Plot with interactive features (only for raw spectra to keep it simple)
        for i in indices:
            # Determine if this spectrum is currently excluded
            if i in self.excluded_spectra:
                current_alpha = 0.05
                current_linewidth = 0.5
            else:
                current_alpha = alpha
                current_linewidth = 1.0

            line, = ax.plot(wavelengths, data[i, :], alpha=current_alpha,
                          color=color, linewidth=current_linewidth)

            # Make clickable for all spectra (raw and derivatives)
            line.set_gid(str(i))  # Store sample index as gid
            line.set_picker(5)  # Enable picking with 5-point tolerance

        ax.set_xlabel('Wavelength (nm)', fontsize=12)
        ax.set_ylabel(ylabel, fontsize=12)
        ax.set_title(f'{title} (n={n_samples})', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)

        if "Derivative" in title:
            ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5)

        # Add click handler to all spectral plots
        fig.canvas.mpl_connect('pick_event', self._on_spectrum_click)

        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()

        # Add navigation toolbar for zoom/pan
        toolbar_frame = ttk.Frame(frame)
        toolbar_frame.pack(side=tk.TOP, fill=tk.X)
        toolbar = NavigationToolbar2Tk(canvas, toolbar_frame)
        toolbar.update()

        # Pack canvas below toolbar
        canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Add export button
        filename = title.lower().replace(' ', '_').replace('(', '').replace(')', '')
        self._add_plot_export_button(frame, fig, filename)

    # ========== OUTLIER DETECTION METHODS (Phase 3) ==========

    def _run_outlier_detection(self):
        """Run outlier detection analysis."""
        if not HAS_OUTLIER_DETECTION:
            messagebox.showerror("Module Missing",
                "Outlier detection module not found. Please check installation.")
            return

        if self.X is None or self.y is None:
            messagebox.showerror("Error", "Please load data first in the 'Import & Preview' tab")
            return

        try:
            # Get parameters
            n_components = self.n_pca_components.get()

            # Hide Y range controls for categorical data
            if self._is_categorical_target():
                # Disable Y range input fields
                self.y_min_entry.config(state='disabled')
                self.y_max_entry.config(state='disabled')
                y_min = None
                y_max = None
            else:
                # Enable Y range input fields
                self.y_min_entry.config(state='normal')
                self.y_max_entry.config(state='normal')
                y_min = float(self.y_min_bound.get()) if self.y_min_bound.get().strip() else None
                y_max = float(self.y_max_bound.get()) if self.y_max_bound.get().strip() else None

            # Apply absorbance transformation if enabled
            X_data = self._apply_transformation(self.X.values)

            # Run detection
            self.tab2_status.config(text="Running outlier detection...")
            self.root.update()

            self.outlier_report = generate_outlier_report(
                X_data, self.y.values, n_components, y_min, y_max
            )

            # Update visualizations
            self._plot_pca_scores()
            self._plot_hotelling_t2()
            self._plot_q_residuals()
            self._plot_mahalanobis()
            self._plot_y_distribution()

            # Populate table
            self._populate_outlier_table()

            # Update status
            n_high = len(self.outlier_report['high_confidence_outliers'])
            n_moderate = len(self.outlier_report['moderate_confidence_outliers'])
            n_low = len(self.outlier_report['low_confidence_outliers'])

            self.tab2_status.config(
                text=f"Detection complete: {n_high} high confidence, {n_moderate} moderate, {n_low} low confidence outliers"
            )

            # Run imbalance detection
            self._detect_and_display_imbalance()

            # Outlier detection complete - results shown in UI

        except Exception as e:
            messagebox.showerror("Error", f"Outlier detection failed:\n{str(e)}")
            self.tab2_status.config(text="Error during outlier detection")

    def _plot_pca_scores(self):
        """Plot PCA scores (PC1 vs PC2) colored by selected variable."""
        if not HAS_MATPLOTLIB or self.outlier_report is None:
            return

        # Clear existing plot
        for widget in self.pca_plot_frame.winfo_children():
            widget.destroy()

        # Add control frame for color selection
        control_frame = ttk.Frame(self.pca_plot_frame)
        control_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)

        ttk.Label(control_frame, text="Color by:", style='TLabel').pack(side='left', padx=5)

        color_options = self._get_available_color_variables()
        color_combo = ttk.Combobox(control_frame,
                                   textvariable=self.pca_color_var,
                                   values=color_options,
                                   width=20,
                                   state='readonly')
        color_combo.pack(side='left', padx=5)
        color_combo.bind('<<ComboboxSelected>>', lambda e: self._plot_pca_scores())

        # Create plot frame
        plot_frame = ttk.Frame(self.pca_plot_frame)
        plot_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        fig = Figure(figsize=(8, 6))
        ax = fig.add_subplot(111)

        scores = self.outlier_report['pca']['scores']
        y_values = self.y.values
        outliers = self.outlier_report['pca']['outlier_flags']

        # Get color variable selection
        color_by = self.pca_color_var.get()

        # Determine coloring values
        if color_by == 'Y Value':
            color_values = y_values
            is_categorical = self._is_categorical_target()
            color_label = 'Y Value'
        elif color_by == 'None':
            color_values = None
            is_categorical = False
            color_label = None
        else:
            # Metadata column - need to align with y_values using specimen IDs
            # Check combined file metadata first, then reference file metadata
            metadata_source = None
            if hasattr(self, 'combined_metadata_df') and self.combined_metadata_df is not None and color_by in self.combined_metadata_df.columns:
                metadata_source = self.combined_metadata_df
            elif self.ref is not None and color_by in self.ref.columns:
                metadata_source = self.ref

            if metadata_source is not None:
                # Get specimen IDs from y (which aligns with scores)
                specimen_ids = self.y.index

                # Align metadata with these specimen IDs
                try:
                    color_values = metadata_source.loc[specimen_ids, color_by].values
                except KeyError:
                    # Some specimen IDs not in ref - shouldn't happen but handle it
                    print(f"Warning: Some specimen IDs not found in metadata")
                    color_values = None
                    is_categorical = False
                    color_label = None
                else:
                    is_categorical = self._is_categorical_variable(color_by, color_values)
                    color_label = color_by
            else:
                color_values = None
                is_categorical = False
                color_label = None

        # Color points based on selection
        if color_values is None:
            # No color - single color for all points
            if outliers is not None and np.any(outliers):
                # Normal samples
                ax.scatter(scores[~outliers, 0], scores[~outliers, 1],
                          color='steelblue', alpha=0.6, edgecolors='black',
                          linewidths=0.5, s=50, label='Normal')
                # Outlier samples
                ax.scatter(scores[outliers, 0], scores[outliers, 1],
                          color='steelblue', alpha=0.6, edgecolors='red',
                          linewidths=2, s=100, marker='x', label='Outlier')
                ax.legend(loc='best', fontsize=8)
            else:
                ax.scatter(scores[:, 0], scores[:, 1],
                          color='steelblue', alpha=0.6, edgecolors='black',
                          linewidths=0.5, s=50)

        elif is_categorical:
            # Categorical coloring with discrete colors
            unique_vals = np.unique(color_values[pd.notna(color_values)])
            n_vals = len(unique_vals)

            # Choose colormap
            if n_vals <= 10:
                colors = plt.cm.tab10(np.linspace(0, 1, 10))
            elif n_vals <= 20:
                colors = plt.cm.tab20(np.linspace(0, 1, 20))
            else:
                colors = plt.cm.tab20(np.linspace(0, 1, 20))

            color_map = {val: colors[i % len(colors)] for i, val in enumerate(unique_vals)}

            # Plot each category separately
            for val in unique_vals:
                mask = color_values == val
                if outliers is not None:
                    # Separate normal and outlier samples
                    normal_mask = mask & ~outliers
                    outlier_mask = mask & outliers

                    if np.any(normal_mask):
                        ax.scatter(scores[normal_mask, 0], scores[normal_mask, 1],
                                  c=[color_map[val]], alpha=0.6, edgecolors='black',
                                  linewidths=0.5, label=str(val), s=50)
                    if np.any(outlier_mask):
                        ax.scatter(scores[outlier_mask, 0], scores[outlier_mask, 1],
                                  c=[color_map[val]], alpha=0.6, edgecolors='red',
                                  linewidths=2, s=100, marker='x')
                else:
                    ax.scatter(scores[mask, 0], scores[mask, 1],
                              c=[color_map[val]], alpha=0.6, edgecolors='black',
                              linewidths=0.5, label=str(val), s=50)

            # Handle NaN values if present
            nan_mask = pd.isna(color_values)
            if np.any(nan_mask):
                if outliers is not None:
                    normal_nan = nan_mask & ~outliers
                    outlier_nan = nan_mask & outliers
                    if np.any(normal_nan):
                        ax.scatter(scores[normal_nan, 0], scores[normal_nan, 1],
                                  c='lightgray', alpha=0.6, edgecolors='black',
                                  linewidths=0.5, label='N/A', s=50)
                    if np.any(outlier_nan):
                        ax.scatter(scores[outlier_nan, 0], scores[outlier_nan, 1],
                                  c='lightgray', alpha=0.6, edgecolors='red',
                                  linewidths=2, s=100, marker='x')
                else:
                    ax.scatter(scores[nan_mask, 0], scores[nan_mask, 1],
                              c='lightgray', alpha=0.6, edgecolors='black',
                              linewidths=0.5, label='N/A', s=50)

            ax.legend(title=color_label, loc='best', fontsize=8, framealpha=0.9)

        else:
            # Continuous coloring with colormap
            # Convert to numeric if needed (handles string numeric values)
            numeric_color_values = pd.to_numeric(color_values, errors='coerce')
            valid_mask = pd.notna(numeric_color_values)

            if outliers is not None and np.any(outliers):
                # Normal samples
                normal_mask = ~outliers & valid_mask
                if np.any(normal_mask):
                    scatter_normal = ax.scatter(scores[normal_mask, 0], scores[normal_mask, 1],
                                               c=numeric_color_values[normal_mask], cmap='viridis',
                                               alpha=0.6, edgecolors='black', linewidths=0.5, s=50)
                    fig.colorbar(scatter_normal, ax=ax, label=color_label)

                # Outlier samples
                outlier_mask = outliers & valid_mask
                if np.any(outlier_mask):
                    ax.scatter(scores[outlier_mask, 0], scores[outlier_mask, 1],
                              c=numeric_color_values[outlier_mask], cmap='viridis',
                              alpha=0.6, edgecolors='red', linewidths=2, s=100, marker='x')

                # Handle NaN values
                nan_mask = ~valid_mask
                if np.any(nan_mask):
                    normal_nan = nan_mask & ~outliers
                    outlier_nan = nan_mask & outliers
                    if np.any(normal_nan):
                        ax.scatter(scores[normal_nan, 0], scores[normal_nan, 1],
                                  c='lightgray', alpha=0.6, edgecolors='black',
                                  linewidths=0.5, s=50, label='N/A')
                    if np.any(outlier_nan):
                        ax.scatter(scores[outlier_nan, 0], scores[outlier_nan, 1],
                                  c='lightgray', alpha=0.6, edgecolors='red',
                                  linewidths=2, s=100, marker='x')
                    ax.legend(loc='best', fontsize=8)
            else:
                # No outliers
                if np.any(valid_mask):
                    scatter = ax.scatter(scores[valid_mask, 0], scores[valid_mask, 1],
                                       c=numeric_color_values[valid_mask], cmap='viridis',
                                       alpha=0.6, edgecolors='black', linewidths=0.5, s=50)
                    fig.colorbar(scatter, ax=ax, label=color_label)

                # Handle NaN values
                nan_mask = ~valid_mask
                if np.any(nan_mask):
                    ax.scatter(scores[nan_mask, 0], scores[nan_mask, 1],
                              c='lightgray', alpha=0.6, edgecolors='black',
                              linewidths=0.5, s=50, label='N/A')
                    ax.legend(loc='best', fontsize=8)

        ax.set_xlabel(f'PC1 ({self.outlier_report["pca"]["variance_explained"][0]*100:.1f}%)')
        ax.set_ylabel(f'PC2 ({self.outlier_report["pca"]["variance_explained"][1]*100:.1f}%)')
        ax.set_title('PCA Score Plot (PC1 vs PC2)')
        ax.grid(True, alpha=0.3)
        fig.tight_layout()

        canvas = FigureCanvasTkAgg(fig, plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add click handler for point identification
        def on_pca_click(event):
            if event.inaxes != ax:
                return

            # Find nearest point to click
            click_x, click_y = event.xdata, event.ydata
            if click_x is None or click_y is None:
                return

            # Calculate distances to all points
            distances = np.sqrt((scores[:, 0] - click_x)**2 + (scores[:, 1] - click_y)**2)
            nearest_idx = np.argmin(distances)

            # Only show annotation if click is reasonably close (within 10% of plot range)
            x_range = ax.get_xlim()[1] - ax.get_xlim()[0]
            y_range = ax.get_ylim()[1] - ax.get_ylim()[0]
            threshold = 0.1 * np.sqrt(x_range**2 + y_range**2)

            if distances[nearest_idx] < threshold:
                # Get specimen info
                y_value = y_values[nearest_idx]
                pc1 = scores[nearest_idx, 0]
                pc2 = scores[nearest_idx, 1]
                is_outlier = outliers[nearest_idx] if outliers is not None else False

                extra_info = {
                    'PC1': pc1,
                    'PC2': pc2,
                    'Outlier': 'Yes' if is_outlier else 'No'
                }

                # Add color variable to extra_info if it's a metadata column
                if color_by not in ['None', 'Y Value'] and color_values is not None:
                    color_val = color_values[nearest_idx]
                    if pd.notna(color_val):
                        if isinstance(color_val, (float, np.floating)):
                            extra_info[color_by] = f"{color_val:.4f}"
                        else:
                            extra_info[color_by] = str(color_val)
                    else:
                        extra_info[color_by] = "N/A"

                info_text = self._format_specimen_info(nearest_idx, y_value=y_value, extra_info=extra_info)
                self._create_or_update_annotation(ax, pc1, pc2, info_text, canvas)

        fig.canvas.mpl_connect('button_press_event', on_pca_click)

        # Add export button
        self._add_plot_export_button(self.pca_plot_frame, fig, "pca_scores")

    def _plot_hotelling_t2(self):
        """Plot Hotelling T² statistics with threshold."""
        if not HAS_MATPLOTLIB or self.outlier_report is None:
            return

        # Clear existing plot
        for widget in self.t2_plot_frame.winfo_children():
            widget.destroy()

        fig = Figure(figsize=(8, 6))
        ax = fig.add_subplot(111)

        t2_values = self.outlier_report['pca']['hotelling_t2']
        threshold = self.outlier_report['pca']['t2_threshold']
        outliers = self.outlier_report['pca']['outlier_flags']

        # Bar chart
        colors = ['red' if o else 'steelblue' for o in outliers]
        ax.bar(range(len(t2_values)), t2_values, color=colors, alpha=0.7)
        ax.axhline(threshold, color='red', linestyle='--', linewidth=2, label=f'95% Threshold ({threshold:.2f})')

        ax.set_xlabel('Sample Index')
        ax.set_ylabel('Hotelling T²')
        ax.set_title('Hotelling T² Statistic')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        fig.tight_layout()

        canvas = FigureCanvasTkAgg(fig, self.t2_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add click handler for bar identification
        def on_t2_click(event):
            if event.inaxes != ax or event.xdata is None:
                return

            # Find nearest bar (round to nearest integer)
            bar_idx = int(round(event.xdata))
            if 0 <= bar_idx < len(t2_values):
                y_value = self.y.values[bar_idx]
                t2_value = t2_values[bar_idx]
                is_outlier = outliers[bar_idx]

                extra_info = {
                    'T²': t2_value,
                    'Threshold': threshold,
                    'Outlier': 'Yes' if is_outlier else 'No'
                }

                info_text = self._format_specimen_info(bar_idx, y_value=y_value, extra_info=extra_info)
                self._create_or_update_annotation(ax, bar_idx, t2_value, info_text, canvas)

        fig.canvas.mpl_connect('button_press_event', on_t2_click)

        # Add export button
        self._add_plot_export_button(self.t2_plot_frame, fig, "hotelling_t2")

    def _plot_q_residuals(self):
        """Plot Q-residuals with threshold."""
        if not HAS_MATPLOTLIB or self.outlier_report is None:
            return

        # Clear existing plot
        for widget in self.q_plot_frame.winfo_children():
            widget.destroy()

        fig = Figure(figsize=(8, 6))
        ax = fig.add_subplot(111)

        q_values = self.outlier_report['q_residuals']['q_residuals']
        threshold = self.outlier_report['q_residuals']['q_threshold']
        outliers = self.outlier_report['q_residuals']['outlier_flags']

        # Bar chart
        colors = ['red' if o else 'steelblue' for o in outliers]
        ax.bar(range(len(q_values)), q_values, color=colors, alpha=0.7)
        ax.axhline(threshold, color='red', linestyle='--', linewidth=2, label=f'95% Threshold ({threshold:.2f})')

        ax.set_xlabel('Sample Index')
        ax.set_ylabel('Q-Residual (SPE)')
        ax.set_title('Q-Residuals (Squared Prediction Error)')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        fig.tight_layout()

        canvas = FigureCanvasTkAgg(fig, self.q_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add click handler for bar identification
        def on_q_click(event):
            if event.inaxes != ax or event.xdata is None:
                return

            # Find nearest bar (round to nearest integer)
            bar_idx = int(round(event.xdata))
            if 0 <= bar_idx < len(q_values):
                y_value = self.y.values[bar_idx]
                q_value = q_values[bar_idx]
                is_outlier = outliers[bar_idx]

                extra_info = {
                    'Q-Residual': q_value,
                    'Threshold': threshold,
                    'Outlier': 'Yes' if is_outlier else 'No'
                }

                info_text = self._format_specimen_info(bar_idx, y_value=y_value, extra_info=extra_info)
                self._create_or_update_annotation(ax, bar_idx, q_value, info_text, canvas)

        fig.canvas.mpl_connect('button_press_event', on_q_click)

        # Add export button
        self._add_plot_export_button(self.q_plot_frame, fig, "q_residuals")

    def _plot_mahalanobis(self):
        """Plot Mahalanobis distances with threshold."""
        if not HAS_MATPLOTLIB or self.outlier_report is None:
            return

        # Clear existing plot
        for widget in self.maha_plot_frame.winfo_children():
            widget.destroy()

        fig = Figure(figsize=(8, 6))
        ax = fig.add_subplot(111)

        distances = self.outlier_report['mahalanobis']['distances']
        threshold = self.outlier_report['mahalanobis']['threshold']
        outliers = self.outlier_report['mahalanobis']['outlier_flags']

        # Bar chart
        colors = ['red' if o else 'steelblue' for o in outliers]
        ax.bar(range(len(distances)), distances, color=colors, alpha=0.7)
        ax.axhline(threshold, color='red', linestyle='--', linewidth=2,
                  label=f'Threshold ({threshold:.2f})')

        ax.set_xlabel('Sample Index')
        ax.set_ylabel('Mahalanobis Distance')
        ax.set_title('Mahalanobis Distance in PCA Space')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        fig.tight_layout()

        canvas = FigureCanvasTkAgg(fig, self.maha_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add click handler for bar identification
        def on_maha_click(event):
            if event.inaxes != ax or event.xdata is None:
                return

            # Find nearest bar (round to nearest integer)
            bar_idx = int(round(event.xdata))
            if 0 <= bar_idx < len(distances):
                y_value = self.y.values[bar_idx]
                distance = distances[bar_idx]
                is_outlier = outliers[bar_idx]

                extra_info = {
                    'Mahalanobis': distance,
                    'Threshold': threshold,
                    'Outlier': 'Yes' if is_outlier else 'No'
                }

                info_text = self._format_specimen_info(bar_idx, y_value=y_value, extra_info=extra_info)
                self._create_or_update_annotation(ax, bar_idx, distance, info_text, canvas)

        fig.canvas.mpl_connect('button_press_event', on_maha_click)

        # Add export button
        self._add_plot_export_button(self.maha_plot_frame, fig, "mahalanobis_distance")

    def _plot_y_distribution(self):
        """Plot Y value distribution - handles both continuous and categorical."""
        if not HAS_MATPLOTLIB or self.outlier_report is None:
            return

        if self._is_categorical_target():
            self._plot_y_distribution_categorical()
        else:
            self._plot_y_distribution_continuous()

    def _plot_y_distribution_continuous(self):
        """Plot continuous Y value distribution with histogram and boxplot."""
        # Clear existing plot
        for widget in self.y_dist_plot_frame.winfo_children():
            widget.destroy()

        fig = Figure(figsize=(8, 6))
        ax1 = fig.add_subplot(211)
        ax2 = fig.add_subplot(212)

        y_values = self.y.values
        outliers = self.outlier_report['y_consistency']['all_outliers']

        # Histogram
        ax1.hist(y_values, bins=30, color='steelblue', alpha=0.7, edgecolor='black')
        if np.any(outliers):
            ax1.hist(y_values[outliers], bins=30, color='red', alpha=0.7,
                    edgecolor='black', label='Y Outliers')
            ax1.legend()
        ax1.set_xlabel('Y Value')
        ax1.set_ylabel('Frequency')
        ax1.set_title('Y Value Distribution')
        ax1.grid(True, alpha=0.3, axis='y')

        # Box plot
        bp = ax2.boxplot([y_values], vert=False, patch_artist=True, widths=0.5)
        bp['boxes'][0].set_facecolor('steelblue')
        bp['boxes'][0].set_alpha(0.7)

        if np.any(outliers):
            ax2.scatter(y_values[outliers], np.ones(np.sum(outliers)),
                       color='red', s=100, zorder=3, label='Y Outliers')
            ax2.legend()

        ax2.set_xlabel('Y Value')
        ax2.set_title('Y Value Box Plot')
        ax2.set_yticks([])
        ax2.grid(True, alpha=0.3, axis='x')

        fig.tight_layout()

        canvas = FigureCanvasTkAgg(fig, self.y_dist_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add export button
        self._add_plot_export_button(self.y_dist_plot_frame, fig, "y_distribution")

    def _plot_y_distribution_categorical(self):
        """Plot categorical Y value distribution with bar chart."""
        # Clear existing plot
        for widget in self.y_dist_plot_frame.winfo_children():
            widget.destroy()

        y_consistency = self.outlier_report['y_consistency']

        # Get class distribution from outlier report
        if 'unique_values' in y_consistency and 'value_counts' in y_consistency:
            unique_values = y_consistency['unique_values']
            counts = y_consistency['value_counts']
        else:
            # Fallback: compute from self.y
            unique_values, counts = np.unique(self.y.values, return_counts=True)

        # Create figure with single bar chart
        fig = Figure(figsize=(10, 6))
        ax = fig.add_subplot(111)

        # Create bar chart
        x_pos = np.arange(len(unique_values))
        bars = ax.bar(x_pos, counts, color='steelblue', alpha=0.7, edgecolor='black')

        # Add value labels on bars
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(count)}',
                   ha='center', va='bottom', fontweight='bold')

        # Set labels and formatting
        ax.set_xticks(x_pos)
        ax.set_xticklabels(unique_values, rotation=45, ha='right')
        ax.set_xlabel('Category', fontsize=10, fontweight='bold')
        ax.set_ylabel('Frequency', fontsize=10, fontweight='bold')
        ax.set_title('Target Variable Class Distribution', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')

        # Add statistics text
        total_samples = sum(counts)
        proportions = [count/total_samples for count in counts]
        stats_text = "Class Distribution:\n"
        for val, count, prop in zip(unique_values, counts, proportions):
            stats_text += f"  {val}: {count} ({prop*100:.1f}%)\n"

        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
               fontfamily='monospace', fontsize=8)

        fig.tight_layout()

        # Add to GUI
        canvas = FigureCanvasTkAgg(fig, self.y_dist_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add export button
        self._add_plot_export_button(self.y_dist_plot_frame, fig, "class_distribution")

    def _populate_outlier_table(self):
        """Populate the outlier summary table."""
        if self.outlier_report is None:
            return

        # Clear existing
        for item in self.outlier_tree.get_children():
            self.outlier_tree.delete(item)

        # Get summary
        summary = self.outlier_report['outlier_summary']

        # Add rows
        for idx, row in summary.iterrows():
            # Format Y value - handle both numeric and categorical
            y_val = row['Y_Value']
            if isinstance(y_val, (int, float)):
                y_display = f"{y_val:.2f}"
            else:
                y_display = str(y_val)

            values = (
                row['Sample_Index'],
                y_display,  # Use formatted value
                "✓" if row['T2_Outlier'] else "",
                "✓" if row['Q_Outlier'] else "",
                "✓" if row['Maha_Outlier'] else "",
                "✓" if row['Y_Outlier'] else "",
                row['Total_Flags']
            )

            # Color code by flags
            if row['Total_Flags'] >= 3:
                tag = 'high'
            elif row['Total_Flags'] == 2:
                tag = 'moderate'
            else:
                tag = 'normal'

            self.outlier_tree.insert('', 'end', values=values, tags=(tag,))

        # Configure tags
        self.outlier_tree.tag_configure('high', background='#ffcccc')
        self.outlier_tree.tag_configure('moderate', background='#ffffcc')
        self.outlier_tree.tag_configure('normal', background='white')

    def _auto_select_flagged(self):
        """Auto-select all flagged samples."""
        if self.outlier_report is None:
            return

        # Clear selection
        for item in self.outlier_tree.selection():
            self.outlier_tree.selection_remove(item)

        if self.select_all_flagged.get():
            # Select all with at least 1 flag
            summary = self.outlier_report['outlier_summary']
            flagged = summary[summary['Total_Flags'] > 0]

            for idx, row in flagged.iterrows():
                # Find the tree item corresponding to this row
                for item in self.outlier_tree.get_children():
                    if int(self.outlier_tree.item(item, 'values')[0]) == row['Sample_Index']:
                        self.outlier_tree.selection_add(item)
                        break

        self._update_outlier_selection_status()

    def _auto_select_high_confidence(self):
        """Auto-select high confidence outliers (3+ flags)."""
        if self.outlier_report is None:
            return

        # Clear selection
        for item in self.outlier_tree.selection():
            self.outlier_tree.selection_remove(item)

        if self.select_high_conf.get():
            # Select samples with 3+ flags
            summary = self.outlier_report['outlier_summary']
            high_conf = summary[summary['Total_Flags'] >= 3]

            for idx, row in high_conf.iterrows():
                for item in self.outlier_tree.get_children():
                    if int(self.outlier_tree.item(item, 'values')[0]) == row['Sample_Index']:
                        self.outlier_tree.selection_add(item)
                        break

        self._update_outlier_selection_status()

    def _auto_select_moderate_confidence(self):
        """Auto-select moderate confidence outliers (2 flags)."""
        if self.outlier_report is None:
            return

        # Clear selection
        for item in self.outlier_tree.selection():
            self.outlier_tree.selection_remove(item)

        if self.select_moderate_conf.get():
            # Select samples with 2 flags
            summary = self.outlier_report['outlier_summary']
            moderate_conf = summary[summary['Total_Flags'] == 2]

            for idx, row in moderate_conf.iterrows():
                for item in self.outlier_tree.get_children():
                    if int(self.outlier_tree.item(item, 'values')[0]) == row['Sample_Index']:
                        self.outlier_tree.selection_add(item)
                        break

        self._update_outlier_selection_status()

    def _update_outlier_selection_status(self):
        """Update the selection status label."""
        n_selected = len(self.outlier_tree.selection())
        self.outlier_selection_status.config(text=f"{n_selected} samples selected")

    def _mark_selected_for_exclusion(self):
        """Add selected samples to unified exclusion set."""
        selected = self.outlier_tree.selection()

        if not selected:
            messagebox.showwarning("No Selection", "Please select samples to exclude")
            return

        # Get selected indices
        added_count = 0
        for item in selected:
            values = self.outlier_tree.item(item, 'values')
            sample_idx = int(values[0])
            if sample_idx not in self.excluded_spectra:
                self.excluded_spectra.add(sample_idx)
                added_count += 1

        # Update plots in Tab 1 if data is loaded
        if self.X is not None:
            self._generate_plots()
        # Samples excluded - plots updated

    def _export_outlier_report(self):
        """Export outlier detection report to CSV."""
        if self.outlier_report is None:
            messagebox.showerror("Error", "Run outlier detection first")
            return

        try:
            # Ask for file location
            filepath = filedialog.asksaveasfilename(
                defaultextension=".csv",
                filetypes=[("CSV files", "*.csv"), ("All files", "*.*")],
                title="Save Outlier Report"
            )

            if filepath:
                summary = self.outlier_report['outlier_summary']
                summary.to_csv(filepath, index=False)
                # Report exported successfully
        except Exception as e:
            messagebox.showerror("Export Error", f"Failed to export report:\n{str(e)}")

    # ========== END OUTLIER DETECTION METHODS ==========

    # ========== IMBALANCE HANDLING METHODS ==========

    def _detect_and_display_imbalance(self):
        """Detect class/target imbalance and update UI with recommendations."""
        if self.y is None:
            return

        try:
            # Import imbalance detection functions
            import sys
            from pathlib import Path
            src_path = Path(__file__).parent / "src"
            if str(src_path) not in sys.path:
                sys.path.insert(0, str(src_path))

            from spectral_predict.imbalance import (
                detect_class_imbalance,
                detect_regression_imbalance,
                recommend_imbalance_method
            )

            task_type_setting = self.task_type.get()
            y_values = self.y.values

            # Auto-detect task type if set to "auto"
            if task_type_setting == "auto":
                # Auto-detect task type (same logic as analysis)
                if self.y.nunique() == 2:
                    task_type = "classification"
                elif self.y.dtype == 'object' or self.y.nunique() < 10:
                    task_type = "classification"
                else:
                    task_type = "regression"
            else:
                task_type = task_type_setting

            # Update method dropdown based on task type
            if task_type == 'classification':
                classification_methods = [
                    'smote', 'adasyn', 'borderline_smote', 'random_undersampler',
                    'tomek_links', 'smote_tomek', 'class_weight'
                ]
                self.imbalance_method_combo['values'] = classification_methods
                # Set default if current selection is not valid
                if self.imbalance_method.get() not in classification_methods:
                    self.imbalance_method.set('smote')
            else:  # regression
                regression_methods = ['undersample', 'binning', 'rare_boost', 'balanced']
                self.imbalance_method_combo['values'] = regression_methods
                # Set default if current selection is not valid
                if self.imbalance_method.get() not in regression_methods:
                    self.imbalance_method.set('undersample')

            if task_type == 'classification':
                # Detect class imbalance
                info = detect_class_imbalance(y_values)
                recommendation = recommend_imbalance_method(y_values, task_type='classification')

                # Build summary text
                class_counts_str = ', '.join([f"{k}: {v}" for k, v in info['class_counts'].items()])
                summary = f"Class distribution: {class_counts_str}\n"
                summary += f"Imbalance ratio: {info['imbalance_ratio']:.2f}:1"

                if info['majority_class'] is not None:
                    summary += f" (majority: {info['majority_class']}, minority: {info['minority_class']})"

                self.dist_summary_label.config(text=summary)

                # Show warning if imbalanced
                if info['is_imbalanced']:
                    severity_colors = {
                        'moderate': '#ff9800',  # Orange
                        'severe': '#ff6b6b',    # Red
                        'extreme': '#d32f2f'    # Dark red
                    }
                    warning_text = f"⚠ {info['severity'].upper()} IMBALANCE DETECTED"
                    self.imbalance_warning_label.config(
                        text=warning_text,
                        foreground=severity_colors.get(info['severity'], '#ff6b6b')
                    )

                    # Show recommendation
                    rec_text = f"Recommendation: {recommendation['reason']}"
                    if recommendation['recommended_method']:
                        rec_text += f"\nSuggested method: {recommendation['recommended_method']}"
                        # Auto-select recommended method
                        self.imbalance_method.set(recommendation['recommended_method'])
                        self._update_imbalance_method_description(None)

                    if recommendation.get('warnings'):
                        rec_text += "\n\nWarnings:\n" + "\n".join(f"• {w}" for w in recommendation['warnings'])

                    self.imbalance_recommendation_label.config(text=rec_text)
                else:
                    self.imbalance_warning_label.config(text="✓ Data is balanced")
                    self.imbalance_recommendation_label.config(text=info['recommendation'])

            else:  # regression
                # Detect target imbalance
                info = detect_regression_imbalance(y_values)
                recommendation = recommend_imbalance_method(y_values, task_type='regression')

                # Build summary text
                summary = f"Target range: {info['target_range'][0]:.2f} to {info['target_range'][1]:.2f}\n"
                summary += f"Number of samples: {info['n_samples']}\n"
                summary += f"Coverage metric: {info['coverage']:.2f}"

                self.dist_summary_label.config(text=summary)

                # Show warning if imbalanced
                if info['is_imbalanced']:
                    warning_text = f"⚠ {info['severity'].upper()} TARGET IMBALANCE"
                    self.imbalance_warning_label.config(text=warning_text, foreground='#ff9800')

                    # Show recommendation
                    rec_text = f"Recommendation: {recommendation['reason']}"
                    if recommendation['recommended_method']:
                        rec_text += f"\nSuggested method: {recommendation['recommended_method']}"
                        # Auto-select recommended method
                        self.imbalance_method.set(recommendation['recommended_method'])
                        self._update_imbalance_method_description(None)

                    if recommendation.get('warnings'):
                        rec_text += "\n\nWarnings:\n" + "\n".join(f"• {w}" for w in recommendation['warnings'])

                    self.imbalance_recommendation_label.config(text=rec_text)
                else:
                    self.imbalance_warning_label.config(text="✓ Target is balanced")
                    self.imbalance_recommendation_label.config(text=info['recommendation'])

        except Exception as e:
            self.dist_summary_label.config(text=f"Error detecting imbalance: {str(e)}")

    def _toggle_imbalance_controls(self):
        """Enable/disable imbalance handling controls based on checkbox."""
        enabled = self.enable_imbalance_handling.get()
        state = 'normal' if enabled else 'disabled'

        # Enable/disable all imbalance controls
        self.imbalance_method_combo.config(state='readonly' if enabled else 'disabled')

        # Update parameter controls based on current method
        if enabled:
            self._update_imbalance_method_description(None)
        else:
            # Disable all parameter controls
            for widget_name in ['k_neighbors_spin', 'n_bins_spin', 'boost_factor_spin']:
                self.imbalance_widgets[widget_name].config(state='disabled')

    def _update_imbalance_method_description(self, event):
        """Update method description and show/hide relevant parameters."""
        method = self.imbalance_method.get()

        # Method descriptions
        descriptions = {
            'smote': 'SMOTE - Synthetic oversampling (standard)',
            'adasyn': 'ADASYN - Adaptive synthetic sampling',
            'borderline_smote': 'BorderlineSMOTE - Focus on borderline cases',
            'random_undersampler': 'Random undersampling of majority class',
            'tomek_links': 'Tomek Links - Remove boundary noise',
            'smote_tomek': 'SMOTETomek - Combined over/undersampling',
            'class_weight': 'Class weights - No resampling, weight loss function',
            'undersample': 'Undersample over-represented ranges (ideal for many zeros)',
            'binning': 'Target binning - Weight by target frequency (regression)',
            'rare_boost': 'Rare-value boost - Emphasize uncommon targets (regression)',
            'balanced': 'Balanced - Simple inverse frequency (regression)'
        }

        self.imbalance_method_desc.config(text=descriptions.get(method, ''))

        # Show/hide parameters based on method
        # First hide all
        for widget_name in ['k_neighbors_label', 'k_neighbors_spin', 'k_neighbors_desc',
                           'n_bins_label', 'n_bins_spin', 'boost_factor_label', 'boost_factor_spin']:
            widget = self.imbalance_widgets[widget_name]
            widget.grid_remove()

        # Show relevant parameters
        enabled = self.enable_imbalance_handling.get()
        if enabled:
            if method in ['smote', 'adasyn', 'borderline_smote', 'smote_tomek']:
                # Show k_neighbors parameter
                self.imbalance_widgets['k_neighbors_label'].grid()
                self.imbalance_widgets['k_neighbors_spin'].config(state='normal')
                self.imbalance_widgets['k_neighbors_spin'].grid()
                self.imbalance_widgets['k_neighbors_desc'].grid()

            elif method in ['undersample', 'binning']:
                # Show n_bins parameter (for both undersample and binning)
                self.imbalance_widgets['n_bins_label'].grid(row=0, column=0, sticky=tk.W, pady=5, padx=(0, 10))
                self.imbalance_widgets['n_bins_spin'].config(state='normal')
                self.imbalance_widgets['n_bins_spin'].grid(row=0, column=1, sticky=tk.W)

            elif method == 'rare_boost':
                # Show boost_factor parameter
                self.imbalance_widgets['boost_factor_label'].grid(row=0, column=0, sticky=tk.W, pady=5, padx=(0, 10))
                self.imbalance_widgets['boost_factor_spin'].config(state='normal')
                self.imbalance_widgets['boost_factor_spin'].grid(row=0, column=1, sticky=tk.W)

    def _get_imbalance_params(self):
        """Get current imbalance handling parameters for analysis."""
        if not self.enable_imbalance_handling.get():
            return None, None

        method = self.imbalance_method.get()

        # Build parameter dict based on method
        params = {}
        if method in ['smote', 'adasyn', 'borderline_smote', 'smote_tomek']:
            params['k_neighbors'] = self.k_neighbors.get()
        elif method in ['undersample', 'binning']:
            params['n_bins'] = self.n_bins.get()
        elif method == 'rare_boost':
            params['boost_factor'] = self.boost_factor.get()

        return method, params

    # ========== END IMBALANCE HANDLING METHODS ==========

    def _export_preprocessed_csv(self, window_size=None):
        """
        Export preprocessed spectral data (2nd derivative) to CSV.

        Parameters
        ----------
        window_size : int, optional
            Window size for Savitzky-Golay filter. If None, uses first selected window size or defaults to 17.
        """
        try:
            from spectral_predict.preprocess import SavgolDerivative
            import pandas as pd

            # Determine window size
            if window_size is None:
                # Collect window sizes from checkboxes
                window_sizes = []
                if self.window_7.get():
                    window_sizes.append(7)
                if self.window_11.get():
                    window_sizes.append(11)
                if self.window_17.get():
                    window_sizes.append(17)
                if self.window_19.get():
                    window_sizes.append(19)

                # Add custom window sizes
                if self.window_custom.get().strip():
                    try:
                        custom_windows = [int(x.strip()) for x in self.window_custom.get().split(',')]
                        window_sizes.extend(custom_windows)
                    except ValueError:
                        self._log_progress(f"Warning: Invalid custom window size(s) ignored: {self.window_custom.get()}")

                # Use first selected or default to 17
                window_size = window_sizes[0] if window_sizes else 17

            # Apply second derivative preprocessing
            preprocessor = SavgolDerivative(deriv=2, window=window_size, polyorder=3)
            X_preprocessed = preprocessor.transform(self.X.values)

            # Create DataFrame with wavelength column names
            df_preprocessed = pd.DataFrame(
                X_preprocessed,
                columns=self.X.columns,
                index=self.X.index
            )

            # Add response variable as first column
            target_name = self.target_column.get()
            # Include sample ID column so user knows which samples are in the export
            sample_id_col = self.spectral_file_column.get()

            # Safety check: Avoid column name collision with wavelength columns
            if sample_id_col in df_preprocessed.columns:
                original_col = sample_id_col
                sample_id_col = f"{sample_id_col}_ID"
                self._log_progress(f"⚠️  Warning: Sample ID column renamed to '{sample_id_col}' to avoid collision with wavelength column '{original_col}'")

            df_export = pd.DataFrame({
                sample_id_col: self.y.index,
                target_name: self.y.values
            })
            df_export = pd.concat([df_export, df_preprocessed.reset_index(drop=True)], axis=1)

            # Generate filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = Path(self.output_dir.get())
            output_dir.mkdir(parents=True, exist_ok=True)

            csv_path = output_dir / f"preprocessed_data_{target_name}_w{window_size}_{timestamp}.csv"

            # Save to CSV (index=False since we already included sample IDs as a column)
            df_export.to_csv(csv_path, index=False)

            # Log success
            self._log_progress(f"✓ Preprocessed CSV exported: {csv_path}")
            self._log_progress(f"  - Window size: {window_size}, Polyorder: 3 (2nd derivative)")
            self._log_progress(f"  - Shape: {df_export.shape[0]} samples × {df_export.shape[1]} columns")

            return str(csv_path)

        except Exception as e:
            error_msg = f"Failed to export preprocessed CSV: {str(e)}"
            self._log_progress(f"✗ {error_msg}")
            messagebox.showerror("Export Error", error_msg)
            return None

    def _run_analysis(self):
        """Run analysis in background thread."""
        # Validate data is loaded
        if self.X is None or self.y is None:
            messagebox.showwarning("No Data", "Please load data first in the 'Import & Preview' tab")
            return

        # Validate at least one model selected
        selected_models = []
        if self.use_pls.get():
            selected_models.append("PLS")
        if self.use_plsda.get():
            selected_models.append("PLS-DA")
        if self.use_ridge.get():
            selected_models.append("Ridge")
        if self.use_lasso.get():
            selected_models.append("Lasso")
        if self.use_elasticnet.get():
            selected_models.append("ElasticNet")
        if self.use_randomforest.get():
            selected_models.append("RandomForest")
        if self.use_mlp.get():
            selected_models.append("MLP")
        if self.use_neuralboosted.get():
            selected_models.append("NeuralBoosted")
        if self.use_svr.get():
            selected_models.append("SVR")
        if self.use_svm.get():
            selected_models.append("SVM")
        if self.use_xgboost.get():
            selected_models.append("XGBoost")
        if self.use_lightgbm.get():
            selected_models.append("LightGBM")
        if self.use_catboost.get():
            selected_models.append("CatBoost")

        # Get tier selection
        tier = self.model_tier.get()

        # If specific models are selected, pass them as enabled_models (overrides tier)
        # If no models are selected, show warning
        if not selected_models:
            messagebox.showwarning("No Models", "Please select at least one model to test")
            return

        # Switch to Analysis Progress tab (index 4)
        # Tab indices: 0=Data Management, 1=Import, 2=Data Viewer, 3=Quality Check, 4=Analysis Config, 5=Analysis Progress, 6=Results, 7=Custom Model Dev
        self.notebook.select(5)

        # Clear progress text
        self.progress_text.delete('1.0', tk.END)
        self.progress_info.config(text="Starting analysis...")
        self.progress_status.config(text="Analysis in progress...")
        self.best_model_info.config(text="(none yet)")
        self.time_estimate_label.config(text="")

        # Reset start time
        self.analysis_start_time = datetime.now()

        # Start running figure animation
        if hasattr(self, 'running_figure'):
            self.running_figure.start_animation()

        # Export preprocessed CSV if requested
        if self.export_preprocessed_csv.get():
            self._log_progress("\n" + "="*70)
            self._log_progress("EXPORTING PREPROCESSED DATA CSV")
            self._log_progress("="*70)
            csv_path = self._export_preprocessed_csv()
            if csv_path:
                self._log_progress(f"✓ CSV export complete\n")
            else:
                self._log_progress(f"✗ CSV export failed\n")

        # Run in thread
        self.analysis_thread = threading.Thread(target=self._run_analysis_thread, args=(selected_models, tier))
        self.analysis_thread.start()

    def _reconstruct_models_from_results(self, top_models_df, X_train, y_train, task_type):
        """
        Reconstruct and refit models from results DataFrame rows.

        Args:
            top_models_df: DataFrame rows with model configurations
            X_train: Training features
            y_train: Training targets
            task_type: 'regression' or 'classification'

        Returns:
            List of tuples: [(fitted_pipeline, model_name, metadata), ...]
        """
        from spectral_predict.models import get_model
        from spectral_predict.preprocess import SavgolDerivative
        from sklearn.pipeline import Pipeline
        from sklearn.preprocessing import StandardScaler
        import ast

        reconstructed_models = []

        for idx, row in enumerate(top_models_df.itertuples(), 1):
            try:
                # Extract configuration
                model_name = row.Model
                params_str = row.Params
                preprocess = row.Preprocess
                deriv = row.Deriv
                window = row.Window
                polyorder = row.Poly

                # Parse parameters
                if params_str and params_str != '{}':
                    try:
                        params_dict = ast.literal_eval(params_str)
                    except:
                        params_dict = {}
                else:
                    params_dict = {}

                # Create preprocessing pipeline
                steps = []

                # Add derivative/SNV preprocessing if applicable
                if preprocess in ['snv', 'sg1', 'sg2', 'deriv_snv']:
                    if preprocess == 'snv':
                        # SNV only - no derivative
                        pass  # Will add scaler below
                    elif preprocess in ['sg1', 'sg2']:
                        # Savitzky-Golay derivative
                        deriv_order = 1 if preprocess == 'sg1' else 2
                        steps.append(('derivative', SavgolDerivative(
                            window=int(window),
                            polyorder=int(polyorder),
                            deriv=deriv_order
                        )))
                    elif preprocess == 'deriv_snv':
                        # Derivative then SNV
                        deriv_order = int(deriv)
                        steps.append(('derivative', SavgolDerivative(
                            window=int(window),
                            polyorder=int(polyorder),
                            deriv=deriv_order
                        )))

                # Add scaler for models that need it
                if model_name not in ['PLS', 'RandomForest', 'XGBoost', 'LightGBM', 'CatBoost']:
                    steps.append(('scaler', StandardScaler()))

                # Create model with exact parameters
                # Filter params_dict to only include parameters that get_model() accepts
                allowed_params = {'n_components', 'max_n_components', 'max_iter'}
                filtered_params = {k: v for k, v in params_dict.items() if k in allowed_params}

                if filtered_params:
                    model = get_model(model_name, task_type=task_type, **filtered_params)
                else:
                    model = get_model(model_name, task_type=task_type)

                steps.append(('model', model))

                # Create pipeline
                if len(steps) > 1:
                    pipeline = Pipeline(steps)
                else:
                    pipeline = model

                # Fit on training data
                pipeline.fit(X_train, y_train)

                # Store metadata (including wavelength information for transparency)
                metadata = {
                    'model_name': model_name,
                    'preprocess': preprocess,
                    'params': params_dict,
                    'score': row.CompositeScore if hasattr(row, 'CompositeScore') else 0.0,
                    # Wavelength information for reproducibility
                    'wavelengths': X_train.columns.tolist(),
                    'n_vars': X_train.shape[1],
                    'use_full_spectrum_preprocessing': True,
                    'full_wavelengths': X_train.columns.tolist()
                }

                reconstructed_models.append((pipeline, model_name, metadata))

            except Exception as e:
                self._log_progress(f"⚠️ Failed to reconstruct {row.Model}: {e}")
                continue

        return reconstructed_models

    def _on_train_ensemble_click(self):
        """Handle Train Ensemble button click with custom model selection."""
        try:
            # === VALIDATION 1: Check if results exist ===
            if self.results_df is None or len(self.results_df) == 0:
                messagebox.showerror(
                    "No Results",
                    "No analysis results available.\n\n"
                    "Please run an analysis first before training ensembles."
                )
                return

            # === VALIDATION 2: Check if training data is cached ===
            if self.training_data_cache is None:
                messagebox.showerror(
                    "No Training Data",
                    "Training data is not available.\n\n"
                    "This can happen if:\n"
                    "• Analysis was run in an older version\n"
                    "• Application was restarted\n\n"
                    "Please re-run the analysis to cache training data."
                )
                return

            # === VALIDATION 3: Check selection count ===
            selected_count = self.results_df['Select'].sum()

            if selected_count == 0:
                messagebox.showwarning(
                    "No Models Selected",
                    "Please select at least 2 models from the Results table.\n\n"
                    "Click the checkboxes in the 'Select' column to choose models."
                )
                return

            if selected_count == 1:
                messagebox.showwarning(
                    "Too Few Models",
                    "Ensemble requires at least 2 models.\n\n"
                    "You have selected 1 model. Please select at least one more."
                )
                return

            # === VALIDATION 4: Warn if too many models selected ===
            if selected_count > 50:
                proceed = messagebox.askyesno(
                    "Many Models Selected",
                    f"You have selected {selected_count} models.\n\n"
                    f"Training ensembles with many models can be slow.\n"
                    f"This may take several minutes.\n\n"
                    f"Do you want to continue?",
                    icon='warning'
                )
                if not proceed:
                    return

            # === VALIDATION 5: Check if ensemble methods are enabled ===
            ensemble_methods_count = sum([
                self.ensemble_simple_average.get(),
                self.ensemble_region_weighted.get(),
                self.ensemble_mixture_experts.get(),
                self.ensemble_stacking.get(),
                self.ensemble_stacking_region.get()
            ])

            if ensemble_methods_count == 0:
                messagebox.showwarning(
                    "No Ensemble Methods",
                    "No ensemble methods are enabled.\n\n"
                    "Please enable at least one ensemble method in the "
                    "Analysis Configuration tab (Ensemble Settings section)."
                )
                return

            # === VALIDATION 6: Check if already training ===
            if hasattr(self, 'ensemble_training_thread') and \
               self.ensemble_training_thread is not None and \
               self.ensemble_training_thread.is_alive():
                messagebox.showwarning(
                    "Training In Progress",
                    "Ensemble training is already in progress.\n\n"
                    "Please wait for the current training to complete."
                )
                return

            # === ALL VALIDATIONS PASSED - START TRAINING ===
            # Show info message
            self.ensemble_status.config(
                text=f"Training ensembles with {selected_count} selected models..."
            )

            # Disable button during training
            self.btn_train_ensemble.config(state='disabled')

            # Launch background thread
            self.ensemble_training_thread = threading.Thread(
                target=self._train_ensemble_thread,
                daemon=True
            )
            self.ensemble_training_thread.start()

        except Exception as e:
            import traceback
            error_msg = f"Failed to start ensemble training:\n{str(e)}\n\n{traceback.format_exc()}"
            messagebox.showerror("Training Error", error_msg)
            self.btn_train_ensemble.config(state='normal')

    def _train_ensemble_thread(self):
        """Background thread for manual ensemble training."""
        try:
            # Extract cached data
            cache = self.training_data_cache
            X_filtered = cache['X_filtered']
            y_filtered = cache['y_filtered']
            task_type = cache['task_type']
            analysis_wl_min = cache.get('analysis_wl_min')
            analysis_wl_max = cache.get('analysis_wl_max')

            # Temporarily set label_encoder for model reconstruction
            original_encoder = self.label_encoder
            self.label_encoder = cache['label_encoder']

            # Train ensembles
            ensemble_results, trained_ensembles = self._train_ensembles(
                self.results_df,
                X_filtered,
                y_filtered,
                task_type,
                analysis_wl_min_value=analysis_wl_min,
                analysis_wl_max_value=analysis_wl_max,
                is_manual_retrain=True
            )

            # Restore original encoder
            self.label_encoder = original_encoder

            # Update UI on main thread
            if ensemble_results is not None:
                # Store results
                self.ensemble_results = ensemble_results
                self.trained_ensembles = trained_ensembles
                self.ensemble_X = X_filtered
                self.ensemble_y = y_filtered

                # Update metadata
                if not hasattr(self, 'ensemble_metadata'):
                    self.ensemble_metadata = {}
                self.ensemble_metadata.update({
                    'wavelengths': X_filtered.columns.tolist(),
                    'n_wavelengths': X_filtered.shape[1],
                    'analysis_wl_min': analysis_wl_min,
                    'analysis_wl_max': analysis_wl_max,
                    'use_full_spectrum_preprocessing': True,
                    'n_base_models': len(ensemble_results),
                    'manually_retrained': True,
                    'retrain_timestamp': datetime.now().isoformat()
                })

                # Refresh display on main thread
                self.root.after(0, self._populate_ensemble_results)
                self.root.after(0, lambda: messagebox.showinfo(
                    "Training Complete",
                    f"Successfully trained {len(ensemble_results)} ensemble(s)!\n\n"
                    f"Best ensemble: {ensemble_results[0]['method']}\n"
                    f"R² = {ensemble_results[0]['r2']:.4f}"
                ))
            else:
                self.root.after(0, lambda: messagebox.showerror(
                    "Training Failed",
                    "Ensemble training failed. Check the log for details."
                ))

        except Exception as e:
            import traceback
            error_msg = f"Ensemble training error:\n{str(e)}\n\n{traceback.format_exc()}"
            self.root.after(0, lambda: messagebox.showerror("Training Error", error_msg))

        finally:
            # Re-enable button on main thread
            self.root.after(0, lambda: self.btn_train_ensemble.config(state='normal'))

    def _train_ensembles(self, results_df, X_filtered, y_filtered, task_type,
                         analysis_wl_min_value=None, analysis_wl_max_value=None,
                         is_manual_retrain=False):
        """
        Train ensemble models from selected models in results_df.

        Args:
            results_df: DataFrame with model results (must have 'Select' column)
            X_filtered: Training features (filtered by exclusions/validation)
            y_filtered: Training targets (aligned with X_filtered)
            task_type: 'regression' or 'classification'
            analysis_wl_min_value: Minimum wavelength used in analysis (for metadata)
            analysis_wl_max_value: Maximum wavelength used in analysis (for metadata)
            is_manual_retrain: If True, this is a manual retrain (affects logging)

        Returns:
            tuple: (ensemble_results, trained_ensembles) or (None, None) on failure
        """
        try:
            self._log_progress(f"\n{'='*70}")
            if is_manual_retrain:
                self._log_progress(f"RETRAINING ENSEMBLES WITH CUSTOM MODEL SELECTION")
            else:
                self._log_progress(f"RUNNING ENSEMBLE METHODS")
            self._log_progress(f"{'='*70}")

            from spectral_predict.ensemble import create_ensemble
            from sklearn.model_selection import cross_val_predict

            # Select models for ensemble based on context
            if is_manual_retrain:
                # Manual retrain: use currently selected models from checkboxes
                top_models_df = results_df[results_df['Select'] == True].copy()
                self._log_progress(f"Using {len(top_models_df)} manually selected models for ensemble:")
            else:
                # Auto-training: use top N models by CompositeScore
                top_n = self.ensemble_top_n.get()
                top_models_df = results_df.nsmallest(top_n, 'CompositeScore')
                self._log_progress(f"Using top {len(top_models_df)} models (by score) for ensemble:")

            if len(top_models_df) == 0:
                self._log_progress(f"⚠️ No models selected for ensemble, skipping...")
                return None, None

            # Validation: minimum 2 models required
            if len(top_models_df) < 2:
                self._log_progress(f"⚠️ Need at least 2 models for ensemble (got {len(top_models_df)}), skipping...")
                return None, None

            # Log selected models
            for i, row in enumerate(top_models_df.itertuples(), 1):
                self._log_progress(f"  {i}. {row.Model} ({row.Preprocess}) - Score: {row.CompositeScore:.4f}")

            # Reconstruct models from results
            self._log_progress(f"\nReconstructing top {len(top_models_df)} models...")
            reconstructed = self._reconstruct_models_from_results(top_models_df, X_filtered, y_filtered, task_type)

            if len(reconstructed) < 2:
                self._log_progress(f"⚠️ Need at least 2 models for ensemble (got {len(reconstructed)}), skipping...")
                return None, None

            self._log_progress(f"✓ Successfully reconstructed {len(reconstructed)} models")

            # Extract models and names
            models = [m[0] for m in reconstructed]
            model_names = [m[1] for m in reconstructed]

            # Collect ensemble methods to run
            ensemble_methods = []
            if self.ensemble_simple_average.get():
                ensemble_methods.append(('simple_average', 'Simple Average'))
            if self.ensemble_region_weighted.get():
                ensemble_methods.append(('region_weighted', 'Region-Aware Weighted'))
            if self.ensemble_mixture_experts.get():
                ensemble_methods.append(('mixture_experts', 'Mixture of Experts'))
            if self.ensemble_stacking.get():
                ensemble_methods.append(('stacking', 'Stacking'))
            if self.ensemble_stacking_region.get():
                ensemble_methods.append(('region_stacking', 'Stacking + Region Features'))

            if not ensemble_methods:
                self._log_progress("⚠️ No ensemble methods selected, skipping...")
                return None, None

            self._log_progress(f"\nTesting {len(ensemble_methods)} ensemble methods...")
            n_regions = self.ensemble_n_regions.get()
            self._log_progress(f"Number of regions: {n_regions}")

            # Train and evaluate each ensemble method
            ensemble_results = []
            trained_ensembles = {}

            for ensemble_type, ensemble_name in ensemble_methods:
                try:
                    self._log_progress(f"\n--- Training {ensemble_name} ---")

                    # Create ensemble
                    ensemble = create_ensemble(
                        models=models,
                        model_names=model_names,
                        X=X_filtered,
                        y=y_filtered,
                        ensemble_type=ensemble_type,
                        n_regions=n_regions,
                        cv=min(5, len(y_filtered))  # Use 5-fold or less if small dataset
                    )

                    # Make predictions
                    ensemble_pred = ensemble.predict(X_filtered)

                    # Calculate metrics
                    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
                    import numpy as np

                    rmse = np.sqrt(mean_squared_error(y_filtered, ensemble_pred))
                    r2 = r2_score(y_filtered, ensemble_pred)
                    mae = mean_absolute_error(y_filtered, ensemble_pred)

                    # Calculate RPD (Ratio of Performance to Deviation)
                    rpd = np.std(y_filtered) / rmse if rmse > 0 else 0

                    self._log_progress(f"✓ {ensemble_name} Results:")
                    self._log_progress(f"   RMSE: {rmse:.4f}")
                    self._log_progress(f"   R²:   {r2:.4f}")
                    self._log_progress(f"   MAE:  {mae:.4f}")
                    self._log_progress(f"   RPD:  {rpd:.2f}")

                    # Add warning for mixture of experts about cross-validated performance
                    if ensemble_type == 'mixture_experts' and r2 > 0.95:
                        self._log_progress(f"   ⚠️ Note: These metrics are on training data.")
                        self._log_progress(f"   ⚠️ Actual performance on new data will be lower.")

                    # Store results
                    ensemble_results.append({
                        'method': ensemble_name,
                        'type': ensemble_type,
                        'rmse': rmse,
                        'r2': r2,
                        'mae': mae,
                        'rpd': rpd,
                        'ensemble': ensemble
                    })

                    # Store trained ensemble
                    trained_ensembles[ensemble_type] = ensemble

                except Exception as e:
                    self._log_progress(f"✗ {ensemble_name} failed: {e}")
                    import traceback
                    self._log_progress(f"   {traceback.format_exc()}")
                    continue

            # Summary of ensemble results
            if ensemble_results:
                self._log_progress(f"\n{'='*70}")
                self._log_progress(f"ENSEMBLE RESULTS SUMMARY")
                self._log_progress(f"{'='*70}")

                # Sort by R² (best first)
                ensemble_results.sort(key=lambda x: x['r2'], reverse=True)

                # Find best individual model for comparison
                best_individual_r2 = results_df['R2'].max() if 'R2' in results_df.columns else 0
                best_individual_rmse = results_df['RMSE'].min() if 'RMSE' in results_df.columns else float('inf')

                self._log_progress(f"\nBest Individual Model: R²={best_individual_r2:.4f}, RMSE={best_individual_rmse:.4f}")
                self._log_progress(f"\nEnsemble Rankings:")

                for i, result in enumerate(ensemble_results, 1):
                    r2_improvement = result['r2'] - best_individual_r2
                    rmse_improvement = ((best_individual_rmse - result['rmse']) / best_individual_rmse * 100) if best_individual_rmse > 0 else 0

                    status = "🏆" if i == 1 else f"  {i}."
                    self._log_progress(f"{status} {result['method']}")
                    self._log_progress(f"     R²:   {result['r2']:.4f} ({r2_improvement:+.4f})")
                    self._log_progress(f"     RMSE: {result['rmse']:.4f} ({rmse_improvement:+.1f}%)")
                    self._log_progress(f"     RPD:  {result['rpd']:.2f}")

                self._log_progress(f"\n✓ Ensemble training complete!")
                self._log_progress(f"✓ Trained {len(ensemble_results)} ensemble(s)")
                if is_manual_retrain:
                    self._log_progress(f"✓ Ensemble results updated in Results tab")
                else:
                    self._log_progress(f"✓ View ensemble results in the Results tab")

                return ensemble_results, trained_ensembles
            else:
                self._log_progress(f"\n⚠️ No ensembles were successfully trained")
                return None, None

        except Exception as e:
            import traceback
            self._log_progress(f"\n⚠️ Ensemble execution failed: {e}")
            self._log_progress(f"   {traceback.format_exc()}")
            self._log_progress(f"   Individual model results are still available")
            return None, None

    def _run_analysis_thread(self, selected_models, tier):
        """Run analysis in background thread."""
        try:
            from spectral_predict.search import run_search
            from spectral_predict.report import write_markdown_report

            # Determine task type
            task_type_setting = self.task_type.get()

            if task_type_setting == "auto":
                # Auto-detect task type
                if self.y.nunique() == 2:
                    task_type = "classification"
                elif self.y.dtype == 'object' or self.y.nunique() < 10:
                    task_type = "classification"
                else:
                    task_type = "regression"
                self._log_progress(f"Task type: {task_type} (auto-detected)")
            else:
                # User explicitly selected task type
                task_type = task_type_setting
                self._log_progress(f"Task type: {task_type} (user-selected)")

            # Collect preprocessing method selections
            preprocessing_methods = {
                'raw': self.use_raw.get(),
                'snv': self.use_snv.get(),
                'sg1': self.use_sg1.get(),
                'sg2': self.use_sg2.get(),
                'deriv_snv': self.use_deriv_snv.get()
            }

            # Collect subset analysis settings
            enable_variable_subsets = self.enable_variable_subsets.get()
            enable_region_subsets = self.enable_region_subsets.get()

            # DEBUG: Print what we're getting from the GUI
            print("\n" + "="*70)
            print("GUI DEBUG: Subset Analysis Settings")
            print("="*70)
            print(f"enable_variable_subsets checkbox value: {self.enable_variable_subsets.get()}")
            print(f"enable_region_subsets checkbox value: {self.enable_region_subsets.get()}")

            # Collect top-N variable counts
            variable_counts = []
            print(f"var_10 checkbox: {self.var_10.get()}")
            if self.var_10.get():
                variable_counts.append(10)
            print(f"var_20 checkbox: {self.var_20.get()}")
            if self.var_20.get():
                variable_counts.append(20)
            print(f"var_50 checkbox: {self.var_50.get()}")
            if self.var_50.get():
                variable_counts.append(50)
            print(f"var_100 checkbox: {self.var_100.get()}")
            if self.var_100.get():
                variable_counts.append(100)
            print(f"var_250 checkbox: {self.var_250.get()}")
            if self.var_250.get():
                variable_counts.append(250)
            print(f"var_500 checkbox: {self.var_500.get()}")
            if self.var_500.get():
                variable_counts.append(500)
            print(f"var_1000 checkbox: {self.var_1000.get()}")
            if self.var_1000.get():
                variable_counts.append(1000)

            print(f"\nCollected variable_counts: {variable_counts}")
            print(f"Final enable_variable_subsets: {enable_variable_subsets}")
            print(f"Final enable_region_subsets: {enable_region_subsets}")
            print(f"Final n_top_regions: {self.n_top_regions.get()}")
            print("="*70 + "\n")

            # Collect window sizes from checkboxes
            window_sizes = []
            if self.window_7.get():
                window_sizes.append(7)
            if self.window_11.get():
                window_sizes.append(11)
            if self.window_17.get():
                window_sizes.append(17)
            if self.window_19.get():
                window_sizes.append(19)

            # Add custom window sizes
            if self.window_custom.get().strip():
                try:
                    custom_windows = [int(x.strip()) for x in self.window_custom.get().split(',')]
                    window_sizes.extend(custom_windows)
                except ValueError:
                    self._log_progress(f"Warning: Invalid custom window size(s) ignored: {self.window_custom.get()}")

            # Default to window size 17 if none specified
            if not window_sizes:
                window_sizes = [17]

            # Collect n_estimators options
            n_estimators_list = []
            if self.n_estimators_50.get():
                n_estimators_list.append(50)
            if self.n_estimators_100.get():
                n_estimators_list.append(100)

            # Add custom n_estimators
            if self.n_estimators_custom.get().strip():
                try:
                    custom_n_est = [int(x.strip()) for x in self.n_estimators_custom.get().split(',')]
                    n_estimators_list.extend(custom_n_est)
                except ValueError:
                    self._log_progress(f"Warning: Invalid custom n_estimators ignored: {self.n_estimators_custom.get()}")

            # Default to 100 if none selected
            if not n_estimators_list:
                n_estimators_list = [100]

            # Collect learning rate options
            learning_rates = []
            if self.lr_005.get():
                learning_rates.append(0.05)
            if self.lr_01.get():
                learning_rates.append(0.1)
            if self.lr_02.get():
                learning_rates.append(0.2)
            if self.lr_03.get():
                learning_rates.append(0.3)

            # Default to [0.1, 0.2, 0.3] if none selected (0.3 is empirically optimal)
            if not learning_rates:
                learning_rates = [0.1, 0.2, 0.3]

            # Collect NeuralBoosted hidden_layer_size
            neuralboosted_hidden_sizes = []
            if self.neuralboosted_hidden_3.get():
                neuralboosted_hidden_sizes.append(3)
            if self.neuralboosted_hidden_5.get():
                neuralboosted_hidden_sizes.append(5)

            custom_hidden = self.neuralboosted_hidden_custom.get().strip()
            if custom_hidden:
                try:
                    custom_val = int(custom_hidden)
                    if custom_val > 0 and custom_val not in neuralboosted_hidden_sizes:
                        neuralboosted_hidden_sizes.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid NeuralBoosted hidden_layer_size '{custom_hidden}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom NeuralBoosted hidden_layer_size '{custom_hidden}', ignoring")

            # Default to [3, 5] if none selected
            if not neuralboosted_hidden_sizes:
                neuralboosted_hidden_sizes = [3, 5]

            # Collect NeuralBoosted activation functions
            neuralboosted_activations = []
            if self.neuralboosted_activation_tanh.get():
                neuralboosted_activations.append('tanh')
            if self.neuralboosted_activation_identity.get():
                neuralboosted_activations.append('identity')
            if self.neuralboosted_activation_relu.get():
                neuralboosted_activations.append('relu')
            if self.neuralboosted_activation_logistic.get():
                neuralboosted_activations.append('logistic')

            # Default to ['tanh', 'identity'] if none selected
            if not neuralboosted_activations:
                neuralboosted_activations = ['tanh', 'identity']

            # Collect Random Forest n_estimators (number of trees)
            rf_n_trees_list = []
            if self.rf_n_trees_100.get():
                rf_n_trees_list.append(100)
            if self.rf_n_trees_200.get():
                rf_n_trees_list.append(200)
            if self.rf_n_trees_500.get():
                rf_n_trees_list.append(500)

            # Add custom value if provided
            custom_trees = self.rf_n_trees_custom.get().strip()
            if custom_trees:
                try:
                    custom_val = int(custom_trees)
                    if custom_val > 0 and custom_val not in rf_n_trees_list:
                        rf_n_trees_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom RF n_trees value '{custom_trees}', ignoring")

            # Default to [100] if none selected
            if not rf_n_trees_list:
                rf_n_trees_list = [100]

            # Sort for consistent ordering
            rf_n_trees_list = sorted(rf_n_trees_list)

            # Collect Random Forest max_depth
            rf_max_depth_list = []

            # Collect from checkboxes
            if self.rf_max_depth_none.get():
                rf_max_depth_list.append(None)
            if self.rf_max_depth_30.get():
                rf_max_depth_list.append(30)

            # Add custom value if provided
            custom_depth = self.rf_max_depth_custom.get().strip()
            if custom_depth:
                if custom_depth.lower() == 'none':
                    if None not in rf_max_depth_list:
                        rf_max_depth_list.append(None)
                else:
                    try:
                        custom_val = int(custom_depth)
                        if custom_val > 0 and custom_val not in rf_max_depth_list:
                            rf_max_depth_list.append(custom_val)
                        elif custom_val <= 0:
                            print(f"WARNING: Invalid custom RF max_depth value '{custom_depth}' (must be > 0), ignoring")
                    except ValueError:
                        print(f"WARNING: Invalid custom RF max_depth value '{custom_depth}', ignoring")

            # Default to [None, 30] if none selected
            if not rf_max_depth_list:
                rf_max_depth_list = [None, 30]

            # Sort for consistent ordering (None sorts first)
            rf_max_depth_list = sorted(rf_max_depth_list, key=lambda x: (x is not None, x))

            # Collect Random Forest min_samples_split
            rf_min_samples_split_list = []
            if self.rf_min_samples_split_2.get():
                rf_min_samples_split_list.append(2)
            if self.rf_min_samples_split_5.get():
                rf_min_samples_split_list.append(5)
            if self.rf_min_samples_split_10.get():
                rf_min_samples_split_list.append(10)
            if self.rf_min_samples_split_20.get():
                rf_min_samples_split_list.append(20)

            custom_min_split = self.rf_min_samples_split_custom.get().strip()
            if custom_min_split:
                try:
                    custom_val = int(custom_min_split)
                    if custom_val >= 2 and custom_val not in rf_min_samples_split_list:
                        rf_min_samples_split_list.append(custom_val)
                    elif custom_val < 2:
                        print(f"WARNING: Invalid RF min_samples_split '{custom_min_split}' (must be >= 2), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid RF min_samples_split '{custom_min_split}', ignoring")

            if not rf_min_samples_split_list:
                rf_min_samples_split_list = None
            else:
                rf_min_samples_split_list = sorted(rf_min_samples_split_list)

            # Collect Random Forest min_samples_leaf
            rf_min_samples_leaf_list = []
            if self.rf_min_samples_leaf_1.get():
                rf_min_samples_leaf_list.append(1)
            if self.rf_min_samples_leaf_2.get():
                rf_min_samples_leaf_list.append(2)
            if self.rf_min_samples_leaf_5.get():
                rf_min_samples_leaf_list.append(5)
            if self.rf_min_samples_leaf_10.get():
                rf_min_samples_leaf_list.append(10)

            custom_min_leaf = self.rf_min_samples_leaf_custom.get().strip()
            if custom_min_leaf:
                try:
                    custom_val = int(custom_min_leaf)
                    if custom_val >= 1 and custom_val not in rf_min_samples_leaf_list:
                        rf_min_samples_leaf_list.append(custom_val)
                    elif custom_val < 1:
                        print(f"WARNING: Invalid RF min_samples_leaf '{custom_min_leaf}' (must be >= 1), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid RF min_samples_leaf '{custom_min_leaf}', ignoring")

            if not rf_min_samples_leaf_list:
                rf_min_samples_leaf_list = None
            else:
                rf_min_samples_leaf_list = sorted(rf_min_samples_leaf_list)

            # Collect Random Forest max_features
            rf_max_features_list = []
            if self.rf_max_features_sqrt.get():
                rf_max_features_list.append('sqrt')
            if self.rf_max_features_log2.get():
                rf_max_features_list.append('log2')
            if self.rf_max_features_none.get():
                rf_max_features_list.append(None)

            custom_max_feat = self.rf_max_features_custom.get().strip()
            if custom_max_feat:
                if custom_max_feat.lower() == 'none':
                    if None not in rf_max_features_list:
                        rf_max_features_list.append(None)
                elif custom_max_feat.lower() in ['sqrt', 'log2']:
                    if custom_max_feat.lower() not in rf_max_features_list:
                        rf_max_features_list.append(custom_max_feat.lower())
                else:
                    try:
                        custom_val = float(custom_max_feat)
                        if custom_val not in rf_max_features_list:
                            rf_max_features_list.append(custom_val)
                    except ValueError:
                        print(f"WARNING: Invalid RF max_features '{custom_max_feat}', ignoring")

            if not rf_max_features_list:
                rf_max_features_list = None

            # Collect Random Forest bootstrap
            rf_bootstrap_list = []
            if self.rf_bootstrap_true.get():
                rf_bootstrap_list.append(True)
            if self.rf_bootstrap_false.get():
                rf_bootstrap_list.append(False)

            if not rf_bootstrap_list:
                rf_bootstrap_list = None

            # Collect Random Forest max_leaf_nodes
            rf_max_leaf_nodes_list = []
            if self.rf_max_leaf_nodes_none.get():
                rf_max_leaf_nodes_list.append(None)
            if self.rf_max_leaf_nodes_50.get():
                rf_max_leaf_nodes_list.append(50)
            if self.rf_max_leaf_nodes_100.get():
                rf_max_leaf_nodes_list.append(100)

            custom_max_leaf = self.rf_max_leaf_nodes_custom.get().strip()
            if custom_max_leaf:
                if custom_max_leaf.lower() == 'none':
                    if None not in rf_max_leaf_nodes_list:
                        rf_max_leaf_nodes_list.append(None)
                else:
                    try:
                        custom_val = int(custom_max_leaf)
                        if custom_val > 0 and custom_val not in rf_max_leaf_nodes_list:
                            rf_max_leaf_nodes_list.append(custom_val)
                        elif custom_val <= 0:
                            print(f"WARNING: Invalid RF max_leaf_nodes '{custom_max_leaf}' (must be > 0), ignoring")
                    except ValueError:
                        print(f"WARNING: Invalid RF max_leaf_nodes '{custom_max_leaf}', ignoring")

            if not rf_max_leaf_nodes_list:
                rf_max_leaf_nodes_list = None
            else:
                rf_max_leaf_nodes_list = sorted(rf_max_leaf_nodes_list, key=lambda x: (x is not None, x))

            # Collect Random Forest min_impurity_decrease
            rf_min_impurity_decrease_list = []
            if self.rf_min_impurity_decrease_0.get():
                rf_min_impurity_decrease_list.append(0.0)
            if self.rf_min_impurity_decrease_001.get():
                rf_min_impurity_decrease_list.append(0.01)
            if self.rf_min_impurity_decrease_01.get():
                rf_min_impurity_decrease_list.append(0.1)

            custom_min_impurity = self.rf_min_impurity_decrease_custom.get().strip()
            if custom_min_impurity:
                try:
                    custom_val = float(custom_min_impurity)
                    if custom_val >= 0.0 and custom_val not in rf_min_impurity_decrease_list:
                        rf_min_impurity_decrease_list.append(custom_val)
                    elif custom_val < 0.0:
                        print(f"WARNING: Invalid RF min_impurity_decrease '{custom_min_impurity}' (must be >= 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid RF min_impurity_decrease '{custom_min_impurity}', ignoring")

            if not rf_min_impurity_decrease_list:
                rf_min_impurity_decrease_list = None
            else:
                rf_min_impurity_decrease_list = sorted(rf_min_impurity_decrease_list)

            # Collect Ridge alpha values
            ridge_alphas_list = []
            if self.ridge_alpha_0001.get():
                ridge_alphas_list.append(0.001)
            if self.ridge_alpha_001.get():
                ridge_alphas_list.append(0.01)
            if self.ridge_alpha_01.get():
                ridge_alphas_list.append(0.1)
            if self.ridge_alpha_1.get():
                ridge_alphas_list.append(1.0)
            if self.ridge_alpha_10.get():
                ridge_alphas_list.append(10.0)

            # Add custom alpha if provided
            custom_ridge_alpha = self.ridge_alpha_custom.get().strip()
            if custom_ridge_alpha:
                try:
                    custom_val = float(custom_ridge_alpha)
                    if custom_val > 0 and custom_val not in ridge_alphas_list:
                        ridge_alphas_list.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid custom Ridge alpha '{custom_ridge_alpha}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom Ridge alpha '{custom_ridge_alpha}', ignoring")

            # Default to [0.001, 0.01, 0.1, 1.0, 10.0] if none selected
            if not ridge_alphas_list:
                ridge_alphas_list = [0.001, 0.01, 0.1, 1.0, 10.0]

            # Sort for consistent ordering
            ridge_alphas_list = sorted(ridge_alphas_list)

            # Collect Ridge solver values
            ridge_solver_list = []
            if self.ridge_solver_auto.get():
                ridge_solver_list.append('auto')
            if self.ridge_solver_svd.get():
                ridge_solver_list.append('svd')
            if self.ridge_solver_cholesky.get():
                ridge_solver_list.append('cholesky')
            if self.ridge_solver_lsqr.get():
                ridge_solver_list.append('lsqr')
            if self.ridge_solver_sag.get():
                ridge_solver_list.append('sag')

            if not ridge_solver_list:
                ridge_solver_list = None

            # Collect Ridge tol values
            ridge_tol_list = []
            if self.ridge_tol_1e4.get():
                ridge_tol_list.append(1e-4)
            if self.ridge_tol_1e3.get():
                ridge_tol_list.append(1e-3)
            if self.ridge_tol_1e5.get():
                ridge_tol_list.append(1e-5)

            custom_ridge_tol = self.ridge_tol_custom.get().strip()
            if custom_ridge_tol:
                try:
                    custom_val = float(custom_ridge_tol)
                    if custom_val > 0 and custom_val not in ridge_tol_list:
                        ridge_tol_list.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid Ridge tol '{custom_ridge_tol}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid Ridge tol '{custom_ridge_tol}', ignoring")

            if not ridge_tol_list:
                ridge_tol_list = None
            else:
                ridge_tol_list = sorted(ridge_tol_list)

            # Collect Lasso alpha values
            lasso_alphas_list = []
            if self.lasso_alpha_0001.get():
                lasso_alphas_list.append(0.001)
            if self.lasso_alpha_001.get():
                lasso_alphas_list.append(0.01)
            if self.lasso_alpha_01.get():
                lasso_alphas_list.append(0.1)
            if self.lasso_alpha_1.get():
                lasso_alphas_list.append(1.0)

            # Add custom alpha if provided
            custom_lasso_alpha = self.lasso_alpha_custom.get().strip()
            if custom_lasso_alpha:
                try:
                    custom_val = float(custom_lasso_alpha)
                    if custom_val > 0 and custom_val not in lasso_alphas_list:
                        lasso_alphas_list.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid custom Lasso alpha '{custom_lasso_alpha}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom Lasso alpha '{custom_lasso_alpha}', ignoring")

            # Default to [0.001, 0.01, 0.1, 1.0] if none selected
            if not lasso_alphas_list:
                lasso_alphas_list = [0.001, 0.01, 0.1, 1.0]

            # Sort for consistent ordering
            lasso_alphas_list = sorted(lasso_alphas_list)

            # Collect Lasso selection values
            lasso_selection_list = []
            if self.lasso_selection_cyclic.get():
                lasso_selection_list.append('cyclic')
            if self.lasso_selection_random.get():
                lasso_selection_list.append('random')

            if not lasso_selection_list:
                lasso_selection_list = None

            # Collect Lasso tol values
            lasso_tol_list = []
            if self.lasso_tol_1e4.get():
                lasso_tol_list.append(1e-4)
            if self.lasso_tol_1e3.get():
                lasso_tol_list.append(1e-3)
            if self.lasso_tol_1e5.get():
                lasso_tol_list.append(1e-5)

            custom_lasso_tol = self.lasso_tol_custom.get().strip()
            if custom_lasso_tol:
                try:
                    custom_val = float(custom_lasso_tol)
                    if custom_val > 0 and custom_val not in lasso_tol_list:
                        lasso_tol_list.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid Lasso tol '{custom_lasso_tol}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid Lasso tol '{custom_lasso_tol}', ignoring")

            if not lasso_tol_list:
                lasso_tol_list = None
            else:
                lasso_tol_list = sorted(lasso_tol_list)

            # Collect ElasticNet alpha values
            elasticnet_alphas_list = []
            if self.elasticnet_alpha_001.get():
                elasticnet_alphas_list.append(0.01)
            if self.elasticnet_alpha_01.get():
                elasticnet_alphas_list.append(0.1)
            if self.elasticnet_alpha_10.get():
                elasticnet_alphas_list.append(1.0)

            # Add custom alpha if provided
            custom_elasticnet_alpha = self.elasticnet_alpha_custom.get().strip()
            if custom_elasticnet_alpha:
                try:
                    custom_val = float(custom_elasticnet_alpha)
                    if custom_val > 0 and custom_val not in elasticnet_alphas_list:
                        elasticnet_alphas_list.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid custom ElasticNet alpha '{custom_elasticnet_alpha}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom ElasticNet alpha '{custom_elasticnet_alpha}', ignoring")

            # Default to [0.01, 0.1, 1.0] if none selected
            if not elasticnet_alphas_list:
                elasticnet_alphas_list = [0.01, 0.1, 1.0]

            # Sort for consistent ordering
            elasticnet_alphas_list = sorted(elasticnet_alphas_list)

            # Collect ElasticNet l1_ratio values
            elasticnet_l1_ratios_list = []
            if self.elasticnet_l1_ratio_03.get():
                elasticnet_l1_ratios_list.append(0.3)
            if self.elasticnet_l1_ratio_05.get():
                elasticnet_l1_ratios_list.append(0.5)
            if self.elasticnet_l1_ratio_07.get():
                elasticnet_l1_ratios_list.append(0.7)

            # Add custom l1_ratio if provided
            custom_elasticnet_l1 = self.elasticnet_l1_ratio_custom.get().strip()
            if custom_elasticnet_l1:
                try:
                    custom_val = float(custom_elasticnet_l1)
                    if 0 < custom_val <= 1 and custom_val not in elasticnet_l1_ratios_list:
                        elasticnet_l1_ratios_list.append(custom_val)
                    elif custom_val <= 0 or custom_val > 1:
                        print(f"WARNING: Invalid custom ElasticNet l1_ratio '{custom_elasticnet_l1}' (must be 0 < l1_ratio <= 1), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom ElasticNet l1_ratio '{custom_elasticnet_l1}', ignoring")

            # Default to [0.3, 0.5, 0.7] if none selected
            if not elasticnet_l1_ratios_list:
                elasticnet_l1_ratios_list = [0.3, 0.5, 0.7]

            # Sort for consistent ordering
            elasticnet_l1_ratios_list = sorted(elasticnet_l1_ratios_list)

            # Collect ElasticNet selection values
            elasticnet_selection_list = []
            if self.elasticnet_selection_cyclic.get():
                elasticnet_selection_list.append('cyclic')
            if self.elasticnet_selection_random.get():
                elasticnet_selection_list.append('random')

            if not elasticnet_selection_list:
                elasticnet_selection_list = None

            # Collect ElasticNet tol values
            elasticnet_tol_list = []
            if self.elasticnet_tol_1e4.get():
                elasticnet_tol_list.append(1e-4)
            if self.elasticnet_tol_1e3.get():
                elasticnet_tol_list.append(1e-3)
            if self.elasticnet_tol_1e5.get():
                elasticnet_tol_list.append(1e-5)

            custom_elasticnet_tol = self.elasticnet_tol_custom.get().strip()
            if custom_elasticnet_tol:
                try:
                    custom_val = float(custom_elasticnet_tol)
                    if custom_val > 0 and custom_val not in elasticnet_tol_list:
                        elasticnet_tol_list.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid ElasticNet tol '{custom_elasticnet_tol}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid ElasticNet tol '{custom_elasticnet_tol}', ignoring")

            if not elasticnet_tol_list:
                elasticnet_tol_list = None
            else:
                elasticnet_tol_list = sorted(elasticnet_tol_list)

            # Collect PLS max_iter values
            pls_max_iters_list = []
            if self.pls_max_iter_500.get():
                pls_max_iters_list.append(500)
            if self.pls_max_iter_1000.get():
                pls_max_iters_list.append(1000)

            # Add custom max_iter if provided
            custom_pls_maxiter = self.pls_max_iter_custom.get().strip()
            if custom_pls_maxiter:
                try:
                    custom_val = int(custom_pls_maxiter)
                    if custom_val > 0 and custom_val not in pls_max_iters_list:
                        pls_max_iters_list.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid custom PLS max_iter '{custom_pls_maxiter}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom PLS max_iter '{custom_pls_maxiter}', ignoring")

            # Default to [500] if none selected
            if not pls_max_iters_list:
                pls_max_iters_list = [500]

            # Sort for consistent ordering
            pls_max_iters_list = sorted(pls_max_iters_list)

            # Collect PLS tol values
            pls_tols_list = []
            if self.pls_tol_1e7.get():
                pls_tols_list.append(1e-7)
            if self.pls_tol_1e6.get():
                pls_tols_list.append(1e-6)
            if self.pls_tol_1e5.get():
                pls_tols_list.append(1e-5)

            # Add custom tol if provided
            custom_pls_tol = self.pls_tol_custom.get().strip()
            if custom_pls_tol:
                try:
                    custom_val = float(custom_pls_tol)
                    if custom_val > 0 and custom_val not in pls_tols_list:
                        pls_tols_list.append(custom_val)
                    elif custom_val <= 0:
                        print(f"WARNING: Invalid custom PLS tol '{custom_pls_tol}' (must be > 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom PLS tol '{custom_pls_tol}', ignoring")

            # Default to [1e-6] if none selected
            if not pls_tols_list:
                pls_tols_list = [1e-6]

            # Sort for consistent ordering
            pls_tols_list = sorted(pls_tols_list)

            # Collect XGBoost hyperparameters
            # n_estimators
            xgb_n_estimators_list = []
            if self.xgb_n_estimators_100.get():
                xgb_n_estimators_list.append(100)
            if self.xgb_n_estimators_200.get():
                xgb_n_estimators_list.append(200)

            custom_xgb_n_est = self.xgb_n_estimators_custom.get().strip()
            if custom_xgb_n_est:
                try:
                    custom_val = int(custom_xgb_n_est)
                    if custom_val > 0 and custom_val not in xgb_n_estimators_list:
                        xgb_n_estimators_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost n_estimators '{custom_xgb_n_est}', ignoring")

            if not xgb_n_estimators_list:
                xgb_n_estimators_list = [100, 200]  # Standard tier defaults
            xgb_n_estimators_list = sorted(xgb_n_estimators_list)

            # learning_rate
            xgb_learning_rates = []
            if self.xgb_lr_005.get():
                xgb_learning_rates.append(0.05)
            if self.xgb_lr_01.get():
                xgb_learning_rates.append(0.1)

            custom_xgb_lr = self.xgb_lr_custom.get().strip()
            if custom_xgb_lr:
                try:
                    custom_val = float(custom_xgb_lr)
                    if 0 < custom_val <= 1.0 and custom_val not in xgb_learning_rates:
                        xgb_learning_rates.append(custom_val)
                    elif custom_val <= 0 or custom_val > 1.0:
                        print(f"WARNING: Invalid XGBoost learning_rate '{custom_xgb_lr}' (must be 0-1), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost learning_rate '{custom_xgb_lr}', ignoring")

            if not xgb_learning_rates:
                xgb_learning_rates = [0.05, 0.1]  # Standard tier defaults
            xgb_learning_rates = sorted(xgb_learning_rates)

            # max_depth
            xgb_max_depths = []
            if self.xgb_max_depth_3.get():
                xgb_max_depths.append(3)
            if self.xgb_max_depth_6.get():
                xgb_max_depths.append(6)
            if self.xgb_max_depth_9.get():
                xgb_max_depths.append(9)

            custom_xgb_depth = self.xgb_max_depth_custom.get().strip()
            if custom_xgb_depth:
                try:
                    custom_val = int(custom_xgb_depth)
                    if custom_val > 0 and custom_val not in xgb_max_depths:
                        xgb_max_depths.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost max_depth '{custom_xgb_depth}', ignoring")

            if not xgb_max_depths:
                xgb_max_depths = [3, 6]  # Standard tier defaults
            xgb_max_depths = sorted(xgb_max_depths)

            # subsample
            xgb_subsample = []
            if self.xgb_subsample_08.get():
                xgb_subsample.append(0.8)
            if self.xgb_subsample_10.get():
                xgb_subsample.append(1.0)

            custom_xgb_subsample = self.xgb_subsample_custom.get().strip()
            if custom_xgb_subsample:
                try:
                    custom_val = float(custom_xgb_subsample)
                    if 0 < custom_val <= 1.0 and custom_val not in xgb_subsample:
                        xgb_subsample.append(custom_val)
                    elif custom_val <= 0 or custom_val > 1.0:
                        print(f"WARNING: Invalid XGBoost subsample '{custom_xgb_subsample}' (must be 0-1), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost subsample '{custom_xgb_subsample}', ignoring")

            if not xgb_subsample:
                xgb_subsample = [0.8, 1.0]  # Standard tier defaults
            xgb_subsample = sorted(xgb_subsample)

            # colsample_bytree
            xgb_colsample_bytree = []
            if self.xgb_colsample_08.get():
                xgb_colsample_bytree.append(0.8)
            if self.xgb_colsample_10.get():
                xgb_colsample_bytree.append(1.0)

            custom_xgb_colsample = self.xgb_colsample_custom.get().strip()
            if custom_xgb_colsample:
                try:
                    custom_val = float(custom_xgb_colsample)
                    if 0 < custom_val <= 1.0 and custom_val not in xgb_colsample_bytree:
                        xgb_colsample_bytree.append(custom_val)
                    elif custom_val <= 0 or custom_val > 1.0:
                        print(f"WARNING: Invalid XGBoost colsample_bytree '{custom_xgb_colsample}' (must be 0-1), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost colsample_bytree '{custom_xgb_colsample}', ignoring")

            if not xgb_colsample_bytree:
                xgb_colsample_bytree = [0.8, 1.0]  # Standard tier defaults
            xgb_colsample_bytree = sorted(xgb_colsample_bytree)

            # reg_alpha (L1 regularization)
            xgb_reg_alpha = []
            if self.xgb_reg_alpha_0.get():
                xgb_reg_alpha.append(0.0)
            if self.xgb_reg_alpha_01.get():
                xgb_reg_alpha.append(0.1)
            if self.xgb_reg_alpha_05.get():
                xgb_reg_alpha.append(0.5)

            custom_xgb_reg_alpha = self.xgb_reg_alpha_custom.get().strip()
            if custom_xgb_reg_alpha:
                try:
                    custom_val = float(custom_xgb_reg_alpha)
                    if custom_val >= 0 and custom_val not in xgb_reg_alpha:
                        xgb_reg_alpha.append(custom_val)
                    elif custom_val < 0:
                        print(f"WARNING: Invalid XGBoost reg_alpha '{custom_xgb_reg_alpha}' (must be >= 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost reg_alpha '{custom_xgb_reg_alpha}', ignoring")

            if not xgb_reg_alpha:
                xgb_reg_alpha = [0.0, 0.1]  # Standard tier defaults
            xgb_reg_alpha = sorted(xgb_reg_alpha)

            # reg_lambda (L2 regularization) - comprehensive tier
            xgb_reg_lambda = []
            if self.xgb_reg_lambda_10.get():
                xgb_reg_lambda.append(1.0)
            if self.xgb_reg_lambda_50.get():
                xgb_reg_lambda.append(5.0)

            custom_xgb_reg_lambda = self.xgb_reg_lambda_custom.get().strip()
            if custom_xgb_reg_lambda:
                try:
                    custom_val = float(custom_xgb_reg_lambda)
                    if custom_val >= 0 and custom_val not in xgb_reg_lambda:
                        xgb_reg_lambda.append(custom_val)
                    elif custom_val < 0:
                        print(f"WARNING: Invalid XGBoost reg_lambda '{custom_xgb_reg_lambda}' (must be >= 0), ignoring")
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost reg_lambda '{custom_xgb_reg_lambda}', ignoring")

            # Only pass reg_lambda if user explicitly selected values (comprehensive tier feature)
            # Default: None (will use tier defaults in models.py)
            if not xgb_reg_lambda:
                xgb_reg_lambda = None
            else:
                xgb_reg_lambda = sorted(xgb_reg_lambda)

            # min_child_weight
            xgb_min_child_weight = []
            if self.xgb_min_child_weight_1.get():
                xgb_min_child_weight.append(1)
            if self.xgb_min_child_weight_3.get():
                xgb_min_child_weight.append(3)
            if self.xgb_min_child_weight_5.get():
                xgb_min_child_weight.append(5)
            if self.xgb_min_child_weight_7.get():
                xgb_min_child_weight.append(7)

            custom_xgb_min_child_weight = self.xgb_min_child_weight_custom.get().strip()
            if custom_xgb_min_child_weight:
                try:
                    custom_val = int(custom_xgb_min_child_weight)
                    if custom_val > 0 and custom_val not in xgb_min_child_weight:
                        xgb_min_child_weight.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost min_child_weight '{custom_xgb_min_child_weight}', ignoring")

            if not xgb_min_child_weight:
                xgb_min_child_weight = None
            else:
                xgb_min_child_weight = sorted(xgb_min_child_weight)

            # gamma
            xgb_gamma = []
            if self.xgb_gamma_0.get():
                xgb_gamma.append(0.0)
            if self.xgb_gamma_01.get():
                xgb_gamma.append(0.1)
            if self.xgb_gamma_03.get():
                xgb_gamma.append(0.3)
            if self.xgb_gamma_05.get():
                xgb_gamma.append(0.5)
            if self.xgb_gamma_10.get():
                xgb_gamma.append(1.0)

            custom_xgb_gamma = self.xgb_gamma_custom.get().strip()
            if custom_xgb_gamma:
                try:
                    custom_val = float(custom_xgb_gamma)
                    if custom_val >= 0 and custom_val not in xgb_gamma:
                        xgb_gamma.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom XGBoost gamma '{custom_xgb_gamma}', ignoring")

            if not xgb_gamma:
                xgb_gamma = None
            else:
                xgb_gamma = sorted(xgb_gamma)

            # Collect LightGBM hyperparameters (Sprint 1)
            # n_estimators
            lightgbm_n_estimators_list = []
            if self.lightgbm_n_estimators_50.get():
                lightgbm_n_estimators_list.append(50)
            if self.lightgbm_n_estimators_100.get():
                lightgbm_n_estimators_list.append(100)
            if self.lightgbm_n_estimators_200.get():
                lightgbm_n_estimators_list.append(200)

            custom_lgbm_n_estimators = self.lightgbm_n_estimators_custom.get().strip()
            if custom_lgbm_n_estimators:
                try:
                    custom_val = int(custom_lgbm_n_estimators)
                    if custom_val > 0 and custom_val not in lightgbm_n_estimators_list:
                        lightgbm_n_estimators_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM n_estimators '{custom_lgbm_n_estimators}', ignoring")

            if not lightgbm_n_estimators_list:
                lightgbm_n_estimators_list = None
            else:
                lightgbm_n_estimators_list = sorted(lightgbm_n_estimators_list)

            # learning_rate
            lightgbm_learning_rates = []
            if self.lightgbm_lr_005.get():
                lightgbm_learning_rates.append(0.05)
            if self.lightgbm_lr_01.get():
                lightgbm_learning_rates.append(0.1)
            if self.lightgbm_lr_02.get():
                lightgbm_learning_rates.append(0.2)

            custom_lgbm_lr = self.lightgbm_lr_custom.get().strip()
            if custom_lgbm_lr:
                try:
                    custom_val = float(custom_lgbm_lr)
                    if 0 < custom_val <= 1.0 and custom_val not in lightgbm_learning_rates:
                        lightgbm_learning_rates.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM learning_rate '{custom_lgbm_lr}', ignoring")

            if not lightgbm_learning_rates:
                lightgbm_learning_rates = None
            else:
                lightgbm_learning_rates = sorted(lightgbm_learning_rates)

            # num_leaves
            lightgbm_num_leaves_list = []
            if self.lightgbm_num_leaves_31.get():
                lightgbm_num_leaves_list.append(31)
            if self.lightgbm_num_leaves_50.get():
                lightgbm_num_leaves_list.append(50)
            if self.lightgbm_num_leaves_70.get():
                lightgbm_num_leaves_list.append(70)

            custom_lgbm_num_leaves = self.lightgbm_num_leaves_custom.get().strip()
            if custom_lgbm_num_leaves:
                try:
                    custom_val = int(custom_lgbm_num_leaves)
                    if custom_val > 1 and custom_val not in lightgbm_num_leaves_list:
                        lightgbm_num_leaves_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM num_leaves '{custom_lgbm_num_leaves}', ignoring")

            if not lightgbm_num_leaves_list:
                lightgbm_num_leaves_list = None
            else:
                lightgbm_num_leaves_list = sorted(lightgbm_num_leaves_list)

            # max_depth
            lightgbm_max_depth_list = []
            if self.lightgbm_max_depth_m1.get():
                lightgbm_max_depth_list.append(-1)
            if self.lightgbm_max_depth_5.get():
                lightgbm_max_depth_list.append(5)
            if self.lightgbm_max_depth_10.get():
                lightgbm_max_depth_list.append(10)
            if self.lightgbm_max_depth_20.get():
                lightgbm_max_depth_list.append(20)
            if self.lightgbm_max_depth_50.get():
                lightgbm_max_depth_list.append(50)

            custom_lgbm_max_depth = self.lightgbm_max_depth_custom.get().strip()
            if custom_lgbm_max_depth:
                try:
                    custom_val = int(custom_lgbm_max_depth)
                    if custom_val not in lightgbm_max_depth_list:
                        lightgbm_max_depth_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM max_depth '{custom_lgbm_max_depth}', ignoring")

            if not lightgbm_max_depth_list:
                lightgbm_max_depth_list = None
            else:
                lightgbm_max_depth_list = sorted(lightgbm_max_depth_list)

            # min_child_samples
            lightgbm_min_child_samples_list = []
            if self.lightgbm_min_child_samples_5.get():
                lightgbm_min_child_samples_list.append(5)
            if self.lightgbm_min_child_samples_10.get():
                lightgbm_min_child_samples_list.append(10)
            if self.lightgbm_min_child_samples_20.get():
                lightgbm_min_child_samples_list.append(20)
            if self.lightgbm_min_child_samples_50.get():
                lightgbm_min_child_samples_list.append(50)
            if self.lightgbm_min_child_samples_100.get():
                lightgbm_min_child_samples_list.append(100)

            custom_lgbm_min_child_samples = self.lightgbm_min_child_samples_custom.get().strip()
            if custom_lgbm_min_child_samples:
                try:
                    custom_val = int(custom_lgbm_min_child_samples)
                    if custom_val > 0 and custom_val not in lightgbm_min_child_samples_list:
                        lightgbm_min_child_samples_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM min_child_samples '{custom_lgbm_min_child_samples}', ignoring")

            if not lightgbm_min_child_samples_list:
                lightgbm_min_child_samples_list = None
            else:
                lightgbm_min_child_samples_list = sorted(lightgbm_min_child_samples_list)

            # subsample
            lightgbm_subsample_list = []
            if self.lightgbm_subsample_05.get():
                lightgbm_subsample_list.append(0.5)
            if self.lightgbm_subsample_07.get():
                lightgbm_subsample_list.append(0.7)
            if self.lightgbm_subsample_08.get():
                lightgbm_subsample_list.append(0.8)
            if self.lightgbm_subsample_085.get():
                lightgbm_subsample_list.append(0.85)
            if self.lightgbm_subsample_10.get():
                lightgbm_subsample_list.append(1.0)

            custom_lgbm_subsample = self.lightgbm_subsample_custom.get().strip()
            if custom_lgbm_subsample:
                try:
                    custom_val = float(custom_lgbm_subsample)
                    if 0 < custom_val <= 1.0 and custom_val not in lightgbm_subsample_list:
                        lightgbm_subsample_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM subsample '{custom_lgbm_subsample}', ignoring")

            if not lightgbm_subsample_list:
                lightgbm_subsample_list = None
            else:
                lightgbm_subsample_list = sorted(lightgbm_subsample_list)

            # colsample_bytree
            lightgbm_colsample_bytree_list = []
            if self.lightgbm_colsample_bytree_05.get():
                lightgbm_colsample_bytree_list.append(0.5)
            if self.lightgbm_colsample_bytree_07.get():
                lightgbm_colsample_bytree_list.append(0.7)
            if self.lightgbm_colsample_bytree_08.get():
                lightgbm_colsample_bytree_list.append(0.8)
            if self.lightgbm_colsample_bytree_085.get():
                lightgbm_colsample_bytree_list.append(0.85)
            if self.lightgbm_colsample_bytree_10.get():
                lightgbm_colsample_bytree_list.append(1.0)

            custom_lgbm_colsample = self.lightgbm_colsample_bytree_custom.get().strip()
            if custom_lgbm_colsample:
                try:
                    custom_val = float(custom_lgbm_colsample)
                    if 0 < custom_val <= 1.0 and custom_val not in lightgbm_colsample_bytree_list:
                        lightgbm_colsample_bytree_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM colsample_bytree '{custom_lgbm_colsample}', ignoring")

            if not lightgbm_colsample_bytree_list:
                lightgbm_colsample_bytree_list = None
            else:
                lightgbm_colsample_bytree_list = sorted(lightgbm_colsample_bytree_list)

            # reg_alpha
            lightgbm_reg_alpha_list = []
            if self.lightgbm_reg_alpha_00.get():
                lightgbm_reg_alpha_list.append(0.0)
            if self.lightgbm_reg_alpha_01.get():
                lightgbm_reg_alpha_list.append(0.1)
            if self.lightgbm_reg_alpha_05.get():
                lightgbm_reg_alpha_list.append(0.5)
            if self.lightgbm_reg_alpha_10.get():
                lightgbm_reg_alpha_list.append(1.0)

            custom_lgbm_reg_alpha = self.lightgbm_reg_alpha_custom.get().strip()
            if custom_lgbm_reg_alpha:
                try:
                    custom_val = float(custom_lgbm_reg_alpha)
                    if custom_val >= 0 and custom_val not in lightgbm_reg_alpha_list:
                        lightgbm_reg_alpha_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM reg_alpha '{custom_lgbm_reg_alpha}', ignoring")

            if not lightgbm_reg_alpha_list:
                lightgbm_reg_alpha_list = None
            else:
                lightgbm_reg_alpha_list = sorted(lightgbm_reg_alpha_list)

            # reg_lambda
            lightgbm_reg_lambda_list = []
            if self.lightgbm_reg_lambda_00.get():
                lightgbm_reg_lambda_list.append(0.0)
            if self.lightgbm_reg_lambda_05.get():
                lightgbm_reg_lambda_list.append(0.5)
            if self.lightgbm_reg_lambda_10.get():
                lightgbm_reg_lambda_list.append(1.0)
            if self.lightgbm_reg_lambda_20.get():
                lightgbm_reg_lambda_list.append(2.0)

            custom_lgbm_reg_lambda = self.lightgbm_reg_lambda_custom.get().strip()
            if custom_lgbm_reg_lambda:
                try:
                    custom_val = float(custom_lgbm_reg_lambda)
                    if custom_val >= 0 and custom_val not in lightgbm_reg_lambda_list:
                        lightgbm_reg_lambda_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom LightGBM reg_lambda '{custom_lgbm_reg_lambda}', ignoring")

            if not lightgbm_reg_lambda_list:
                lightgbm_reg_lambda_list = None
            else:
                lightgbm_reg_lambda_list = sorted(lightgbm_reg_lambda_list)

            # Collect CatBoost hyperparameters (NEW PHASE 1)
            # iterations (n_estimators equivalent)
            catboost_iterations_list = []
            if self.catboost_iterations_100.get():
                catboost_iterations_list.append(100)
            if self.catboost_iterations_200.get():
                catboost_iterations_list.append(200)

            custom_cb_iterations = self.catboost_iterations_custom.get().strip()
            if custom_cb_iterations:
                try:
                    custom_val = int(custom_cb_iterations)
                    if custom_val > 0 and custom_val not in catboost_iterations_list:
                        catboost_iterations_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom CatBoost iterations '{custom_cb_iterations}', ignoring")

            if not catboost_iterations_list:
                catboost_iterations_list = None
            else:
                catboost_iterations_list = sorted(catboost_iterations_list)

            # learning_rate
            catboost_learning_rates = []
            if self.catboost_lr_005.get():
                catboost_learning_rates.append(0.05)
            if self.catboost_lr_01.get():
                catboost_learning_rates.append(0.1)

            custom_cb_lr = self.catboost_lr_custom.get().strip()
            if custom_cb_lr:
                try:
                    custom_val = float(custom_cb_lr)
                    if 0 < custom_val <= 1.0 and custom_val not in catboost_learning_rates:
                        catboost_learning_rates.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom CatBoost learning_rate '{custom_cb_lr}', ignoring")

            if not catboost_learning_rates:
                catboost_learning_rates = None
            else:
                catboost_learning_rates = sorted(catboost_learning_rates)

            # depth (max tree depth)
            catboost_depths = []
            if self.catboost_depth_4.get():
                catboost_depths.append(4)
            if self.catboost_depth_6.get():
                catboost_depths.append(6)

            custom_cb_depth = self.catboost_depth_custom.get().strip()
            if custom_cb_depth:
                try:
                    custom_val = int(custom_cb_depth)
                    if custom_val > 0 and custom_val not in catboost_depths:
                        catboost_depths.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom CatBoost depth '{custom_cb_depth}', ignoring")

            if not catboost_depths:
                catboost_depths = None
            else:
                catboost_depths = sorted(catboost_depths)

            # l2_leaf_reg
            catboost_l2_leaf_reg_list = []
            if self.catboost_l2_leaf_reg_10.get():
                catboost_l2_leaf_reg_list.append(1.0)
            if self.catboost_l2_leaf_reg_30.get():
                catboost_l2_leaf_reg_list.append(3.0)
            if self.catboost_l2_leaf_reg_100.get():
                catboost_l2_leaf_reg_list.append(10.0)
            if self.catboost_l2_leaf_reg_300.get():
                catboost_l2_leaf_reg_list.append(30.0)

            custom_cb_l2_leaf_reg = self.catboost_l2_leaf_reg_custom.get().strip()
            if custom_cb_l2_leaf_reg:
                try:
                    custom_val = float(custom_cb_l2_leaf_reg)
                    if custom_val >= 0 and custom_val not in catboost_l2_leaf_reg_list:
                        catboost_l2_leaf_reg_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom CatBoost l2_leaf_reg '{custom_cb_l2_leaf_reg}', ignoring")

            if not catboost_l2_leaf_reg_list:
                catboost_l2_leaf_reg_list = None
            else:
                catboost_l2_leaf_reg_list = sorted(catboost_l2_leaf_reg_list)

            # border_count
            catboost_border_count_list = []
            if self.catboost_border_count_32.get():
                catboost_border_count_list.append(32)
            if self.catboost_border_count_64.get():
                catboost_border_count_list.append(64)
            if self.catboost_border_count_128.get():
                catboost_border_count_list.append(128)
            if self.catboost_border_count_254.get():
                catboost_border_count_list.append(254)

            custom_cb_border_count = self.catboost_border_count_custom.get().strip()
            if custom_cb_border_count:
                try:
                    custom_val = int(custom_cb_border_count)
                    if custom_val > 0 and custom_val not in catboost_border_count_list:
                        catboost_border_count_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom CatBoost border_count '{custom_cb_border_count}', ignoring")

            if not catboost_border_count_list:
                catboost_border_count_list = None
            else:
                catboost_border_count_list = sorted(catboost_border_count_list)

            # bagging_temperature
            catboost_bagging_temperature_list = []
            if self.catboost_bagging_temperature_00.get():
                catboost_bagging_temperature_list.append(0.0)
            if self.catboost_bagging_temperature_05.get():
                catboost_bagging_temperature_list.append(0.5)
            if self.catboost_bagging_temperature_10.get():
                catboost_bagging_temperature_list.append(1.0)
            if self.catboost_bagging_temperature_30.get():
                catboost_bagging_temperature_list.append(3.0)

            custom_cb_bagging_temp = self.catboost_bagging_temperature_custom.get().strip()
            if custom_cb_bagging_temp:
                try:
                    custom_val = float(custom_cb_bagging_temp)
                    if custom_val >= 0 and custom_val not in catboost_bagging_temperature_list:
                        catboost_bagging_temperature_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom CatBoost bagging_temperature '{custom_cb_bagging_temp}', ignoring")

            if not catboost_bagging_temperature_list:
                catboost_bagging_temperature_list = None
            else:
                catboost_bagging_temperature_list = sorted(catboost_bagging_temperature_list)

            # random_strength
            catboost_random_strength_list = []
            if self.catboost_random_strength_05.get():
                catboost_random_strength_list.append(0.5)
            if self.catboost_random_strength_10.get():
                catboost_random_strength_list.append(1.0)
            if self.catboost_random_strength_20.get():
                catboost_random_strength_list.append(2.0)
            if self.catboost_random_strength_50.get():
                catboost_random_strength_list.append(5.0)

            custom_cb_random_strength = self.catboost_random_strength_custom.get().strip()
            if custom_cb_random_strength:
                try:
                    custom_val = float(custom_cb_random_strength)
                    if custom_val >= 0 and custom_val not in catboost_random_strength_list:
                        catboost_random_strength_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom CatBoost random_strength '{custom_cb_random_strength}', ignoring")

            if not catboost_random_strength_list:
                catboost_random_strength_list = None
            else:
                catboost_random_strength_list = sorted(catboost_random_strength_list)

            # Collect SVR hyperparameters (NEW PHASE 1 - CRITICAL FIX)
            # kernel
            svr_kernels = []
            if self.svr_kernel_rbf.get():
                svr_kernels.append('rbf')
            if self.svr_kernel_linear.get():
                svr_kernels.append('linear')
            if not svr_kernels:
                svr_kernels = ['rbf', 'linear']  # Default

            # C (regularization parameter)
            svr_C_list = []
            if self.svr_C_10.get():
                svr_C_list.append(1.0)
            if self.svr_C_100.get():
                svr_C_list.append(10.0)
            custom_svr_C = self.svr_C_custom.get().strip()
            if custom_svr_C:
                try:
                    custom_val = float(custom_svr_C)
                    if custom_val > 0 and custom_val not in svr_C_list:
                        svr_C_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom SVR C '{custom_svr_C}', ignoring")
            if not svr_C_list:
                svr_C_list = [1.0, 10.0]  # Default

            # gamma (kernel coefficient)
            svr_gamma_list = []
            if self.svr_gamma_scale.get():
                svr_gamma_list.append('scale')
            if self.svr_gamma_auto.get():
                svr_gamma_list.append('auto')
            custom_svr_gamma = self.svr_gamma_custom.get().strip()
            if custom_svr_gamma:
                try:
                    custom_val = float(custom_svr_gamma)
                    if custom_val > 0 and custom_val not in svr_gamma_list:
                        svr_gamma_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom SVR gamma '{custom_svr_gamma}', ignoring")
            if not svr_gamma_list:
                svr_gamma_list = ['scale']  # Default

            # epsilon (tube width)
            svr_epsilon_list = []
            if self.svr_epsilon_001.get():
                svr_epsilon_list.append(0.01)
            if self.svr_epsilon_005.get():
                svr_epsilon_list.append(0.05)
            if self.svr_epsilon_01.get():
                svr_epsilon_list.append(0.1)
            if self.svr_epsilon_02.get():
                svr_epsilon_list.append(0.2)
            if self.svr_epsilon_05.get():
                svr_epsilon_list.append(0.5)
            if not svr_epsilon_list:
                svr_epsilon_list = [0.1]  # Default

            # degree (for poly kernel)
            svr_degree_list = []
            if self.svr_degree_2.get():
                svr_degree_list.append(2)
            if self.svr_degree_3.get():
                svr_degree_list.append(3)
            if self.svr_degree_4.get():
                svr_degree_list.append(4)
            if self.svr_degree_5.get():
                svr_degree_list.append(5)
            if not svr_degree_list:
                svr_degree_list = [3]  # Default

            # coef0 (independent term in kernel)
            svr_coef0_list = []
            if self.svr_coef0_00.get():
                svr_coef0_list.append(0.0)
            if self.svr_coef0_05.get():
                svr_coef0_list.append(0.5)
            if self.svr_coef0_10.get():
                svr_coef0_list.append(1.0)
            if self.svr_coef0_20.get():
                svr_coef0_list.append(2.0)
            if not svr_coef0_list:
                svr_coef0_list = [0.0]  # Default

            # shrinking (use shrinking heuristic)
            svr_shrinking_list = []
            if self.svr_shrinking_true.get():
                svr_shrinking_list.append(True)
            if self.svr_shrinking_false.get():
                svr_shrinking_list.append(False)
            if not svr_shrinking_list:
                svr_shrinking_list = [True]  # Default

            # Collect MLP hyperparameters (NEW PHASE 1 - CRITICAL FIX)
            # hidden_layer_sizes
            mlp_hidden_layer_sizes_list = []
            if self.mlp_hidden_64.get():
                mlp_hidden_layer_sizes_list.append((64,))
            if self.mlp_hidden_128_64.get():
                mlp_hidden_layer_sizes_list.append((128, 64))
            custom_mlp_hidden = self.mlp_hidden_custom.get().strip()
            if custom_mlp_hidden:
                try:
                    # Parse "100,50,25" → (100, 50, 25)
                    sizes = tuple(int(x.strip()) for x in custom_mlp_hidden.split(','))
                    if all(s > 0 for s in sizes) and sizes not in mlp_hidden_layer_sizes_list:
                        mlp_hidden_layer_sizes_list.append(sizes)
                except ValueError:
                    print(f"WARNING: Invalid custom MLP hidden_layer_sizes '{custom_mlp_hidden}', ignoring")
            if not mlp_hidden_layer_sizes_list:
                mlp_hidden_layer_sizes_list = [(64,), (128, 64)]  # Default

            # alpha (L2 regularization)
            mlp_alphas_list = []
            if self.mlp_alpha_1e3.get():
                mlp_alphas_list.append(0.001)
            custom_mlp_alpha = self.mlp_alpha_custom.get().strip()
            if custom_mlp_alpha:
                try:
                    custom_val = float(custom_mlp_alpha)
                    if custom_val > 0 and custom_val not in mlp_alphas_list:
                        mlp_alphas_list.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom MLP alpha '{custom_mlp_alpha}', ignoring")
            if not mlp_alphas_list:
                mlp_alphas_list = [0.001]  # Default

            # learning_rate_init
            mlp_learning_rate_inits = []
            if self.mlp_lr_init_1e3.get():
                mlp_learning_rate_inits.append(0.001)
            custom_mlp_lr_init = self.mlp_lr_init_custom.get().strip()
            if custom_mlp_lr_init:
                try:
                    custom_val = float(custom_mlp_lr_init)
                    if custom_val > 0 and custom_val not in mlp_learning_rate_inits:
                        mlp_learning_rate_inits.append(custom_val)
                except ValueError:
                    print(f"WARNING: Invalid custom MLP learning_rate_init '{custom_mlp_lr_init}', ignoring")
            if not mlp_learning_rate_inits:
                mlp_learning_rate_inits = [0.001]  # Default

            # activation
            mlp_activation_list = []
            if self.mlp_activation_relu.get():
                mlp_activation_list.append('relu')
            if self.mlp_activation_tanh.get():
                mlp_activation_list.append('tanh')
            if self.mlp_activation_logistic.get():
                mlp_activation_list.append('logistic')
            if self.mlp_activation_identity.get():
                mlp_activation_list.append('identity')
            if not mlp_activation_list:
                mlp_activation_list = ['relu']  # Default

            # solver
            mlp_solver_list = []
            if self.mlp_solver_adam.get():
                mlp_solver_list.append('adam')
            if self.mlp_solver_lbfgs.get():
                mlp_solver_list.append('lbfgs')
            if self.mlp_solver_sgd.get():
                mlp_solver_list.append('sgd')
            if not mlp_solver_list:
                mlp_solver_list = ['adam']  # Default

            # batch_size
            mlp_batch_size_list = []
            if self.mlp_batch_auto.get():
                mlp_batch_size_list.append('auto')
            if self.mlp_batch_32.get():
                mlp_batch_size_list.append(32)
            if self.mlp_batch_64.get():
                mlp_batch_size_list.append(64)
            if self.mlp_batch_128.get():
                mlp_batch_size_list.append(128)
            if not mlp_batch_size_list:
                mlp_batch_size_list = ['auto']  # Default

            # learning_rate_schedule
            mlp_learning_rate_schedule_list = []
            if self.mlp_lr_schedule_constant.get():
                mlp_learning_rate_schedule_list.append('constant')
            if self.mlp_lr_schedule_invscaling.get():
                mlp_learning_rate_schedule_list.append('invscaling')
            if self.mlp_lr_schedule_adaptive.get():
                mlp_learning_rate_schedule_list.append('adaptive')
            if not mlp_learning_rate_schedule_list:
                mlp_learning_rate_schedule_list = ['constant']  # Default

            # momentum
            mlp_momentum_list = []
            if self.mlp_momentum_09.get():
                mlp_momentum_list.append(0.9)
            if not mlp_momentum_list:
                mlp_momentum_list = [0.9]  # Default

            self._log_progress(f"\n{'='*70}")
            self._log_progress(f"ANALYSIS CONFIGURATION")
            self._log_progress(f"{'='*70}")
            self._log_progress(f"Task type: {task_type}")
            self._log_progress(f"Tier: {tier.upper()}")
            self._log_progress(f"Models: {', '.join(selected_models)}")
            self._log_progress(f"Preprocessing: {', '.join([k for k, v in preprocessing_methods.items() if v])}")
            self._log_progress(f"Window sizes: {window_sizes}")

            # Only show model-specific parameters if that model is selected
            if 'NeuralBoosted' in selected_models:
                self._log_progress(f"NeuralBoosted n_estimators: {n_estimators_list}")
                self._log_progress(f"NeuralBoosted learning rates: {learning_rates}")

            if 'RandomForest' in selected_models:
                self._log_progress(f"RandomForest n_trees: {rf_n_trees_list}")
                self._log_progress(f"RandomForest max_depth: {rf_max_depth_list}")

            if 'Ridge' in selected_models:
                self._log_progress(f"Ridge alphas: {ridge_alphas_list}")

            if 'Lasso' in selected_models:
                self._log_progress(f"Lasso alphas: {lasso_alphas_list}")
            self._log_progress(f"\n** SUBSET ANALYSIS SETTINGS **")
            self._log_progress(f"Variable subsets: {'ENABLED' if enable_variable_subsets else 'DISABLED'}")
            self._log_progress(f"  enable_variable_subsets value: {enable_variable_subsets}")
            if enable_variable_subsets:
                self._log_progress(f"  Variable counts selected: {variable_counts if variable_counts else 'NONE'}")
                if not variable_counts:
                    self._log_progress(f"  ⚠️ WARNING: Variable subsets enabled but no counts selected!")
            else:
                self._log_progress(f"  ⚠️ Variable subsets are DISABLED - no subset analysis will run")
            self._log_progress(f"Region subsets: {'ENABLED' if enable_region_subsets else 'DISABLED'}")
            if enable_region_subsets:
                self._log_progress(f"  Region analysis depth: {self.n_top_regions.get()} regions")
            self._log_progress(f"Data: {len(self.X)} samples × {self.X.shape[1]} wavelengths")
            self._log_progress(f"{'='*70}\n")

            # Run search
            # Filter out excluded spectra
            if self.excluded_spectra:
                mask = ~np.isin(np.arange(len(self.X)), list(self.excluded_spectra))
                X_filtered = self.X[mask]
                y_filtered = self.y[mask]

                # Update progress with exclusion info
                self.root.after(0, lambda: self.progress_text.insert(tk.END,
                    f"\nℹ️ Excluding {len(self.excluded_spectra)} user-selected spectra from analysis...\n"))
                self.root.after(0, lambda: self.progress_text.see(tk.END))
            else:
                X_filtered = self.X
                y_filtered = self.y

            # Filter out validation set (if enabled)
            if self.validation_enabled.get() and self.validation_indices:
                # Remove validation samples from training data
                X_filtered = X_filtered[~X_filtered.index.isin(self.validation_indices)]
                y_filtered = y_filtered[~y_filtered.index.isin(self.validation_indices)]

                n_val = len(self.validation_indices)
                n_cal = len(X_filtered)

                # Update progress with validation info
                self.root.after(0, lambda: self.progress_text.insert(tk.END,
                    f"\n🔬 Validation Set Enabled:\n"))
                self.root.after(0, lambda: self.progress_text.insert(tk.END,
                    f"   • Calibration samples: {n_cal}\n"))
                self.root.after(0, lambda: self.progress_text.insert(tk.END,
                    f"   • Validation samples (held out): {n_val}\n"))
                self.root.after(0, lambda: self.progress_text.insert(tk.END,
                    f"   • Algorithm: {self.validation_algorithm.get()}\n\n"))
                self.root.after(0, lambda: self.progress_text.see(tk.END))

                self._log_progress(f"\n🔬 VALIDATION SET:")
                self._log_progress(f"   Calibration samples: {n_cal}")
                self._log_progress(f"   Validation samples (held out): {n_val}\n")

            # Apply wavelength restriction for analysis (if enabled)
            # These will be passed to run_search() to filter variable selection only
            analysis_wl_min_value = None
            analysis_wl_max_value = None

            if self.enable_analysis_wl_restriction.get():
                wl_min_str = self.analysis_wl_min.get().strip()
                wl_max_str = self.analysis_wl_max.get().strip()

                if wl_min_str or wl_max_str:
                    try:
                        wavelengths = X_filtered.columns.astype(float)
                        wl_mask = pd.Series(True, index=X_filtered.columns)

                        original_wl_count = X_filtered.shape[1]

                        if wl_min_str:
                            wl_min = float(wl_min_str)
                            wl_mask &= wavelengths >= wl_min
                            analysis_wl_min_value = wl_min
                        else:
                            wl_min = wavelengths.min()

                        if wl_max_str:
                            wl_max = float(wl_max_str)
                            wl_mask &= wavelengths <= wl_max
                            analysis_wl_max_value = wl_max
                        else:
                            wl_max = wavelengths.max()

                        # Validate range
                        if wl_min >= wl_max:
                            raise ValueError(f"Minimum wavelength ({wl_min}) must be less than maximum ({wl_max})")

                        # NOTE: We do NOT filter X_filtered here
                        # Preprocessing (SNV, derivatives) needs full imported spectrum
                        # The wavelength restriction will be applied INSIDE run_search()
                        # AFTER preprocessing, to constrain variable selection only
                        restricted_wl_count = wl_mask.sum()

                        # Validate minimum wavelengths exist in range
                        if restricted_wl_count < 1:
                            raise ValueError(f"No wavelengths in range {wl_min:.1f}-{wl_max:.1f} nm. Please adjust the range.")

                        # Log the wavelength range
                        self._log_progress(f"\n🔬 WAVELENGTH RANGE FOR VARIABLE SELECTION:")
                        self._log_progress(f"   Requested range: {wl_min:.1f} - {wl_max:.1f} nm ({restricted_wl_count} wavelengths)")
                        self._log_progress(f"   Full imported spectrum: {original_wl_count} wavelengths")
                        self._log_progress(f"")
                        self._log_progress(f"   ✓ Preprocessing (SNV, derivatives) will use FULL imported spectrum")
                        self._log_progress(f"   ✓ Variable selection will be constrained to specified range")
                        self._log_progress(f"   ✓ This ensures proper spectral context for derivatives\n")

                    except ValueError as e:
                        self._log_progress(f"\n⚠️ WARNING: Invalid wavelength restriction values, ignoring: {e}\n")
                        analysis_wl_min_value = None
                        analysis_wl_max_value = None
                    except Exception as e:
                        self._log_progress(f"\n⚠️ WARNING: Failed to validate wavelength restriction: {e}\n")
                        analysis_wl_min_value = None
                        analysis_wl_max_value = None


            # Parse UVE n_components (empty string = None)
            uve_n_comp = None
            if self.uve_n_components.get().strip():
                try:
                    uve_n_comp = int(self.uve_n_components.get())
                except ValueError:
                    self._log_progress("⚠️ Warning: Invalid UVE n_components, using auto-determination")

            # Collect selected variable selection methods
            selected_varsel_methods = []
            if self.varsel_importance.get():
                selected_varsel_methods.append('importance')
            if self.varsel_spa.get():
                selected_varsel_methods.append('spa')
            if self.varsel_uve.get():
                selected_varsel_methods.append('uve')
            if self.varsel_uve_spa.get():
                selected_varsel_methods.append('uve_spa')
            if self.varsel_ipls.get():
                selected_varsel_methods.append('ipls')

            # Default to importance if none selected
            if not selected_varsel_methods:
                selected_varsel_methods = ['importance']
                self._log_progress("⚠️ No variable selection method selected, defaulting to 'importance'")

            # SAFETY CHECK: Ensure X and y are properly aligned
            if len(X_filtered) != len(y_filtered):
                error_msg = (
                    f"Data alignment error: X has {len(X_filtered)} samples but y has {len(y_filtered)} samples.\n\n"
                    f"This usually happens when:\n"
                    f"1. The spectral files and CSV reference data don't match properly\n"
                    f"2. Some samples have missing target values\n\n"
                    f"Try reloading the data or check that the ID column in your CSV matches the spectral file names."
                )
                self._log_progress(f"\n❌ ERROR: {error_msg}")
                messagebox.showerror("Alignment Error", error_msg)
                raise ValueError(error_msg)

            # Ensure indices match
            if not X_filtered.index.equals(y_filtered.index):
                self._log_progress("⚠️ Warning: Realigning X and y indices to ensure consistency...")
                # Realign by taking only common indices
                common_idx = X_filtered.index.intersection(y_filtered.index)
                X_filtered = X_filtered.loc[common_idx]
                y_filtered = y_filtered.loc[common_idx]
                self._log_progress(f"✓ Realigned to {len(common_idx)} common samples")

            # Adjust max_n_components based on restricted wavelength count
            # PLS cannot use more components than min(n_features, n_samples)
            n_features = X_filtered.shape[1]
            n_samples = len(X_filtered)
            user_max_components = self.max_n_components.get()
            adjusted_max_components = min(user_max_components, n_features, n_samples)

            if adjusted_max_components < user_max_components:
                self._log_progress(f"\n⚠️ PLS COMPONENT ADJUSTMENT:")
                self._log_progress(f"   User setting: {user_max_components} components")
                self._log_progress(f"   Adjusted to: {adjusted_max_components} (limited by {n_features} features, {n_samples} samples)")
                self._log_progress(f"   Reason: PLS requires n_components ≤ min(n_features, n_samples)\n")

            # Calculate excluded and validation counts for saving in results
            n_excluded = len(self.excluded_spectra) if self.excluded_spectra else 0
            n_validation = len(self.validation_indices) if self.validation_enabled.get() and self.validation_indices else 0
            n_total_original = len(self.X)  # Total samples before filtering

            # Get imbalance handling parameters (if enabled)
            imbalance_method, imbalance_params = self._get_imbalance_params()
            if imbalance_method:
                self._log_progress(f"Imbalance handling: {imbalance_method} with params {imbalance_params}")

            results_df, label_encoder = run_search(
                X_filtered,
                y_filtered,
                task_type=task_type,
                folds=self.folds.get(),
                excluded_count=n_excluded,
                validation_count=n_validation,
                total_samples_original=n_total_original,
                variable_penalty=self.variable_penalty.get(),
                complexity_penalty=self.complexity_penalty.get(),
                max_n_components=adjusted_max_components,
                max_iter=self.max_iter.get(),
                models_to_test=selected_models,
                preprocessing_methods=preprocessing_methods,
                window_sizes=window_sizes,
                n_estimators_list=n_estimators_list,
                learning_rates=learning_rates,
                neuralboosted_hidden_sizes=neuralboosted_hidden_sizes,
                neuralboosted_activations=neuralboosted_activations,
                rf_n_trees_list=rf_n_trees_list,
                rf_max_depth_list=rf_max_depth_list,
                rf_min_samples_split_list=rf_min_samples_split_list,
                rf_min_samples_leaf_list=rf_min_samples_leaf_list,
                rf_max_features_list=rf_max_features_list,
                rf_bootstrap_list=rf_bootstrap_list,
                rf_max_leaf_nodes_list=rf_max_leaf_nodes_list,
                rf_min_impurity_decrease_list=rf_min_impurity_decrease_list,
                ridge_alphas_list=ridge_alphas_list,
                ridge_solver_list=ridge_solver_list,
                ridge_tol_list=ridge_tol_list,
                lasso_alphas_list=lasso_alphas_list,
                lasso_selection_list=lasso_selection_list,
                lasso_tol_list=lasso_tol_list,
                elasticnet_alphas_list=elasticnet_alphas_list,
                elasticnet_l1_ratios=elasticnet_l1_ratios_list,
                elasticnet_selection_list=elasticnet_selection_list,
                elasticnet_tol_list=elasticnet_tol_list,
                pls_max_iter_list=pls_max_iters_list,
                pls_tol_list=pls_tols_list,
                xgb_n_estimators_list=xgb_n_estimators_list,
                xgb_learning_rates=xgb_learning_rates,
                xgb_max_depths=xgb_max_depths,
                xgb_subsample=xgb_subsample,
                xgb_colsample_bytree=xgb_colsample_bytree,
                xgb_reg_alpha=xgb_reg_alpha,
                xgb_reg_lambda=xgb_reg_lambda,
                xgb_min_child_weight_list=xgb_min_child_weight,
                xgb_gamma_list=xgb_gamma,
                lightgbm_n_estimators_list=lightgbm_n_estimators_list,
                lightgbm_learning_rates=lightgbm_learning_rates,
                lightgbm_num_leaves_list=lightgbm_num_leaves_list,
                lightgbm_max_depth_list=lightgbm_max_depth_list,
                lightgbm_min_child_samples_list=lightgbm_min_child_samples_list,
                lightgbm_subsample_list=lightgbm_subsample_list,
                lightgbm_colsample_bytree_list=lightgbm_colsample_bytree_list,
                lightgbm_reg_alpha_list=lightgbm_reg_alpha_list,
                lightgbm_reg_lambda_list=lightgbm_reg_lambda_list,
                catboost_iterations_list=catboost_iterations_list,
                catboost_learning_rates=catboost_learning_rates,
                catboost_depths=catboost_depths,
                catboost_l2_leaf_reg_list=catboost_l2_leaf_reg_list,
                catboost_border_count_list=catboost_border_count_list,
                catboost_bagging_temperature_list=catboost_bagging_temperature_list,
                catboost_random_strength_list=catboost_random_strength_list,
                svr_kernels=svr_kernels,
                svr_C_list=svr_C_list,
                svr_gamma_list=svr_gamma_list,
                svr_epsilon_list=svr_epsilon_list,
                svr_degree_list=svr_degree_list,
                svr_coef0_list=svr_coef0_list,
                svr_shrinking_list=svr_shrinking_list,
                mlp_hidden_layer_sizes_list=mlp_hidden_layer_sizes_list,
                mlp_alphas_list=mlp_alphas_list,
                mlp_learning_rate_inits=mlp_learning_rate_inits,
                mlp_activation_list=mlp_activation_list,
                mlp_solver_list=mlp_solver_list,
                mlp_batch_size_list=mlp_batch_size_list,
                mlp_learning_rate_schedule_list=mlp_learning_rate_schedule_list,
                mlp_momentum_list=mlp_momentum_list,
                enable_variable_subsets=enable_variable_subsets,
                variable_counts=variable_counts if variable_counts else None,
                enable_region_subsets=enable_region_subsets,
                n_top_regions=self.n_top_regions.get(),
                progress_callback=self._progress_callback,
                # Variable selection parameters (NEW - supports multiple methods)
                variable_selection_methods=selected_varsel_methods,
                apply_uve_prefilter=self.apply_uve_prefilter.get(),
                uve_cutoff_multiplier=self.uve_cutoff_multiplier.get(),
                uve_n_components=uve_n_comp,
                spa_n_random_starts=self.spa_n_random_starts.get(),
                ipls_n_intervals=self.ipls_n_intervals.get(),
                # Tier system (NEW - Phase 3 implementation)
                tier=tier,
                enabled_models=selected_models,  # User's manual selection overrides tier defaults
                # Wavelength restriction for variable selection only (preprocessing uses full spectrum)
                analysis_wl_min=analysis_wl_min_value,
                analysis_wl_max=analysis_wl_max_value,
                # Imbalance handling (NEW - Phase 2 implementation)
                imbalance_method=imbalance_method,
                imbalance_params=imbalance_params
            )

            # Store label_encoder for saving with models
            self.label_encoder = label_encoder

            # Store training configuration for validation when transferring to Model Dev
            # This captures the exact data configuration used for training
            self.last_training_config = {
                'folds': self.folds.get(),
                'n_samples_used': len(X_filtered),  # Actual calibration samples used
                'excluded_count': n_excluded,
                'validation_count': n_validation,
                'total_samples_original': len(self.X) if self.X is not None else 0
            }
            print(f"\n✓ Stored training configuration:")
            print(f"  Calibration samples: {self.last_training_config['n_samples_used']}")
            print(f"  Excluded samples: {self.last_training_config['excluded_count']}")
            print(f"  Validation samples: {self.last_training_config['validation_count']}")
            print(f"  CV folds: {self.last_training_config['folds']}")

            # Add "Select" column for ensemble model selection
            # Auto-select top N models by CompositeScore (lower is better)
            top_n = self.ensemble_top_n.get()
            results_df.insert(0, 'Select', False)  # Insert as first column
            if len(results_df) > 0:
                # Get indices of top N models (lowest CompositeScore)
                top_indices = results_df.nsmallest(min(top_n, len(results_df)), 'CompositeScore').index
                results_df.loc[top_indices, 'Select'] = True

            # Save results
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = Path(self.output_dir.get())
            output_dir.mkdir(parents=True, exist_ok=True)

            results_path = output_dir / f"results_{self.target_column.get()}_{timestamp}.csv"
            results_df.to_csv(results_path, index=False)

            # Generate report
            report_dir = Path("reports")
            report_dir.mkdir(parents=True, exist_ok=True)
            write_markdown_report(self.target_column.get(), results_df, str(report_dir))

            # === Run Ensemble Methods (if enabled) ===
            if self.enable_ensembles.get():
                ensemble_results, trained_ensembles = self._train_ensembles(
                    results_df,
                    X_filtered,
                    y_filtered,
                    task_type,
                    analysis_wl_min_value=analysis_wl_min_value,
                    analysis_wl_max_value=analysis_wl_max_value,
                    is_manual_retrain=False
                )

                if ensemble_results is not None:
                    # Store ensemble results for later use
                    self.ensemble_results = ensemble_results
                    self.trained_ensembles = trained_ensembles
                    # Store training data for visualizations
                    self.ensemble_X = X_filtered
                    self.ensemble_y = y_filtered
                    # Store wavelength metadata for transparency and reproducibility
                    self.ensemble_metadata = {
                        'wavelengths': X_filtered.columns.tolist(),
                        'n_wavelengths': X_filtered.shape[1],
                        'analysis_wl_min': analysis_wl_min_value,
                        'analysis_wl_max': analysis_wl_max_value,
                        'use_full_spectrum_preprocessing': True,
                        'n_base_models': len(ensemble_results[0]['ensemble'].models) if ensemble_results else 0,
                        'base_model_names': ensemble_results[0]['ensemble'].model_names if ensemble_results else []
                    }

                    # Populate ensemble results table in Tab 5
                    self.root.after(0, self._populate_ensemble_results)

            # Store results for Results tab
            self.results_df = results_df

            # === CACHE TRAINING DATA FOR MANUAL ENSEMBLE RETRAINING ===
            # Store filtered data and configuration so user can retrain ensembles
            # with different model selections after initial analysis
            self.training_data_cache = {
                'X_filtered': X_filtered.copy(),  # Deep copy to prevent modifications
                'y_filtered': y_filtered.copy(),
                'task_type': task_type,
                'folds': self.folds.get(),
                'label_encoder': label_encoder,
                'analysis_wl_min': analysis_wl_min_value,
                'analysis_wl_max': analysis_wl_max_value,
                'timestamp': datetime.now().isoformat()
            }
            self._log_progress(f"\n✓ Cached training data for ensemble retraining")

            # Populate Results tab
            self.root.after(0, lambda: self._populate_results_table(results_df))

            self._log_progress(f"\n✓ Analysis complete!")
            self._log_progress(f"Results saved to: {results_path}")

            self.root.after(0, lambda: self.progress_status.config(text="✓ Analysis complete!"))
            self.root.after(0, lambda: self.progress_info.config(text="Analysis Complete"))

            # Stop running figure animation
            if hasattr(self, 'running_figure'):
                self.root.after(0, lambda: self.running_figure.stop_animation())

            # Play completion chime
            self.root.after(0, self._play_completion_chime)

            # Analysis complete - status updated

        except Exception as e:
            import traceback
            error_msg = traceback.format_exc()
            error_str = str(e)
            self._log_progress(f"\n✗ Error: {e}\n{error_msg}")
            self.root.after(0, lambda: self.progress_status.config(text="✗ Analysis failed"))

            # Stop running figure animation on error
            if hasattr(self, 'running_figure'):
                self.root.after(0, lambda: self.running_figure.stop_animation())

            self.root.after(0, lambda: messagebox.showerror("Error", f"Analysis failed:\n{error_str}"))

    def _progress_callback(self, info):
        """Handle progress updates."""
        msg = info.get('message', '')
        self._log_progress(msg)

        current = info.get('current', 0)
        total = info.get('total', 1)
        best_model = info.get('best_model', None)

        # Calculate time estimate
        if self.analysis_start_time and current > 0:
            elapsed = (datetime.now() - self.analysis_start_time).total_seconds()
            time_per_config = elapsed / current
            remaining_configs = total - current
            estimated_remaining = time_per_config * remaining_configs

            # Format elapsed time
            if elapsed < 60:
                elapsed_str = f"{int(elapsed)}s"
            elif elapsed < 3600:
                elapsed_str = f"{int(elapsed / 60)}m {int(elapsed % 60)}s"
            else:
                hours = int(elapsed / 3600)
                minutes = int((elapsed % 3600) / 60)
                elapsed_str = f"{hours}h {minutes}m"

            # Format time remaining
            if estimated_remaining < 60:
                remaining_str = f"~{int(estimated_remaining)}s"
            elif estimated_remaining < 3600:
                remaining_str = f"~{int(estimated_remaining / 60)}m {int(estimated_remaining % 60)}s"
            else:
                hours = int(estimated_remaining / 3600)
                minutes = int((estimated_remaining % 3600) / 60)
                remaining_str = f"~{hours}h {minutes}m"

            # Combine elapsed and remaining
            time_str = f"Elapsed: {elapsed_str} | Remaining: {remaining_str}"
        else:
            time_str = "Calculating..."

        # Update progress info
        self.root.after(0, lambda: self.progress_info.config(text=f"Progress: {current}/{total} configurations"))
        self.root.after(0, lambda: self.time_estimate_label.config(text=time_str))

        # Update best model display
        if best_model:
            # Determine task type from best_model dict
            if 'RMSE' in best_model:
                # Regression
                model_text = f"{best_model['Model']} | {best_model['Preprocess']}"
                if best_model.get('Deriv'):
                    model_text += f" (d{best_model['Deriv']})"
                model_text += f"\nRMSE: {best_model['RMSE']:.4f} | R²: {best_model['R2']:.4f}"

                # Add top wavelengths if available
                if 'top_vars' in best_model and best_model['top_vars'] != 'N/A':
                    top_vars = best_model['top_vars'].split(',')[:5]  # First 5
                    model_text += f"\nTop λ: {', '.join(top_vars)} nm"
            else:
                # Classification
                model_text = f"{best_model['Model']} | {best_model['Preprocess']}"
                if best_model.get('Deriv'):
                    model_text += f" (d{best_model['Deriv']})"
                model_text += f"\nAcc: {best_model['Accuracy']:.4f}"
                if 'ROC_AUC' in best_model and not np.isnan(best_model['ROC_AUC']):
                    model_text += f" | AUC: {best_model['ROC_AUC']:.4f}"

                # Add top wavelengths if available
                if 'top_vars' in best_model and best_model['top_vars'] != 'N/A':
                    top_vars = best_model['top_vars'].split(',')[:5]  # First 5
                    model_text += f"\nTop λ: {', '.join(top_vars)} nm"

            self.root.after(0, lambda text=model_text: self.best_model_info.config(text=text))

    def _log_progress(self, message):
        """Log message to progress text area."""
        self.root.after(0, lambda: self._append_progress(message))

    def _append_progress(self, message):
        """Append message to progress text (must be called from main thread)."""
        self.progress_text.insert(tk.END, message + "\n")
        self.progress_text.see(tk.END)

        # Limit text widget to last 2000 lines to prevent performance degradation
        # This keeps memory usage low and tab switching fast
        line_count = int(self.progress_text.index('end-1c').split('.')[0])
        if line_count > 2000:
            # Delete oldest lines, keeping only the last 2000
            self.progress_text.delete('1.0', f'{line_count - 2000}.0')

    def _debounced_configure_scrollregion(self, canvas_id, canvas):
        """
        Debounced handler for canvas Configure events to prevent slowdowns during tab switches.
        Only updates scrollregion after configure events have stopped firing for 100ms.
        """
        # Cancel any pending timer for this canvas
        if canvas_id in self._configure_timers:
            self.root.after_cancel(self._configure_timers[canvas_id])

        # Schedule new update after 100ms delay
        def update_scrollregion():
            try:
                canvas.configure(scrollregion=canvas.bbox("all"))
            except:
                pass  # Canvas may have been destroyed
            finally:
                if canvas_id in self._configure_timers:
                    del self._configure_timers[canvas_id]

        self._configure_timers[canvas_id] = self.root.after(100, update_scrollregion)

    def _on_tab_changed(self, event):
        """Handle tab change events."""
        # Cancel all pending scroll region updates for non-visible tabs
        # This prevents unnecessary recalculations when switching tabs
        current_tab = self.notebook.index(self.notebook.select())

        # Only keep timers for the current tab, cancel others
        tabs_to_cancel = []
        for canvas_id in list(self._configure_timers.keys()):
            # Extract tab number from canvas_id (e.g., "tab1" -> 0)
            if canvas_id.startswith("tab"):
                try:
                    tab_num = int(canvas_id.replace("tab", "")) - 1
                    if tab_num != current_tab:
                        self.root.after_cancel(self._configure_timers[canvas_id])
                        tabs_to_cancel.append(canvas_id)
                except (ValueError, AttributeError):
                    pass

        # Remove cancelled timers
        for canvas_id in tabs_to_cancel:
            del self._configure_timers[canvas_id]

        # Stop live monitoring when leaving Multi-Model tab (index 8)
        if hasattr(self, 'live_monitoring_active') and self.live_monitoring_active:
            if current_tab != 9:  # Tab 10 (Multi-Model) is now index 9
                self._stop_live_monitoring()

        # Auto-refresh calibration transfer instruments when switching to that tab (index 10)
        if current_tab == 10 and self.instrument_profiles:
            inst_ids = list(self.instrument_profiles.keys())
            self.ct_master_instrument_combo['values'] = inst_ids
            self.ct_slave_instrument_combo['values'] = inst_ids

    def _show_help(self):
        """Show help dialog."""
        help_text = """Spectral Predict - Quick Start

1. IMPORT & PREVIEW Tab:
   - Select ASD directory or CSV file
   - Select reference CSV
   - Auto-detect columns
   - Load data to see plots

2. ANALYSIS CONFIGURATION Tab:
   - Configure analysis options
   - Select models to test
   - Run analysis

3. ANALYSIS PROGRESS Tab:
   - Auto-switches during analysis
   - Shows live progress
   - Displays results when complete

4. RESULTS Tab:
   - View all model results
   - Double-click a row to refine

5. REFINE MODEL Tab:
   - Tweak model parameters
   - Run refined models
"""
        messagebox.showinfo("Help", help_text)

    def _sort_results_by_column(self, col):
        """Sort results table by the specified column."""
        if self.results_df is None or len(self.results_df) == 0:
            return

        # Toggle sort direction if clicking the same column
        if self.results_sort_column == col:
            self.results_sort_reverse = not self.results_sort_reverse
        else:
            self.results_sort_column = col
            # For "higher is better" metrics, default to descending (best first)
            higher_is_better_cols = ['R2', 'R²', 'Accuracy', 'ROC_AUC', 'F1']
            if col in higher_is_better_cols:
                self.results_sort_reverse = True  # Start with descending (best first)
            else:
                self.results_sort_reverse = False  # Start with ascending

        # Create a copy to sort
        sorted_df = self.results_df.copy()

        # Convert column to numeric if possible for proper sorting
        try:
            sorted_df[col] = pd.to_numeric(sorted_df[col])
        except (ValueError, TypeError):
            pass  # Keep as string if not numeric

        # Sort the dataframe
        sorted_df = sorted_df.sort_values(by=col, ascending=not self.results_sort_reverse)

        # Repopulate with sorted data
        self._populate_results_table(sorted_df, is_sorted=True)

    def _populate_results_table(self, results_df, is_sorted=False):
        """Populate the results table with analysis results."""
        if results_df is None or len(results_df) == 0:
            self.results_status.config(text="No results to display")
            return

        # Store original results if this is the first population (not a sort)
        if not is_sorted:
            self.results_df = results_df
            self.results_sort_column = None
            self.results_sort_reverse = False

        # Clear existing items
        for item in self.results_tree.get_children():
            self.results_tree.delete(item)

        columns = list(results_df.columns)

        # Only configure columns on first load (not during sorting) to prevent jumping
        if not is_sorted:
            # Set up columns
            self.results_tree['columns'] = columns

            # Configure column widths and anchors
            for col in columns:
                # Set column width based on content
                if col == 'Select':
                    width = 60
                elif col in ['Model', 'Preprocess', 'Subset']:
                    width = 120
                elif col in ['top_vars']:
                    width = 200
                else:
                    width = 80
                self.results_tree.column(col, width=width, anchor='center')

        # Always update column headings (for sort indicators and click bindings)
        for col in columns:
            # Add sort indicator if this column is currently sorted
            header_text = col
            if self.results_sort_column == col:
                if self.results_sort_reverse:
                    header_text = f"{col} ▼"  # Descending (high to low)
                else:
                    header_text = f"{col} ▲"  # Ascending (low to high)

            # Bind click event to sort by this column
            self.results_tree.heading(col, text=header_text,
                                     command=lambda c=col: self._sort_results_by_column(c))

        # Insert data rows
        for idx, row in results_df.iterrows():
            values = []
            for col in columns:
                if col == 'Select':
                    # Convert boolean to checkbox symbol
                    values.append('☑' if row[col] else '☐')
                else:
                    values.append(row[col])
            self.results_tree.insert('', 'end', iid=str(idx), values=values)

        # Update status
        self.results_status.config(text=f"Displaying {len(results_df)} results. Double-click a row to refine the model.")

        # Enable Train Ensemble button if conditions are met
        if len(results_df) >= 2 and self.training_data_cache is not None:
            self.btn_train_ensemble.config(state='normal')
        else:
            self.btn_train_ensemble.config(state='disabled')

    def _on_result_click(self, event):
        """Handle single-click on results table to toggle checkbox selection."""
        # Identify which column was clicked
        region = self.results_tree.identify('region', event.x, event.y)
        if region != 'cell':
            return

        column = self.results_tree.identify_column(event.x)
        row_id = self.results_tree.identify_row(event.y)

        if not row_id:
            return

        # Check if click was on the Select column (column #0)
        if column != '#1':  # #1 is the first column (Select)
            return

        # Toggle the selection
        row_idx = int(row_id)
        if self.results_df is not None:
            # Toggle boolean value in DataFrame
            self.results_df.loc[row_idx, 'Select'] = not self.results_df.loc[row_idx, 'Select']

            # Update display (toggle checkbox symbol)
            current_values = list(self.results_tree.item(row_id, 'values'))
            current_values[0] = '☑' if self.results_df.loc[row_idx, 'Select'] else '☐'
            self.results_tree.item(row_id, values=current_values)

    def _on_result_double_click(self, event):
        """Handle double-click on a result row."""
        selection = self.results_tree.selection()
        if not selection:
            return

        # Get the selected row index
        item_id = selection[0]
        row_idx = int(item_id)

        if self.results_df is None:
            return

        # Get the selected model configuration
        # CRITICAL: Use .loc (label-based) not .iloc (position-based)
        # because treeview IID uses the dataframe's original index labels
        model_config = self.results_df.loc[row_idx].to_dict()

        # Attach training configuration if available (for validation checking)
        # This ensures Model Dev can verify it's using the same data configuration as Results tab
        if hasattr(self, 'last_training_config') and self.last_training_config is not None:
            model_config['training_config'] = self.last_training_config
            print(f"✓ Attached training configuration to model transfer")
            print(f"  Expected calibration samples: {self.last_training_config['n_samples_used']}")

            # CRITICAL: Also attach validation configuration
            # Without this, validation_indices gets cleared and causes false mismatch warnings
            if self.validation_enabled.get() and self.validation_indices:
                model_config['validation_indices'] = list(self.validation_indices)
                model_config['validation_set_enabled'] = True
                print(f"  Validation samples: {len(self.validation_indices)}")
            else:
                model_config['validation_set_enabled'] = False
                print(f"  Validation: not used")

            # Also attach excluded spectra info for consistency
            if self.excluded_spectra:
                model_config['excluded_spectra'] = list(self.excluded_spectra)
                print(f"  Excluded samples: {len(self.excluded_spectra)}")

        self.selected_model_config = model_config

        # Validation logging
        rank = model_config.get('Rank', 'N/A')
        r2_or_acc = model_config.get('R2', model_config.get('Accuracy', 'N/A'))
        model_name = model_config.get('Model', 'N/A')
        print(f"✓ Loading Rank {rank}: {model_name} (R²/Acc={r2_or_acc}, n_vars={model_config.get('n_vars', 'N/A')})")

        # Populate the Model Development tab
        self._load_model_for_refinement(model_config)

        # Switch to the Model Development tab
        self.notebook.select(7)  # Tab 7 (index 7)

        # Always switch to Selection subtab (first subtab) when loading a model
        self.model_dev_notebook.select(0)  # Selection subtab

    def _populate_ensemble_results(self):
        """Populate the ensemble results table with trained ensemble performance."""
        if not hasattr(self, 'ensemble_results') or self.ensemble_results is None or len(self.ensemble_results) == 0:
            self.ensemble_status.config(text="No ensemble results. Enable ensembles in Analysis Configuration.")
            # Disable buttons
            self.btn_save_best_ensemble.config(state='disabled')
            self.btn_show_regional_perf.config(state='disabled')
            self.btn_show_weights.config(state='disabled')
            self.btn_show_specialization.config(state='disabled')
            return

        # Clear existing items
        for item in self.ensemble_tree.get_children():
            self.ensemble_tree.delete(item)

        # Define columns
        columns = ['Rank', 'Method', 'RMSE', 'R²', 'MAE', 'RPD', 'vs Best Individual']
        self.ensemble_tree['columns'] = columns

        # Configure columns
        column_widths = {
            'Rank': 60,
            'Method': 200,
            'RMSE': 100,
            'R²': 100,
            'MAE': 100,
            'RPD': 80,
            'vs Best Individual': 150
        }

        for col in columns:
            self.ensemble_tree.heading(col, text=col)
            self.ensemble_tree.column(col, width=column_widths.get(col, 100), anchor='center')

        # Get best individual model performance for comparison
        best_individual_r2 = self.results_df['R2'].max() if self.results_df is not None and 'R2' in self.results_df.columns else 0
        best_individual_rmse = self.results_df['RMSE'].min() if self.results_df is not None and 'RMSE' in self.results_df.columns else float('inf')

        # Populate table (already sorted by R² descending from training)
        for i, result in enumerate(self.ensemble_results, 1):
            # Calculate improvement
            r2_improvement = result['r2'] - best_individual_r2

            # Format comparison
            if r2_improvement > 0:
                comparison = f"+{r2_improvement:.4f} ✓"
            elif r2_improvement < 0:
                comparison = f"{r2_improvement:.4f} ✗"
            else:
                comparison = "Same"

            # Add rank indicator
            rank = "🏆 1" if i == 1 else str(i)

            values = (
                rank,
                result['method'],
                f"{result['rmse']:.4f}",
                f"{result['r2']:.4f}",
                f"{result['mae']:.4f}",
                f"{result['rpd']:.2f}",
                comparison
            )

            # Highlight best ensemble
            tag = 'best' if i == 1 else ''
            # Store index (i-1) as the item ID so we can retrieve the ensemble later
            self.ensemble_tree.insert('', 'end', iid=str(i-1), values=values, tags=(tag,))

        # Configure tag colors
        self.ensemble_tree.tag_configure('best', background='#d4edda', foreground='#155724')

        # Select the best ensemble by default
        if len(self.ensemble_results) > 0:
            self.ensemble_tree.selection_set('0')
            self.ensemble_tree.focus('0')

        # Update status
        n_ensembles = len(self.ensemble_results)
        best_method = self.ensemble_results[0]['method']
        self.ensemble_status.config(
            text=f"✓ {n_ensembles} ensemble(s) trained. Best: {best_method} (R²={self.ensemble_results[0]['r2']:.4f})")

        # Enable buttons
        self.btn_save_best_ensemble.config(state='normal')
        self.btn_show_regional_perf.config(state='normal')
        self.btn_show_weights.config(state='normal')
        self.btn_show_specialization.config(state='normal')

    def _get_selected_ensemble(self):
        """Get the currently selected ensemble from the tree."""
        selection = self.ensemble_tree.selection()
        if not selection:
            messagebox.showwarning("No Selection", "Please select an ensemble from the table first.")
            return None

        # Get the index from the item ID
        selected_idx = int(selection[0])
        return self.ensemble_results[selected_idx]

    def _show_regional_performance(self):
        """Show regional performance heatmap for all models in the selected ensemble."""
        if not hasattr(self, 'ensemble_results') or not self.ensemble_results:
            messagebox.showwarning("No Ensembles", "No ensemble results available.")
            return

        if not hasattr(self, 'ensemble_X') or not hasattr(self, 'ensemble_y'):
            messagebox.showwarning("No Training Data", "Ensemble training data not available. Visualizations only work for freshly trained ensembles.")
            return

        try:
            from spectral_predict.ensemble_viz import plot_regional_performance
            import matplotlib.pyplot as plt

            # Get selected ensemble
            selected_result = self._get_selected_ensemble()
            if selected_result is None:
                return
            ensemble = selected_result['ensemble']

            # Check if ensemble has analyzer (only region-aware ensembles)
            if not hasattr(ensemble, 'analyzer_'):
                messagebox.showinfo(
                    "Not Applicable",
                    f"Regional performance visualization is only available for region-aware ensembles.\n\n"
                    f"Selected ensemble type: {selected_result['method']}\n\n"
                    f"Try 'Region-Aware Weighted' or 'Mixture of Experts' ensemble methods."
                )
                return

            # Create predictions dict from base models
            predictions_dict = {}
            for model, name in zip(ensemble.models, ensemble.model_names):
                predictions_dict[name] = model.predict(self.ensemble_X)

            # Create figure
            fig, axes = plot_regional_performance(
                analyzer=ensemble.analyzer_,
                y_true=self.ensemble_y,
                predictions_dict=predictions_dict,
                metric='rmse'
            )

            # Add very explicit explanation text
            ensemble_method = selected_result['method']
            n_models = len(ensemble.models)
            fig.suptitle(
                f'INDIVIDUAL MODEL PERFORMANCE WITHIN "{ensemble_method}" ENSEMBLE\n'
                f'Comparing {n_models} Base Models Across Different Prediction Regions\n'
                f'(Each row = 1 model inside the ensemble | Each column = 1 target value range)\n'
                f'DARKER BLUE = BETTER (Lower RMSE error)',
                fontsize=11, y=0.98, weight='bold'
            )

            # Show in popup window with clearer title
            fig.canvas.manager.set_window_title(f'Base Model Performance - {ensemble_method} Ensemble')
            plt.show()

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            messagebox.showerror("Visualization Error", f"Failed to show regional performance:\n{str(e)}\n\nDetails:\n{error_details}")

    def _show_ensemble_weights(self):
        """Show ensemble weights visualization for selected ensemble."""
        if not hasattr(self, 'ensemble_results') or not self.ensemble_results:
            messagebox.showwarning("No Ensembles", "No ensemble results available.")
            return

        try:
            from spectral_predict.ensemble_viz import plot_ensemble_weights
            import matplotlib.pyplot as plt

            # Get selected ensemble
            selected_result = self._get_selected_ensemble()
            if selected_result is None:
                return
            ensemble = selected_result['ensemble']

            # Check if ensemble has weights
            if not hasattr(ensemble, 'weights_'):
                messagebox.showinfo(
                    "Not Applicable",
                    f"Ensemble weights visualization is only available for weighted ensembles.\n\n"
                    f"Selected ensemble type: {selected_result['method']}\n\n"
                    f"Weighted ensemble types: 'Region-Aware Weighted', 'Optimized Weighted', 'Error-Based Weighted'"
                )
                return

            # Create figure
            fig, axes = plot_ensemble_weights(ensemble=ensemble)

            # Add explicit explanation
            ensemble_method = selected_result['method']
            n_models = len(ensemble.models)
            fig.suptitle(
                f'MODEL WEIGHTS IN "{ensemble_method}" ENSEMBLE\n'
                f'How Much Each of the {n_models} Base Models Contributes to Final Predictions\n'
                f'HIGHER WEIGHT = MORE INFLUENCE on ensemble output',
                fontsize=11, y=0.98, weight='bold'
            )

            # Show in popup window with clearer title
            fig.canvas.manager.set_window_title(f'Model Weights - {ensemble_method} Ensemble')
            plt.show()

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            messagebox.showerror("Visualization Error", f"Failed to show ensemble weights:\n{str(e)}\n\nDetails:\n{error_details}")

    def _show_specialization_profile(self):
        """Show model specialization profiles for selected ensemble."""
        if not hasattr(self, 'ensemble_results') or not self.ensemble_results:
            messagebox.showwarning("No Ensembles", "No ensemble results available.")
            return

        if not hasattr(self, 'ensemble_X') or not hasattr(self, 'ensemble_y'):
            messagebox.showwarning("No Training Data", "Ensemble training data not available. Visualizations only work for freshly trained ensembles.")
            return

        try:
            from spectral_predict.ensemble_viz import plot_model_specialization_profile
            import matplotlib.pyplot as plt

            # Get selected ensemble
            selected_result = self._get_selected_ensemble()
            if selected_result is None:
                return
            ensemble = selected_result['ensemble']

            # Check if ensemble has analyzer (only region-aware ensembles)
            if not hasattr(ensemble, 'analyzer_'):
                messagebox.showinfo(
                    "Not Applicable",
                    f"Specialization profile is only available for region-aware ensembles.\n\n"
                    f"Selected ensemble type: {selected_result['method']}\n\n"
                    f"Try 'Region-Aware Weighted' or 'Mixture of Experts' ensemble methods."
                )
                return

            # Create figure
            fig, axes = plot_model_specialization_profile(ensemble=ensemble)

            # Add very explicit explanation
            ensemble_method = selected_result['method']
            n_models = len(ensemble.models)
            model_list = ', '.join(ensemble.model_names[:3]) + (f' + {n_models-3} more' if n_models > 3 else '')
            fig.suptitle(
                f'MODEL SPECIALIZATION IN "{ensemble_method}" ENSEMBLE\n'
                f'Which of the {n_models} Base Models Excel at Different Target Value Ranges\n'
                f'Models: {model_list}\n'
                f'(Each line = 1 model\'s performance across low/medium/high values)',
                fontsize=11, y=0.98, weight='bold'
            )

            # Show in popup window with clearer title
            fig.canvas.manager.set_window_title(f'Model Specialization - {ensemble_method} Ensemble')
            plt.show()

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            messagebox.showerror("Visualization Error", f"Failed to show specialization profile:\n{str(e)}\n\nDetails:\n{error_details}")

    def _save_selected_ensemble(self):
        """Save the selected ensemble model to a .dasp file."""
        if not hasattr(self, 'ensemble_results') or not self.ensemble_results:
            messagebox.showwarning("No Ensembles", "No ensemble results available.")
            return

        try:
            from pathlib import Path
            from spectral_predict.model_io import save_ensemble

            # Get selected ensemble
            selected_result = self._get_selected_ensemble()
            if selected_result is None:
                return
            ensemble = selected_result['ensemble']
            ensemble_type = selected_result['type']
            ensemble_name = selected_result['method']

            # Determine initial directory (prefer data import folder over output folder)
            initial_dir = None
            if self.spectral_data_path.get():
                data_path = Path(self.spectral_data_path.get())
                initial_dir = str(data_path.parent if data_path.is_file() else data_path)
            elif self.output_dir.get():
                initial_dir = self.output_dir.get()
            else:
                initial_dir = str(Path.home())

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            default_name = f"ensemble_{ensemble_type}_{timestamp}.dasp"

            filepath = filedialog.asksaveasfilename(
                title="Save Ensemble Model",
                initialdir=initial_dir,
                initialfile=default_name,
                defaultextension=".dasp",
                filetypes=[("DASP Model Files", "*.dasp"), ("All Files", "*.*")]
            )

            if not filepath:
                return  # User cancelled

            # Get wavelengths from ensemble training data
            if hasattr(self, 'ensemble_X') and self.ensemble_X is not None:
                wavelengths = self.ensemble_X.columns.astype(float).tolist()
                n_vars = len(wavelengths)
            elif hasattr(self, 'X') and self.X is not None:
                wavelengths = self.X.columns.astype(float).tolist()
                n_vars = len(wavelengths)
            else:
                messagebox.showerror("Error", "Cannot determine wavelengths from training data.")
                return

            # Determine task type from target column or results
            task_type = 'regression'  # Default
            if hasattr(self, 'task_type_var'):
                task_type = self.task_type_var.get()
            elif hasattr(self, 'y') and self.y is not None:
                # Infer from data
                import numpy as np
                if self.y.dtype == object or not np.issubdtype(self.y.dtype, np.number):
                    task_type = 'classification'
                elif len(np.unique(self.y)) < 20:  # Heuristic for classification
                    task_type = 'classification'

            # Get preprocessing information from results DataFrame if available
            preprocessing = 'unknown'
            window = None
            use_full_spectrum_preprocessing = True  # Always true with our new implementation
            full_wavelengths = wavelengths  # Same as wavelengths since we use full spectrum

            if hasattr(self, 'results_df') and self.results_df is not None and len(self.results_df) > 0:
                # Get the most common preprocessing from top models used in ensemble
                top_preprocess = self.results_df.nsmallest(5, 'CompositeScore')['Preprocess'].mode()
                if len(top_preprocess) > 0:
                    preprocessing = top_preprocess[0]

                # Get window size if applicable
                if 'Window' in self.results_df.columns:
                    top_windows = self.results_df.nsmallest(5, 'CompositeScore')['Window'].mode()
                    if len(top_windows) > 0:
                        window = int(top_windows[0])

            # Get wavelength restriction info if available
            analysis_wl_min_saved = None
            analysis_wl_max_saved = None
            if hasattr(self, 'ensemble_metadata') and self.ensemble_metadata is not None:
                analysis_wl_min_saved = self.ensemble_metadata.get('analysis_wl_min')
                analysis_wl_max_saved = self.ensemble_metadata.get('analysis_wl_max')

            # Get training data for applicability domain if available
            X_train = None
            if hasattr(self, 'ensemble_X') and self.ensemble_X is not None:
                # Convert to numpy array (preprocessed data)
                X_train = self.ensemble_X.values
                self._log_progress(f"Including applicability domain data ({X_train.shape[0]} samples)")

            # Get CV data if available (from ensemble predictions)
            cv_residuals = None
            cv_predictions = None
            cv_actuals = None
            if hasattr(self, 'ensemble_y') and self.ensemble_y is not None:
                cv_actuals = self.ensemble_y.values if hasattr(self.ensemble_y, 'values') else np.array(self.ensemble_y)
                # Get ensemble predictions
                try:
                    cv_predictions = ensemble.predict(self.ensemble_X.values if hasattr(self.ensemble_X, 'values') else self.ensemble_X)
                    if task_type == 'regression':
                        cv_residuals = cv_predictions - cv_actuals
                    self._log_progress(f"Including uncertainty estimation data")
                except Exception as e:
                    self._log_progress(f"Warning: Could not generate CV predictions: {e}")

            # Get preprocessor if available (usually None for ensembles as preprocessing is in pipelines)
            preprocessor = None

            # Get label encoder if available
            label_encoder = getattr(self, 'label_encoder', None)

            # Build comprehensive metadata
            metadata = {
                # Ensemble-specific fields
                'ensemble_type': ensemble_type,
                'ensemble_name': ensemble_name,
                'n_models': len(ensemble.models),
                'model_names': ensemble.model_names,

                # Required fields for save_model validation
                'task_type': task_type,
                'wavelengths': wavelengths,
                'n_vars': n_vars,

                # Preprocessing information
                'preprocessing': preprocessing,
                'window': window,
                'use_full_spectrum_preprocessing': use_full_spectrum_preprocessing,
                'full_wavelengths': full_wavelengths,

                # Wavelength restriction info (for transparency and reproducibility)
                'analysis_wl_min': analysis_wl_min_saved,
                'analysis_wl_max': analysis_wl_max_saved,

                # Performance metrics
                'performance': {
                    'RMSE': float(selected_result['rmse']),
                    'R2': float(selected_result['r2']),
                    'MAE': float(selected_result['mae']),
                    'RPD': float(selected_result['rpd'])
                },

                # Training information
                'training_date': datetime.now().isoformat(),
                'target_column': self.target_column.get(),
                'n_training_samples': int(len(self.ensemble_y)) if hasattr(self, 'ensemble_y') else 0,

                # Optional fields for applicability domain and uncertainty
                'X_train': X_train,
                'cv_residuals': cv_residuals,
                'cv_predictions': cv_predictions,
                'cv_actuals': cv_actuals,
                'preprocessor': preprocessor,
                'label_encoder': label_encoder
            }

            # Save ensemble
            self._log_progress(f"\nSaving ensemble to: {filepath}")
            self._log_progress(f"  Task type: {task_type}")
            self._log_progress(f"  Wavelengths: {n_vars}")
            self._log_progress(f"  Preprocessing: {preprocessing}")
            self._log_progress(f"  Models: {', '.join(ensemble.model_names)}")

            save_ensemble(ensemble, filepath, metadata)
            self._log_progress(f"✓ Ensemble saved successfully!")

            messagebox.showinfo(
                "Success",
                f"Ensemble saved successfully!\n\nFile: {Path(filepath).name}\nLocation: {Path(filepath).parent}"
            )

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            messagebox.showerror("Save Error", f"Failed to save ensemble:\n{str(e)}\n\n{error_details}")
            self._log_progress(f"\n❌ Error saving ensemble:\n{error_details}")

    def _export_results_table(self):
        """Export the current results table to a CSV file."""
        if self.results_df is None or len(self.results_df) == 0:
            messagebox.showwarning(
                "No Results",
                "No results to export. Run an analysis first.")
            return

        try:
            # Get default directory from spectral data path
            initial_dir = None
            if self.spectral_data_path.get():
                data_path = Path(self.spectral_data_path.get())
                initial_dir = str(data_path.parent if data_path.is_file() else data_path)

            # Ask user for save location
            default_name = f"analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            filepath = filedialog.asksaveasfilename(
                defaultextension=".csv",
                filetypes=[
                    ("CSV files", "*.csv"),
                    ("Excel files", "*.xlsx"),
                    ("All files", "*.*")
                ],
                initialfile=default_name,
                initialdir=initial_dir,
                title="Export Results"
            )

            if not filepath:
                return  # User cancelled

            # Export the dataframe based on file extension
            filepath_obj = Path(filepath)
            if filepath_obj.suffix.lower() in ['.xlsx', '.xls']:
                from spectral_predict.io import write_excel_spectra
                # For results table, just use pandas to_excel directly since it's not spectral data
                self.results_df.to_excel(filepath, index=False, engine='xlsxwriter')
            else:
                self.results_df.to_csv(filepath, index=False)

            # Export successful - status updated
            self.results_status.config(text=f"✓ Results exported to {Path(filepath).name}")

        except Exception as e:
            messagebox.showerror(
                "Export Error",
                f"Failed to export results:\n\n{str(e)}"
            )

    def _populate_data_viewer(self):
        """Populate the data viewer with Excel-like scrolling using tksheet.

        PERFORMANCE: Uses tksheet's virtual scrolling - only visible cells are rendered.
        No pagination needed - smooth scrolling through entire dataset.
        """
        # Check if data is loaded
        if self.X is None or self.y is None:
            self.data_viewer_info.config(
                text="Load data in the Import & Preview tab to view it here.")
            self.data_viewer_status.config(text="")
            # Clear sheet
            self.data_viewer_sheet.set_sheet_data([[]])
            self.data_viewer_sheet.headers([])
            return

        try:
            # Show loading cursor
            original_cursor = self.root.cget('cursor')
            self.root.config(cursor='wait')
            self.root.update_idletasks()

            # Get settings
            show_excluded = self.show_excluded_data_viewer.get()

            # Get target column name
            target_col = "Target"
            if hasattr(self, 'target_col') and self.target_col.get():
                target_col = self.target_col.get()

            # Filter samples based on exclusion setting
            if show_excluded:
                display_df = self.X.copy()
                display_y = self.y.copy()
                excluded_indices = list(self.excluded_spectra)
            else:
                # Filter out excluded samples
                mask = ~self.X.index.isin(self.excluded_spectra)
                display_df = self.X[mask].copy()
                display_y = self.y[mask].copy()
                excluded_indices = []

            # Check if metadata columns exist (from self.ref or self.combined_metadata_df)
            metadata_cols = []
            display_metadata = None

            # For combined files, metadata is in combined_metadata_df
            if hasattr(self, 'combined_metadata_df') and self.combined_metadata_df is not None and len(self.combined_metadata_df.columns) > 0:
                # Filter metadata to match displayed samples
                if show_excluded:
                    display_metadata = self.combined_metadata_df.copy()
                else:
                    display_metadata = self.combined_metadata_df[mask].copy()
                metadata_cols = list(self.combined_metadata_df.columns)
            # For separate spectral + reference files, metadata is in self.ref
            elif self.ref is not None and len(self.ref.columns) > 0:
                # Filter metadata to match displayed samples
                if show_excluded:
                    display_metadata = self.ref.copy()
                else:
                    display_metadata = self.ref[mask].copy()
                metadata_cols = list(self.ref.columns)

            # Build headers: Sample ID, Metadata columns, Target, then all wavelengths
            wavelength_headers = [str(wl) for wl in display_df.columns]
            headers = ['Sample ID'] + metadata_cols + [target_col] + wavelength_headers

            # Pre-format all data
            formatted_data = []
            for idx in display_df.index:
                # Sample ID
                sample_id = str(idx)

                # Metadata values (if any)
                metadata_vals = []
                if display_metadata is not None:
                    for col in metadata_cols:
                        val = display_metadata.loc[idx, col]
                        # Format based on type
                        if pd.isna(val):
                            metadata_vals.append('')
                        elif np.issubdtype(type(val), np.number):
                            metadata_vals.append(f"{val:.4f}")
                        else:
                            metadata_vals.append(str(val))

                # Target value
                if np.issubdtype(display_y.dtype, np.number):
                    target_val = f"{display_y.loc[idx]:.4f}"
                else:
                    target_val = str(display_y.loc[idx])

                # Spectral values (format to 5 decimals)
                spectral_vals = [f"{val:.5f}" for val in display_df.loc[idx].values]

                row_data = [sample_id] + metadata_vals + [target_val] + spectral_vals
                formatted_data.append(row_data)

            # Set sheet data and headers
            self.data_viewer_sheet.set_sheet_data(formatted_data)
            self.data_viewer_sheet.headers(headers)

            # Highlight excluded samples in pink
            if show_excluded and len(excluded_indices) > 0:
                # Find row indices of excluded samples in the display dataframe
                for display_row_idx, original_idx in enumerate(display_df.index):
                    if original_idx in excluded_indices:
                        self.data_viewer_sheet[display_row_idx].bg = "#FFE0E0"

            # Update status
            n_total_samples = len(self.X)
            n_total_wavelengths = len(self.X.columns)
            n_displayed = len(display_df)
            n_excluded = len(self.excluded_spectra)

            if n_excluded > 0 and not show_excluded:
                status_text = f"Showing {n_displayed} samples × {n_total_wavelengths} wavelengths | {n_excluded} excluded samples hidden"
            elif n_excluded > 0 and show_excluded:
                status_text = f"Showing {n_displayed} samples × {n_total_wavelengths} wavelengths | {n_excluded} excluded samples shown in pink"
            else:
                status_text = f"Showing {n_displayed} samples × {n_total_wavelengths} wavelengths"

            self.data_viewer_status.config(text=status_text)
            self.data_viewer_info.config(
                text=f"Total dataset: {n_total_samples} samples × {n_total_wavelengths} wavelengths | Scroll to navigate")

            # Restore cursor
            self.root.config(cursor=original_cursor)

        except Exception as e:
            # Restore cursor on error
            self.root.config(cursor='arrow')
            import traceback
            traceback.print_exc()
            messagebox.showerror(
                "Data Viewer Error",
                f"Failed to populate data viewer:\n\n{str(e)}"
            )
            self.data_viewer_info.config(text=f"Error loading data: {str(e)}")

    def _export_data_viewer_to_csv(self):
        """Export the currently displayed data to a CSV file."""
        if self.X is None or self.y is None:
            messagebox.showwarning(
                "No Data",
                "No data to export. Load data first.")
            return

        try:
            # Get default directory from spectral data path
            initial_dir = None
            if self.spectral_data_path.get():
                data_path = Path(self.spectral_data_path.get())
                initial_dir = str(data_path.parent if data_path.is_file() else data_path)

            # Ask user for save location
            default_name = f"spectral_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            filepath = filedialog.asksaveasfilename(
                defaultextension=".csv",
                filetypes=[("CSV files", "*.csv"), ("All files", "*.*")],
                initialfile=default_name,
                initialdir=initial_dir,
                title="Export Data to CSV"
            )

            if not filepath:
                return  # User cancelled

            # Determine what to export based on checkbox
            show_excluded = self.show_excluded_data_viewer.get()

            # Get target column name
            target_col = "Target"
            if hasattr(self, 'target_col') and self.target_col.get():
                target_col = self.target_col.get()

            # Create combined dataframe
            export_df = self.X.copy()

            # Add metadata columns if available (insert before target column)
            # Check combined file metadata first, then reference file metadata
            insert_position = 0
            if hasattr(self, 'combined_metadata_df') and self.combined_metadata_df is not None and len(self.combined_metadata_df.columns) > 0:
                for col in self.combined_metadata_df.columns:
                    export_df.insert(insert_position, col, self.combined_metadata_df[col])
                    insert_position += 1
            elif self.ref is not None and len(self.ref.columns) > 0:
                for col in self.ref.columns:
                    export_df.insert(insert_position, col, self.ref[col])
                    insert_position += 1

            # Add target column
            export_df.insert(insert_position, target_col, self.y)
            export_df.index.name = 'Sample_ID'

            # Filter out excluded samples if checkbox is unchecked
            if not show_excluded and len(self.excluded_spectra) > 0:
                export_df = export_df.drop(index=self.excluded_spectra, errors='ignore')

            # Export
            export_df.to_csv(filepath, index=True)

            # Update status
            n_rows = len(export_df)
            self.data_viewer_status.config(text=f"✓ Exported {n_rows} samples to {Path(filepath).name}")

        except Exception as e:
            messagebox.showerror(
                "Export Error",
                f"Failed to export data:\n\n{str(e)}"
            )

    def _validate_data_for_refinement(self):
        """Validate that required data is available for refinement."""
        if self.X_original is None:
            messagebox.showwarning(
                "Data Not Loaded",
                "Please load data in the Data Upload tab before using Model Development."
            )
            return False

        if self.y is None:
            messagebox.showwarning(
                "Target Variable Missing",
                "Please ensure target variable (y) is loaded in the Data Upload tab."
            )
            return False

        return True

    def _validate_training_configuration(self, training_config):
        """Validate that current state matches training configuration and warn about mismatches."""
        warnings = []

        # Check fold count
        current_folds = self.refine_folds.get()
        saved_folds = training_config.get("folds", 5)
        if current_folds != saved_folds:
            warnings.append(f"CV folds: trained with {saved_folds}, current setting is {current_folds}")

        # Calculate current data state
        # IMPORTANT: Use X_original to get total count BEFORE any exclusions
        # self.X may already have validation samples removed, causing double-counting
        current_total = len(self.X_original) if self.X_original is not None else (len(self.X) if self.X is not None else 0)
        current_excluded = len(self.excluded_spectra) if self.excluded_spectra else 0
        current_validation = len(self.validation_indices) if self.validation_enabled.get() and self.validation_indices else 0
        current_calibration = current_total - current_excluded - current_validation

        # Check sample counts
        saved_samples = training_config.get("n_samples_used")
        if saved_samples and abs(current_calibration - saved_samples) > 0:
            warnings.append(
                f"Calibration samples: trained with {saved_samples}, current has {current_calibration} "
                f"(difference: {current_calibration - saved_samples:+d})"
            )

        # Check excluded count
        saved_excluded = training_config.get("excluded_count", 0)
        if current_excluded != saved_excluded:
            warnings.append(
                f"Excluded samples: trained with {saved_excluded} excluded, current has {current_excluded} excluded"
            )

        # Check validation count
        saved_validation = training_config.get("validation_count", 0)
        if current_validation != saved_validation:
            warnings.append(
                f"Validation set: trained with {saved_validation} validation, current has {current_validation} validation"
            )

        if warnings:
            # Show detailed warning message
            message = "⚠️ TRAINING CONFIGURATION MISMATCH DETECTED!\n\n"
            message += "The following differences were found between the original training and current state:\n\n"
            for warning in warnings:
                message += f"• {warning}\n"
            message += "\nThis WILL cause R² scores to differ from the Results tab.\n\n"
            message += "Recommendations:\n"
            message += "1. Reset excluded samples and validation set to match original training\n"
            message += "2. Set CV folds to match original setting\n"
            message += "3. OR accept that performance metrics will differ with current dataset\n"
            message += "4. OR re-run the full search with current settings\n\n"
            message += "Do you want to continue anyway?"

            # Show warning dialog
            response = messagebox.askyesno(
                "Training Configuration Mismatch",
                message,
                icon='warning'
            )

            if not response:
                # User chose not to continue
                raise ValueError("Model loading cancelled due to configuration mismatch")

            # Also print to console for debugging
            print(f"\n{'='*80}")
            print("⚠️ TRAINING CONFIGURATION MISMATCH")
            print(f"{'='*80}")
            for warning in warnings:
                print(f"  • {warning}")
            print(f"{'='*80}\n")
        else:
            print("\n✓ Training configuration matches current state - R² should be consistent")

    def _load_default_parameters(self):
        """Load default parameters for fresh model development."""
        if not self._validate_data_for_refinement():
            return

        # Set all wavelengths
        wavelengths = list(self.X_original.columns.astype(float).values)
        wl_spec = self._format_wavelengths_as_spec(wavelengths)

        self.refine_wl_spec.config(state='normal')
        self.refine_wl_spec.delete('1.0', 'end')
        self.refine_wl_spec.insert('1.0', wl_spec)

        # Default parameters
        self.refine_model_type.set('PLS')
        self.refine_task_type.set('regression')
        self.refine_preprocess.set('raw')
        self.refine_window.set(17)
        self.refine_folds.set(5)
        self.refine_max_iter.set(100)

        # Update model info display
        self.refine_model_info.config(state='normal')
        self.refine_model_info.delete('1.0', 'end')
        self.refine_model_info.insert('1.0', "Default parameters loaded. Ready for fresh model development.")
        self.refine_model_info.config(state='disabled')

        # Update mode label and status
        self.refine_mode_label.config(text="Mode: Fresh Development (Defaults Loaded)")
        self.refine_status.config(text="Ready to develop custom model")

        # Enable the run buttons (both Configuration and Selection tabs)
        self.refine_run_button.config(state='normal')
        self.refine_run_button_selection.config(state='normal')

        # Update the wavelength count display
        self._update_wavelength_count()

    def _validate_refinement_parameters(self):
        """Validate all refinement parameters before execution."""
        errors = []

        # Validate wavelength specification
        wl_spec_text = self.refine_wl_spec.get('1.0', 'end')
        if not wl_spec_text or wl_spec_text.strip() == '':
            errors.append("Wavelength specification is empty")

        # Validate model type
        model_type = self.refine_model_type.get()
        task_type = self.refine_task_type.get()  # Get the actual task type from the Model Development tab
        if is_valid_model is not None:
            # Use registry validation with correct task type
            if not is_valid_model(model_type, task_type):
                errors.append(f"Invalid model type selected: '{model_type}' for {task_type}")
        else:
            # Fallback validation
            valid_models_regression = ['PLS', 'Ridge', 'Lasso', 'ElasticNet', 'RandomForest', 'MLP', 'NeuralBoosted', 'SVR', 'XGBoost', 'LightGBM', 'CatBoost']
            valid_models_classification = ['PLS-DA', 'RandomForest', 'MLP', 'NeuralBoosted', 'SVM', 'XGBoost', 'LightGBM', 'CatBoost']
            valid_models = valid_models_classification if task_type == 'classification' else valid_models_regression
            if model_type not in valid_models:
                errors.append(f"Invalid model type selected: '{model_type}' for {task_type}")

        # Validate CV folds
        if self.refine_folds.get() < 3:
            errors.append("CV folds must be at least 3")

        if errors:
            messagebox.showerror("Validation Error", "\n".join(errors))
            return False

        return True

    def _collect_refine_hyperparams(self, model_name):
        """
        Collect hyperparameter values from Model Development tab widgets.

        Unlike Analysis Config (which collects lists for grid search), this collects
        single values for direct model training.

        Args:
            model_name: Name of the model (e.g., 'PLS', 'Ridge', 'XGBoost')

        Returns:
            dict: Hyperparameters ready for model.set_params(**params)
                  Returns empty dict if no custom params or model not found
        """
        params = {}

        # Helper function to convert string "None" to actual None
        def parse_none(value):
            """Convert string 'None' to Python None, otherwise return value."""
            if isinstance(value, str) and value.strip().lower() == 'none':
                return None
            return value

        # Helper function to parse tuple strings like "(100, 50)" or "100"
        def parse_hidden_layer_sizes(value):
            """Parse MLP hidden layer sizes from string."""
            if isinstance(value, str):
                value = value.strip()
                if value.lower() == 'none':
                    return None
                # Try to parse as tuple
                try:
                    # Handle formats: "(100, 50)", "(100,)", "100"
                    if value.startswith('(') and value.endswith(')'):
                        # Remove parentheses and split
                        inner = value[1:-1].strip()
                        if not inner:
                            return ()
                        parts = [int(x.strip()) for x in inner.split(',') if x.strip()]
                        return tuple(parts)
                    else:
                        # Single value "100" -> (100,)
                        return (int(value),)
                except (ValueError, AttributeError):
                    print(f"WARNING: Could not parse hidden_layer_sizes '{value}', using default")
                    return None
            return value

        # ========== PLS ==========
        if model_name == 'PLS' or model_name == 'PLS-DA':
            params['n_components'] = self.refine_pls_n_components.get()
            params['max_iter'] = self.refine_pls_max_iter.get()

            tol_str = self.refine_pls_tol.get().strip()
            try:
                params['tol'] = float(tol_str)
            except ValueError:
                print(f"WARNING: Invalid PLS tol '{tol_str}', using default")

        # ========== Ridge ==========
        elif model_name == 'Ridge':
            alpha_str = self.refine_ridge_alpha.get().strip()
            try:
                params['alpha'] = float(alpha_str)
            except ValueError:
                print(f"WARNING: Invalid Ridge alpha '{alpha_str}', using default")

            params['solver'] = self.refine_ridge_solver.get()

            tol_str = self.refine_ridge_tol.get().strip()
            try:
                params['tol'] = float(tol_str)
            except ValueError:
                print(f"WARNING: Invalid Ridge tol '{tol_str}', using default")

        # ========== Lasso ==========
        elif model_name == 'Lasso':
            alpha_str = self.refine_lasso_alpha.get().strip()
            try:
                params['alpha'] = float(alpha_str)
            except ValueError:
                print(f"WARNING: Invalid Lasso alpha '{alpha_str}', using default")

            params['selection'] = self.refine_lasso_selection.get()

            tol_str = self.refine_lasso_tol.get().strip()
            try:
                params['tol'] = float(tol_str)
            except ValueError:
                print(f"WARNING: Invalid Lasso tol '{tol_str}', using default")

            params['max_iter'] = self.refine_lasso_max_iter.get()

        # ========== ElasticNet ==========
        elif model_name == 'ElasticNet':
            alpha_str = self.refine_elasticnet_alpha.get().strip()
            try:
                params['alpha'] = float(alpha_str)
            except ValueError:
                print(f"WARNING: Invalid ElasticNet alpha '{alpha_str}', using default")

            l1_ratio_str = self.refine_elasticnet_l1_ratio.get().strip()
            try:
                params['l1_ratio'] = float(l1_ratio_str)
            except ValueError:
                print(f"WARNING: Invalid ElasticNet l1_ratio '{l1_ratio_str}', using default")

            params['selection'] = self.refine_elasticnet_selection.get()

            tol_str = self.refine_elasticnet_tol.get().strip()
            try:
                params['tol'] = float(tol_str)
            except ValueError:
                print(f"WARNING: Invalid ElasticNet tol '{tol_str}', using default")

            params['max_iter'] = self.refine_elasticnet_max_iter.get()

        # ========== RandomForest ==========
        elif model_name == 'RandomForest':
            params['n_estimators'] = self.refine_rf_n_estimators.get()

            max_depth = parse_none(self.refine_rf_max_depth.get())
            if max_depth is not None:
                try:
                    params['max_depth'] = int(max_depth)
                except ValueError:
                    pass
            else:
                params['max_depth'] = None

            params['min_samples_split'] = self.refine_rf_min_samples_split.get()
            params['min_samples_leaf'] = self.refine_rf_min_samples_leaf.get()
            params['max_features'] = self.refine_rf_max_features.get()
            params['bootstrap'] = self.refine_rf_bootstrap.get()

            max_leaf_nodes = parse_none(self.refine_rf_max_leaf_nodes.get())
            if max_leaf_nodes is not None:
                try:
                    params['max_leaf_nodes'] = int(max_leaf_nodes)
                except ValueError:
                    pass
            else:
                params['max_leaf_nodes'] = None

            params['min_impurity_decrease'] = self.refine_rf_min_impurity_decrease.get()

        # ========== XGBoost ==========
        elif model_name == 'XGBoost':
            params['n_estimators'] = self.refine_xgb_n_estimators.get()
            params['learning_rate'] = self.refine_xgb_learning_rate.get()
            params['max_depth'] = self.refine_xgb_max_depth.get()
            params['subsample'] = self.refine_xgb_subsample.get()
            params['colsample_bytree'] = self.refine_xgb_colsample_bytree.get()
            params['reg_alpha'] = self.refine_xgb_reg_alpha.get()
            params['reg_lambda'] = self.refine_xgb_reg_lambda.get()
            params['min_child_weight'] = self.refine_xgb_min_child_weight.get()
            params['gamma'] = self.refine_xgb_gamma.get()

        # ========== LightGBM ==========
        elif model_name == 'LightGBM':
            params['n_estimators'] = self.refine_lgbm_n_estimators.get()
            params['learning_rate'] = self.refine_lgbm_learning_rate.get()
            params['num_leaves'] = self.refine_lgbm_num_leaves.get()
            params['max_depth'] = self.refine_lgbm_max_depth.get()
            params['min_child_samples'] = self.refine_lgbm_min_child_samples.get()
            params['subsample'] = self.refine_lgbm_subsample.get()
            params['colsample_bytree'] = self.refine_lgbm_colsample_bytree.get()
            params['reg_alpha'] = self.refine_lgbm_reg_alpha.get()
            params['reg_lambda'] = self.refine_lgbm_reg_lambda.get()

        # ========== CatBoost ==========
        elif model_name == 'CatBoost':
            params['iterations'] = self.refine_catboost_iterations.get()
            params['learning_rate'] = self.refine_catboost_learning_rate.get()
            params['depth'] = self.refine_catboost_depth.get()
            params['l2_leaf_reg'] = self.refine_catboost_l2_leaf_reg.get()
            params['border_count'] = self.refine_catboost_border_count.get()
            params['bagging_temperature'] = self.refine_catboost_bagging_temperature.get()
            params['random_strength'] = self.refine_catboost_random_strength.get()

        # ========== SVR / SVM ==========
        elif model_name == 'SVR' or model_name == 'SVM':
            # C is a DoubleVar, already returns float
            params['C'] = self.refine_svr_C.get()

            params['kernel'] = self.refine_svr_kernel.get()

            gamma_val = self.refine_svr_gamma.get()
            # Handle 'scale' and 'auto' as strings, numeric values as floats
            if gamma_val in ['scale', 'auto']:
                params['gamma'] = gamma_val
            else:
                try:
                    params['gamma'] = float(gamma_val)
                except ValueError:
                    print(f"WARNING: Invalid SVR/SVM gamma '{gamma_val}', using default")

            # epsilon, coef0 are DoubleVars, already return floats
            params['epsilon'] = self.refine_svr_epsilon.get()

            # degree is IntVar, already returns int
            params['degree'] = self.refine_svr_degree.get()
            params['coef0'] = self.refine_svr_coef0.get()
            params['shrinking'] = self.refine_svr_shrinking.get()

        # ========== MLP ==========
        elif model_name == 'MLP':
            # Hidden layer sizes - special parsing for tuple format
            hidden_layers_str = self.refine_mlp_hidden_layer_sizes.get()
            hidden_layers = parse_hidden_layer_sizes(hidden_layers_str)
            if hidden_layers is not None:
                params['hidden_layer_sizes'] = hidden_layers

            params['activation'] = self.refine_mlp_activation.get()
            params['solver'] = self.refine_mlp_solver.get()

            # Alpha (L2 regularization)
            alpha_str = self.refine_mlp_alpha.get().strip()
            try:
                params['alpha'] = float(alpha_str)
            except ValueError:
                print(f"WARNING: Invalid MLP alpha '{alpha_str}', using default")

            params['max_iter'] = self.refine_mlp_max_iter.get()

            # Learning rate init
            lr_init_str = self.refine_mlp_learning_rate_init.get().strip()
            try:
                params['learning_rate_init'] = float(lr_init_str)
            except ValueError:
                print(f"WARNING: Invalid MLP learning_rate_init '{lr_init_str}', using default")

            # Batch size - handle 'auto' or numeric
            batch_size_val = self.refine_mlp_batch_size.get()
            if batch_size_val == 'auto':
                params['batch_size'] = 'auto'
            else:
                try:
                    params['batch_size'] = int(batch_size_val)
                except ValueError:
                    print(f"WARNING: Invalid MLP batch_size '{batch_size_val}', using default")

            params['learning_rate'] = self.refine_mlp_learning_rate.get()
            params['momentum'] = self.refine_mlp_momentum.get()

        # ========== NeuralBoosted ==========
        elif model_name == 'NeuralBoosted':
            params['n_estimators'] = self.refine_neuralboosted_n_estimators.get()
            params['learning_rate'] = self.refine_neuralboosted_learning_rate.get()

            # Hidden layer sizes - special parsing for tuple format
            hidden_layers_str = self.refine_neuralboosted_hidden_layer_size.get()
            hidden_layers = parse_hidden_layer_sizes(hidden_layers_str)
            if hidden_layers is not None:
                params['hidden_layer_size'] = hidden_layers

            params['activation'] = self.refine_neuralboosted_activation.get()
            params['early_stopping'] = self.refine_neuralboosted_early_stopping.get()

        return params

    def _estimate_grid_size(self, param_dict):
        """
        Calculate total grid size from parameter dictionary.

        Args:
            param_dict: Dict of {param_name: [value_list]}

        Returns:
            int: Total number of configurations (product of all list lengths)

        Example:
            {'n_estimators': [100, 200], 'max_depth': [10, 20, 30]}
            -> 2 * 3 = 6 configurations
        """
        if not param_dict:
            return 1

        total_size = 1

        for param_name, param_values in param_dict.items():
            # Handle nested parameter structures (e.g., PLS-DA with two-stage models)
            if isinstance(param_values, dict):
                # Recursively calculate size for nested dictionaries
                nested_size = self._estimate_grid_size(param_values)
                total_size *= nested_size
            elif isinstance(param_values, list):
                # Empty lists count as 1 (default value will be used)
                if len(param_values) == 0:
                    list_size = 1
                else:
                    list_size = len(param_values)
                total_size *= list_size
            else:
                # Single values (None, int, float, str) count as 1
                total_size *= 1

        return total_size

    def _validate_parameter_bounds(self, param_name, values, min_val=None,
                                   max_val=None, allowed_types=None):
        """
        Validate parameter values are within acceptable bounds.

        Args:
            param_name: Parameter name for error messages
            values: List of values to validate
            min_val: Minimum allowed value (optional)
            max_val: Maximum allowed value (optional)
            allowed_types: List of allowed types (optional)

        Returns:
            tuple: (is_valid: bool, error_message: str or None)

        Example:
            _validate_parameter_bounds('n_estimators', [50, 100, 200],
                                       min_val=1, max_val=10000)
            -> (True, None)

            _validate_parameter_bounds('n_estimators', [0, -5], min_val=1)
            -> (False, "n_estimators values must be >= 1")
        """
        # Handle non-list inputs
        if not isinstance(values, list):
            values = [values]

        # Skip validation for empty lists
        if len(values) == 0:
            return (True, None)

        # Type checking
        if allowed_types is not None:
            for value in values:
                # None is a special case - it's often allowed
                if value is None:
                    continue

                value_type = type(value)
                if value_type not in allowed_types:
                    type_names = ', '.join([t.__name__ for t in allowed_types])
                    return (False, f"{param_name} must be of type(s): {type_names}. Got {value_type.__name__}.")

        # Range validation (only for numeric types)
        for value in values:
            # Skip None values for range checking
            if value is None:
                continue

            # Check if value is numeric
            if not isinstance(value, (int, float)):
                continue

            if min_val is not None and value < min_val:
                return (False, f"{param_name} values must be >= {min_val}. Got {value}.")

            if max_val is not None and value > max_val:
                return (False, f"{param_name} values must be <= {max_val}. Got {value}.")

        return (True, None)

    def _check_grid_size_warning(self, grid_size, threshold=1000):
        """
        Check if grid size exceeds warning threshold.

        Args:
            grid_size: Estimated number of configurations
            threshold: Warning threshold (default: 1000)

        Returns:
            tuple: (show_warning: bool, warning_message: str or None)
        """
        if grid_size < 100:
            # No warning needed
            return (False, None)

        elif grid_size < 1000:
            # Info message
            message = (f"Grid search will evaluate {grid_size} configurations.\n"
                      f"This should complete in a reasonable time.")
            return (True, message)

        elif grid_size < 10000:
            # Warning message
            message = (f"Grid search will evaluate {grid_size} configurations.\n"
                      f"This may take a considerable amount of time.\n"
                      f"Consider reducing the parameter grid size.")
            return (True, message)

        else:
            # Strong warning message
            message = (f"WARNING: Grid search will evaluate {grid_size} configurations!\n"
                      f"This will likely take a very long time to complete.\n"
                      f"It is strongly recommended to reduce the parameter grid size.\n"
                      f"Consider using random search or Bayesian optimization instead.")
            return (True, message)

    def _populate_refine_hyperparams(self, model_name, params_dict):
        """
        Populate Model Development tab hyperparameter widgets from saved parameters.

        This is the reverse of _collect_refine_hyperparams() - it takes a dictionary
        of parameters and populates the appropriate UI widgets.

        Args:
            model_name: Name of the model (e.g., 'PLS', 'Ridge', 'XGBoost')
            params_dict: Dictionary of hyperparameters {param_name: value}
        """
        if not params_dict:
            print(f"DEBUG: No hyperparameters to populate for {model_name}")
            return

        print(f"\n{'='*80}")
        print(f"POPULATING UI HYPERPARAMETERS FOR {model_name}")
        print(f"{'='*80}")
        print(f"Loading {len(params_dict)} parameters from saved model:")
        for key, val in sorted(params_dict.items()):
            print(f"  {key}: {val}")
        print(f"{'='*80}\n")

        try:
            # ========== PLS ==========
            if model_name == 'PLS' or model_name == 'PLS-DA':
                if 'n_components' in params_dict:
                    self.refine_pls_n_components.set(int(params_dict['n_components']))
                if 'max_iter' in params_dict:
                    self.refine_pls_max_iter.set(int(params_dict['max_iter']))
                if 'tol' in params_dict:
                    self.refine_pls_tol.set(str(params_dict['tol']))

            # ========== Ridge ==========
            elif model_name == 'Ridge':
                if 'alpha' in params_dict:
                    self.refine_ridge_alpha.set(str(params_dict['alpha']))
                if 'solver' in params_dict:
                    self.refine_ridge_solver.set(str(params_dict['solver']))
                if 'tol' in params_dict:
                    self.refine_ridge_tol.set(str(params_dict['tol']))

            # ========== Lasso ==========
            elif model_name == 'Lasso':
                if 'alpha' in params_dict:
                    self.refine_lasso_alpha.set(str(params_dict['alpha']))
                if 'selection' in params_dict:
                    self.refine_lasso_selection.set(str(params_dict['selection']))
                if 'tol' in params_dict:
                    self.refine_lasso_tol.set(str(params_dict['tol']))
                if 'max_iter' in params_dict:
                    self.refine_lasso_max_iter.set(int(params_dict['max_iter']))

            # ========== ElasticNet ==========
            elif model_name == 'ElasticNet':
                if 'alpha' in params_dict:
                    self.refine_elasticnet_alpha.set(str(params_dict['alpha']))
                if 'l1_ratio' in params_dict:
                    self.refine_elasticnet_l1_ratio.set(str(params_dict['l1_ratio']))
                if 'selection' in params_dict:
                    self.refine_elasticnet_selection.set(str(params_dict['selection']))
                if 'tol' in params_dict:
                    self.refine_elasticnet_tol.set(str(params_dict['tol']))
                if 'max_iter' in params_dict:
                    self.refine_elasticnet_max_iter.set(int(params_dict['max_iter']))

            # ========== RandomForest ==========
            elif model_name == 'RandomForest':
                if 'n_estimators' in params_dict:
                    self.refine_rf_n_estimators.set(int(params_dict['n_estimators']))
                if 'max_depth' in params_dict:
                    val = params_dict['max_depth']
                    self.refine_rf_max_depth.set('None' if val is None else str(val))
                if 'min_samples_split' in params_dict:
                    self.refine_rf_min_samples_split.set(int(params_dict['min_samples_split']))
                if 'min_samples_leaf' in params_dict:
                    self.refine_rf_min_samples_leaf.set(int(params_dict['min_samples_leaf']))
                if 'max_features' in params_dict:
                    self.refine_rf_max_features.set(str(params_dict['max_features']))
                if 'bootstrap' in params_dict:
                    self.refine_rf_bootstrap.set(bool(params_dict['bootstrap']))
                if 'max_leaf_nodes' in params_dict:
                    val = params_dict['max_leaf_nodes']
                    self.refine_rf_max_leaf_nodes.set('None' if val is None else str(val))
                if 'min_impurity_decrease' in params_dict:
                    self.refine_rf_min_impurity_decrease.set(float(params_dict['min_impurity_decrease']))

            # ========== XGBoost ==========
            elif model_name == 'XGBoost':
                if 'n_estimators' in params_dict:
                    self.refine_xgb_n_estimators.set(int(params_dict['n_estimators']))
                if 'learning_rate' in params_dict:
                    self.refine_xgb_learning_rate.set(float(params_dict['learning_rate']))
                if 'max_depth' in params_dict:
                    self.refine_xgb_max_depth.set(int(params_dict['max_depth']))
                if 'subsample' in params_dict:
                    self.refine_xgb_subsample.set(float(params_dict['subsample']))
                if 'colsample_bytree' in params_dict:
                    self.refine_xgb_colsample_bytree.set(float(params_dict['colsample_bytree']))
                if 'reg_alpha' in params_dict:
                    self.refine_xgb_reg_alpha.set(float(params_dict['reg_alpha']))
                if 'reg_lambda' in params_dict:
                    self.refine_xgb_reg_lambda.set(float(params_dict['reg_lambda']))
                if 'min_child_weight' in params_dict:
                    self.refine_xgb_min_child_weight.set(int(params_dict['min_child_weight']))
                if 'gamma' in params_dict:
                    self.refine_xgb_gamma.set(float(params_dict['gamma']))

            # ========== LightGBM ==========
            elif model_name == 'LightGBM':
                if 'n_estimators' in params_dict:
                    self.refine_lgbm_n_estimators.set(int(params_dict['n_estimators']))
                if 'learning_rate' in params_dict:
                    self.refine_lgbm_learning_rate.set(float(params_dict['learning_rate']))
                if 'num_leaves' in params_dict:
                    self.refine_lgbm_num_leaves.set(int(params_dict['num_leaves']))
                if 'max_depth' in params_dict:
                    self.refine_lgbm_max_depth.set(int(params_dict['max_depth']))
                if 'min_child_samples' in params_dict:
                    self.refine_lgbm_min_child_samples.set(int(params_dict['min_child_samples']))
                if 'subsample' in params_dict:
                    self.refine_lgbm_subsample.set(float(params_dict['subsample']))
                if 'colsample_bytree' in params_dict:
                    self.refine_lgbm_colsample_bytree.set(float(params_dict['colsample_bytree']))
                if 'reg_alpha' in params_dict:
                    self.refine_lgbm_reg_alpha.set(float(params_dict['reg_alpha']))
                if 'reg_lambda' in params_dict:
                    self.refine_lgbm_reg_lambda.set(float(params_dict['reg_lambda']))

            # ========== CatBoost ==========
            elif model_name == 'CatBoost':
                if 'iterations' in params_dict:
                    self.refine_catboost_iterations.set(int(params_dict['iterations']))
                if 'learning_rate' in params_dict:
                    self.refine_catboost_learning_rate.set(float(params_dict['learning_rate']))
                if 'depth' in params_dict:
                    self.refine_catboost_depth.set(int(params_dict['depth']))
                if 'l2_leaf_reg' in params_dict:
                    self.refine_catboost_l2_leaf_reg.set(float(params_dict['l2_leaf_reg']))
                if 'border_count' in params_dict:
                    self.refine_catboost_border_count.set(int(params_dict['border_count']))
                if 'bagging_temperature' in params_dict:
                    self.refine_catboost_bagging_temperature.set(float(params_dict['bagging_temperature']))
                if 'random_strength' in params_dict:
                    self.refine_catboost_random_strength.set(float(params_dict['random_strength']))

            # ========== SVR / SVM ==========
            elif model_name == 'SVR' or model_name == 'SVM':
                if 'C' in params_dict:
                    self.refine_svr_C.set(str(params_dict['C']))
                if 'kernel' in params_dict:
                    self.refine_svr_kernel.set(str(params_dict['kernel']))
                if 'gamma' in params_dict:
                    self.refine_svr_gamma.set(str(params_dict['gamma']))
                if 'epsilon' in params_dict:
                    self.refine_svr_epsilon.set(float(params_dict['epsilon']))
                if 'degree' in params_dict:
                    self.refine_svr_degree.set(int(params_dict['degree']))
                if 'coef0' in params_dict:
                    self.refine_svr_coef0.set(float(params_dict['coef0']))
                if 'shrinking' in params_dict:
                    self.refine_svr_shrinking.set(bool(params_dict['shrinking']))

            # ========== MLP ==========
            elif model_name == 'MLP':
                if 'hidden_layer_sizes' in params_dict:
                    val = params_dict['hidden_layer_sizes']
                    # Convert tuple to string format
                    if isinstance(val, tuple):
                        if len(val) == 1:
                            self.refine_mlp_hidden_layer_sizes.set(f"({val[0]},)")
                        else:
                            self.refine_mlp_hidden_layer_sizes.set(str(val))
                    else:
                        self.refine_mlp_hidden_layer_sizes.set(str(val))
                if 'activation' in params_dict:
                    self.refine_mlp_activation.set(str(params_dict['activation']))
                if 'solver' in params_dict:
                    self.refine_mlp_solver.set(str(params_dict['solver']))
                if 'alpha' in params_dict:
                    self.refine_mlp_alpha.set(float(params_dict['alpha']))
                if 'learning_rate_init' in params_dict:
                    self.refine_mlp_learning_rate_init.set(str(params_dict['learning_rate_init']))
                if 'batch_size' in params_dict:
                    self.refine_mlp_batch_size.set(str(params_dict['batch_size']))
                if 'learning_rate' in params_dict:
                    self.refine_mlp_learning_rate.set(str(params_dict['learning_rate']))
                if 'momentum' in params_dict:
                    self.refine_mlp_momentum.set(float(params_dict['momentum']))
                if 'max_iter' in params_dict:
                    self.refine_mlp_max_iter.set(int(params_dict['max_iter']))

            # ========== NeuralBoosted ==========
            elif model_name == 'NeuralBoosted':
                if 'n_estimators' in params_dict:
                    self.refine_neuralboosted_n_estimators.set(int(params_dict['n_estimators']))
                if 'learning_rate' in params_dict:
                    self.refine_neuralboosted_learning_rate.set(float(params_dict['learning_rate']))
                # Check for both singular and plural naming (backwards compatibility)
                if 'hidden_layer_size' in params_dict:
                    val = params_dict['hidden_layer_size']
                    # NeuralBoosted uses hidden_layer_size (singular) - convert from tuple to int
                    if isinstance(val, tuple) and len(val) > 0:
                        self.refine_neuralboosted_hidden_layer_size.set(int(val[0]))
                    elif isinstance(val, int):
                        self.refine_neuralboosted_hidden_layer_size.set(int(val))
                elif 'hidden_layer_sizes' in params_dict:
                    val = params_dict['hidden_layer_sizes']
                    # Backwards compatibility with old plural naming
                    if isinstance(val, tuple) and len(val) > 0:
                        self.refine_neuralboosted_hidden_layer_size.set(int(val[0]))
                    elif isinstance(val, int):
                        self.refine_neuralboosted_hidden_layer_size.set(int(val))
                if 'activation' in params_dict:
                    self.refine_neuralboosted_activation.set(str(params_dict['activation']))
                if 'early_stopping' in params_dict:
                    self.refine_neuralboosted_early_stopping.set(bool(params_dict['early_stopping']))

            print(f"✓ Successfully populated {model_name} hyperparameters in UI")

        except Exception as e:
            print(f"WARNING: Error populating hyperparameters for {model_name}: {e}")
            import traceback
            traceback.print_exc()

    def _load_model_for_refinement(self, config):
        """Load a model configuration into the Model Development tab."""
        # Store config for use in _run_refined_model_thread
        self.loaded_model_config = config

        # Validate data availability
        if not self._validate_data_for_refinement():
            return

        # CRITICAL: Restore validation indices BEFORE validation check
        # This ensures the check sees the correct validation state
        if 'validation_indices' in config and config.get('validation_set_enabled'):
            self.validation_indices = set(config.get('validation_indices', []))
            self.validation_enabled.set(True)
            print(f"✓ Restored {len(self.validation_indices)} validation indices from model config")
        else:
            # Clear validation if not used in original model
            self.validation_indices = set()
            self.validation_enabled.set(False)
            print("✓ No validation indices to restore (model was trained on all data)")

        # CRITICAL: Also restore excluded spectra if available
        # This ensures excluded count matches for validation check
        if 'excluded_spectra' in config:
            self.excluded_spectra = set(config.get('excluded_spectra', []))
            if len(self.excluded_spectra) > 0:
                print(f"✓ Restored {len(self.excluded_spectra)} excluded samples from model config")
        else:
            # Don't clear excluded_spectra - user may have manually excluded samples in Data Upload tab
            # Only update if explicitly provided in config
            pass

        # Check for training configuration mismatch
        if 'training_config' in config:
            self._validate_training_configuration(config['training_config'])
        else:
            # Legacy model without training configuration
            print("\n⚠️  Note: This model was saved before training configuration tracking was added.")
            print("    Cannot verify if current dataset state matches original training state.")

        # Build configuration text
        info_text = f"""Model: {config.get('Model', 'N/A')}
Preprocessing: {config.get('Preprocess', 'N/A')}
Subset: {config.get('SubsetTag', config.get('Subset', 'N/A'))}
Window Size: {config.get('Window', 'N/A')}
Imbalance Handling: {config.get('Imbalance', '—')}
"""

        # Add performance metrics
        if 'RMSE' in config and not pd.isna(config.get('RMSE')):
            # Regression
            info_text += f"""
Performance (Regression):
  RMSE: {config.get('RMSE', 'N/A')}
  R²: {config.get('R2', 'N/A')}
"""
        elif 'Accuracy' in config and not pd.isna(config.get('Accuracy')):
            # Classification
            info_text += f"""
Performance (Classification):
  Accuracy: {config.get('Accuracy', 'N/A')}
"""
            if 'ROC_AUC' in config and not pd.isna(config['ROC_AUC']):
                info_text += f"  ROC AUC: {config.get('ROC_AUC', 'N/A')}\n"

        # Add wavelength information
        n_vars = config.get('n_vars', 'N/A')
        full_vars = config.get('full_vars', 'N/A')
        subset_tag = config.get('SubsetTag', config.get('Subset', 'full'))

        info_text += f"\nWavelengths: {n_vars} of {full_vars} used"
        if subset_tag != 'full' and subset_tag != 'N/A':
            info_text += f" ({subset_tag})"
        info_text += "\n"

        if 'top_vars' in config and config['top_vars'] != 'N/A':
            top_vars_list = config['top_vars'].split(',')
            n_shown = len(top_vars_list)
            if n_shown < n_vars and n_vars != 'N/A':
                info_text += f"Most important {n_shown} wavelengths shown below (model used {n_vars} total):\n"
            else:
                info_text += f"Wavelengths used:\n"
            info_text += f"  {config['top_vars']}\n"

        # Add hyperparameters if available
        if 'n_estimators' in config and not pd.isna(config['n_estimators']):
            info_text += f"\nHyperparameters:\n"
            info_text += f"  n_estimators: {config.get('n_estimators', 'N/A')}\n"
            if 'learning_rate' in config:
                info_text += f"  learning_rate: {config.get('learning_rate', 'N/A')}\n"

        # Update the info text widget
        self.refine_model_info.config(state='normal')
        self.refine_model_info.delete('1.0', tk.END)
        self.refine_model_info.insert('1.0', info_text)
        self.refine_model_info.config(state='disabled')

        # Populate refinement parameters with current values
        # Populate wavelength specification - Show ONLY wavelengths used in the selected model
        print(f"DEBUG: X_original is None? {self.X_original is None}")

        if self.X_original is not None:
            try:
                print(f"DEBUG: X_original shape: {self.X_original.shape}")

                # Extract all available wavelengths
                all_wavelengths = self.X_original.columns.astype(float).values
                print(f"DEBUG: Total available wavelengths: {len(all_wavelengths)}")

                subset_tag = config.get('SubsetTag', config.get('Subset', 'full'))
                n_vars = config.get('n_vars', len(all_wavelengths))

                # Determine which wavelengths to show
                model_wavelengths = None

                # For subset models: Prefer all_vars (complete list) over top_vars (top 30)
                # This fixes the variable count mismatch for models with >30 variables
                if 'all_vars' in config and config['all_vars'] != 'N/A' and config['all_vars']:
                    print(f"DEBUG: Model has all_vars, parsing complete wavelength list")
                    try:
                        # Parse wavelengths from all_vars string (e.g., "1520.0, 1540.0, 1560.0, ...")
                        all_vars_str = str(config['all_vars']).strip()
                        wavelength_strings = [w.strip() for w in all_vars_str.split(',')]
                        model_wavelengths = [float(w) for w in wavelength_strings if w]
                        model_wavelengths = sorted(model_wavelengths)  # Sort for formatting
                        print(f"DEBUG: Parsed {len(model_wavelengths)} wavelengths from all_vars")
                    except Exception as e:
                        print(f"WARNING: Could not parse all_vars: {e}")
                        model_wavelengths = None

                # Fallback to top_vars for backward compatibility with old results
                if model_wavelengths is None and 'top_vars' in config and config['top_vars'] != 'N/A' and config['top_vars']:
                    model_name = config.get('Model', 'Unknown')
                    print(f"⚠️  CRITICAL WARNING: Model '{model_name}' missing complete wavelength list ('all_vars')")
                    print(f"⚠️  Falling back to 'top_vars' - this may cause R² mismatch if model used >30 wavelengths!")
                    print(f"⚠️  Expected n_vars: {config.get('n_vars', 'unknown')}")
                    try:
                        # Parse wavelengths from top_vars string (e.g., "1520.0, 1540.0, 1560.0")
                        top_vars_str = str(config['top_vars']).strip()
                        wavelength_strings = [w.strip() for w in top_vars_str.split(',')]
                        model_wavelengths = [float(w) for w in wavelength_strings if w]
                        model_wavelengths = sorted(model_wavelengths)  # Sort for formatting
                        expected_n_vars = config.get('n_vars', len(model_wavelengths))
                        if len(model_wavelengths) < expected_n_vars:
                            print(f"⚠️  MISMATCH: Loaded {len(model_wavelengths)} wavelengths but model expects {expected_n_vars}!")
                            print(f"⚠️  This WILL cause different R² when running refined model!")
                        print(f"DEBUG: Parsed {len(model_wavelengths)} wavelengths from top_vars")
                    except Exception as e:
                        print(f"WARNING: Could not parse top_vars: {e}")
                        model_wavelengths = None

                # For full models or if parsing failed: Use all wavelengths
                if model_wavelengths is None:
                    if subset_tag == 'full' or subset_tag == 'N/A':
                        print(f"DEBUG: Full model - using all {len(all_wavelengths)} wavelengths")
                        model_wavelengths = list(all_wavelengths)
                    else:
                        # Fallback: use all wavelengths
                        print(f"WARNING: Subset model but no top_vars, using all wavelengths")
                        model_wavelengths = list(all_wavelengths)

                # Format the wavelengths (ranges for consecutive, individual otherwise)
                wl_spec = self._format_wavelengths_as_spec(model_wavelengths)
                print(f"DEBUG: Formatted {len(model_wavelengths)} wavelengths into {len(wl_spec)} character spec")

                # FALLBACK: If formatter returns empty, use simple range
                if not wl_spec or len(wl_spec) == 0:
                    print("WARNING: Formatter returned empty, using fallback")
                    if len(model_wavelengths) > 0:
                        wl_spec = f"{model_wavelengths[0]:.1f}-{model_wavelengths[-1]:.1f}"
                    else:
                        wl_spec = "# ERROR: No wavelengths available"

                # Ensure widget is in normal state before editing
                self.refine_wl_spec.config(state='normal')
                self.refine_wl_spec.delete('1.0', 'end')
                self.refine_wl_spec.insert('1.0', wl_spec)

                # Verify insertion
                content = self.refine_wl_spec.get('1.0', 'end-1c')
                print(f"DEBUG: Text widget content length: {len(content)} characters")

                if len(content) == 0:
                    print("ERROR: Text widget is empty after insertion!")

            except Exception as e:
                print(f"ERROR loading wavelengths: {e}")
                import traceback
                traceback.print_exc()

                # Show error to user in the text widget
                self.refine_wl_spec.config(state='normal')
                self.refine_wl_spec.delete('1.0', 'end')
                self.refine_wl_spec.insert('1.0', f"# ERROR: Could not load wavelengths\n# {str(e)}\n# Please check console for details")

        else:
            print("WARNING: X_original is None - data not loaded")
            self.refine_wl_spec.config(state='normal')
            self.refine_wl_spec.delete('1.0', 'end')
            self.refine_wl_spec.insert('1.0', "# ERROR: Data not loaded\n# Please load data in Data Upload tab first")

        # Set window size
        window = config.get('Window', 17)
        if not pd.isna(window) and window in [7, 11, 17, 19]:
            self.refine_window.set(int(window))

        # Set task type FIRST (auto-detect from data) - CRITICAL FIX for PLS-DA loading
        # This must happen before model validation so we validate against the correct task type
        if self.y is not None:
            if self.y.nunique() == 2 or self.y.dtype == 'object' or self.y.nunique() < 10:
                detected_task_type = 'classification'
                self.refine_task_type.set('classification')
            else:
                detected_task_type = 'regression'
                self.refine_task_type.set('regression')
        else:
            # Fallback to regression if no y data available
            detected_task_type = 'regression'
            self.refine_task_type.set('regression')

        # Set model type with validation (now using correct task type)
        model_name = config.get('Model', 'PLS')
        # Use central model registry for validation
        if is_valid_model is not None:
            # Validate against detected task type (not hardcoded 'regression')
            if is_valid_model(model_name, detected_task_type):
                self.refine_model_type.set(model_name)
                print(f"✓ Model type '{model_name}' validated and loaded for {detected_task_type}")
            else:
                valid_models = get_supported_models(detected_task_type) if get_supported_models is not None else []
                default_model = 'PLS-DA' if detected_task_type == 'classification' else 'PLS'
                print(f"⚠️  WARNING: Unknown model type '{model_name}' for {detected_task_type} - defaulting to {default_model}")
                print(f"⚠️  Valid models: {', '.join(valid_models)}")
                self.refine_model_type.set(default_model)
        else:
            # Fallback if registry import failed
            valid_models_regression = ['PLS', 'Ridge', 'Lasso', 'ElasticNet', 'RandomForest', 'MLP', 'NeuralBoosted', 'SVR', 'XGBoost', 'LightGBM', 'CatBoost']
            valid_models_classification = ['PLS-DA', 'RandomForest', 'MLP', 'NeuralBoosted', 'SVM', 'XGBoost', 'LightGBM', 'CatBoost']
            valid_models = valid_models_classification if detected_task_type == 'classification' else valid_models_regression

            if model_name in valid_models or (model_name == 'PLS' and detected_task_type == 'classification'):
                # Allow 'PLS' to map to 'PLS-DA' for classification
                if model_name == 'PLS' and detected_task_type == 'classification':
                    model_name = 'PLS-DA'
                self.refine_model_type.set(model_name)
                print(f"✓ Model type '{model_name}' validated and loaded for {detected_task_type}")
            else:
                default_model = 'PLS-DA' if detected_task_type == 'classification' else 'PLS'
                print(f"⚠️  WARNING: Unknown model type '{model_name}' for {detected_task_type} - defaulting to {default_model}")
                print(f"⚠️  Valid models: {', '.join(valid_models)}")
                self.refine_model_type.set(default_model)

        # Set preprocessing method
        preprocess = config.get('Preprocess', 'raw')
        deriv = config.get('Deriv', None)

        # Convert from search.py naming to GUI naming
        if preprocess == 'deriv' and deriv == 1:
            gui_preprocess = 'sg1'
        elif preprocess == 'deriv' and deriv == 2:
            gui_preprocess = 'sg2'
        elif preprocess == 'snv_deriv':
            # SNV then derivative - NOW PROPERLY SUPPORTED!
            gui_preprocess = 'snv_sg1' if deriv == 1 else 'snv_sg2'
        elif preprocess == 'deriv_snv':
            gui_preprocess = 'deriv_snv'
        elif preprocess in ['raw', 'snv']:
            gui_preprocess = preprocess
        else:
            gui_preprocess = 'raw'  # Fallback

        if gui_preprocess in ['raw', 'snv', 'sg1', 'sg2', 'snv_sg1', 'snv_sg2', 'deriv_snv']:
            self.refine_preprocess.set(gui_preprocess)

        # Enable the run buttons (both Configuration and Selection tabs)
        self.refine_run_button.config(state='normal')
        self.refine_run_button_selection.config(state='normal')
        self.refine_status.config(text=f"Loaded: {config.get('Model', 'N/A')} | {config.get('Preprocess', 'N/A')}")

        # Update mode label to indicate loaded from results
        rank = config.get('Rank', 'N/A')
        self.refine_mode_label.config(text=f"Mode: Loaded from Results (Rank {rank})")

        # Populate hyperparameters from saved model config
        # This allows users to see and modify the exact hyperparameters used in the selected model
        model_name = config.get('Model', 'Unknown')
        if 'Params' in config and config['Params']:
            try:
                import ast
                raw_params = config.get('Params')
                if isinstance(raw_params, str) and raw_params.strip():
                    # Parse the params string (e.g., "{'n_estimators': 100, 'learning_rate': 0.1}")
                    params_dict = ast.literal_eval(raw_params)
                    if isinstance(params_dict, dict):
                        print(f"\n{'='*80}")
                        print(f"AUTO-POPULATING HYPERPARAMETERS FROM RESULTS TAB")
                        print(f"{'='*80}")
                        print(f"Model: {model_name}")
                        print(f"Rank: {rank}")
                        print(f"Loaded {len(params_dict)} hyperparameters from saved model")
                        print(f"{'='*80}\n")

                        # Populate the UI widgets with saved parameters
                        self._populate_refine_hyperparams(model_name, params_dict)
                    else:
                        print(f"DEBUG: Params is not a dict: {type(params_dict)}")
                else:
                    print(f"DEBUG: No valid Params string to parse")
            except (ValueError, SyntaxError) as e:
                print(f"WARNING: Could not parse saved hyperparameters: {e}")
            except Exception as e:
                print(f"WARNING: Error loading hyperparameters: {e}")
                import traceback
                traceback.print_exc()
        else:
            print(f"DEBUG: No 'Params' field in config for {model_name}")

        # Update the wavelength count display
        self._update_wavelength_count()

    def _plot_refined_predictions(self):
        """Plot reference vs predicted - handles both regression and classification."""
        if not HAS_MATPLOTLIB:
            return

        task_type = self.refined_config.get('task_type', 'regression') if hasattr(self, 'refined_config') and self.refined_config else 'regression'

        if task_type == 'classification':
            self._plot_classification_predictions()
        else:
            self._plot_regression_predictions()

    def _plot_regression_predictions(self):
        """Plot scatter of reference vs predicted for regression."""
        if not hasattr(self, 'refined_y_true') or not hasattr(self, 'refined_y_pred'):
            return

        # Clear existing plot
        for widget in self.refine_plot_frame.winfo_children():
            widget.destroy()

        # Add control frame for color selection
        control_frame = ttk.Frame(self.refine_plot_frame)
        control_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)

        ttk.Label(control_frame, text="Color by:", style='TLabel').pack(side='left', padx=5)

        color_options = self._get_available_color_variables()
        color_combo = ttk.Combobox(control_frame,
                                   textvariable=self.regression_pred_color_var,
                                   values=color_options,
                                   width=20,
                                   state='readonly')
        color_combo.pack(side='left', padx=5)
        color_combo.bind('<<ComboboxSelected>>', lambda e: self._plot_regression_predictions())

        # Create plot frame
        plot_frame = ttk.Frame(self.refine_plot_frame)
        plot_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Create figure
        fig = Figure(figsize=(8, 6))
        ax = fig.add_subplot(111)

        y_true = self.refined_y_true
        y_pred = self.refined_y_pred

        # Get color variable selection
        color_by = self.regression_pred_color_var.get()

        # Determine coloring values - need to align with CV indices
        if color_by == 'Y Value':
            color_values = y_true.copy()
            is_categorical = self._is_categorical_target()
            color_label = 'Y Value'
        elif color_by == 'None':
            color_values = None
            is_categorical = False
            color_label = None
        else:
            # Metadata column - need to map using refined_cv_indices
            if self.ref is not None and color_by in self.ref.columns and hasattr(self, 'refined_cv_indices'):
                # Get specimen IDs for CV samples
                # CRITICAL FIX: Use the stored specimen IDs that match the CV data
                if hasattr(self, 'refined_specimen_ids'):
                    cv_specimen_ids = self.refined_specimen_ids
                else:
                    # Fallback to old behavior (might be incorrect with validation set)
                    cv_specimen_ids = self.y.index[self.refined_cv_indices]

                # Align metadata with these specimen IDs
                try:
                    color_values = self.ref.loc[cv_specimen_ids, color_by].values
                    is_categorical = self._is_categorical_variable(color_by, color_values)
                    color_label = color_by
                except KeyError:
                    color_values = None
                    is_categorical = False
                    color_label = None
            else:
                color_values = None
                is_categorical = False
                color_label = None

        # Apply coloring
        if color_values is None:
            # No color - single color
            ax.scatter(y_true, y_pred, alpha=0.6, edgecolors='black', linewidths=0.5, s=50, color='steelblue')
        elif is_categorical:
            # Categorical coloring
            unique_vals = np.unique(color_values[pd.notna(color_values)])
            n_vals = len(unique_vals)

            if n_vals <= 10:
                colors = plt.cm.tab10(np.linspace(0, 1, 10))
            elif n_vals <= 20:
                colors = plt.cm.tab20(np.linspace(0, 1, 20))
            else:
                colors = plt.cm.tab20(np.linspace(0, 1, 20))

            color_map = {val: colors[i % len(colors)] for i, val in enumerate(unique_vals)}

            for val in unique_vals:
                mask = color_values == val
                if np.any(mask):
                    ax.scatter(y_true[mask], y_pred[mask],
                             c=[color_map[val]], alpha=0.6, edgecolors='black',
                             linewidths=0.5, label=str(val), s=50)

            # Handle NaN values
            nan_mask = pd.isna(color_values)
            if np.any(nan_mask):
                ax.scatter(y_true[nan_mask], y_pred[nan_mask],
                         c='lightgray', alpha=0.6, edgecolors='black',
                         linewidths=0.5, label='N/A', s=50)

            ax.legend(title=color_label, loc='upper left', fontsize=8, framealpha=0.9)
        else:
            # Continuous coloring
            # Convert to numeric if needed (handles string numeric values)
            numeric_color_values = pd.to_numeric(color_values, errors='coerce')
            valid_mask = pd.notna(numeric_color_values)
            if np.any(valid_mask):
                scatter = ax.scatter(y_true[valid_mask], y_pred[valid_mask],
                                   c=numeric_color_values[valid_mask], cmap='viridis',
                                   alpha=0.6, edgecolors='black', linewidths=0.5, s=50)
                fig.colorbar(scatter, ax=ax, label=color_label)

            nan_mask = ~valid_mask
            if np.any(nan_mask):
                ax.scatter(y_true[nan_mask], y_pred[nan_mask],
                         c='lightgray', alpha=0.6, edgecolors='black',
                         linewidths=0.5, s=50, label='N/A')
                ax.legend(loc='upper left', fontsize=8)

        # 1:1 line
        min_val = min(y_true.min(), y_pred.min())
        max_val = max(y_true.max(), y_pred.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='1:1 Line')

        # Calculate statistics
        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
        r2 = r2_score(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)

        # Add statistics text box
        stats_text = f'R² = {r2:.4f}\nRMSE = {rmse:.4f}\nMAE = {mae:.4f}\nn = {len(y_true)}'
        ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                fontsize=10, family='monospace')

        ax.set_xlabel('Reference Values', fontsize=11)
        ax.set_ylabel('Predicted Values', fontsize=11)
        ax.set_title('Cross-Validation: Reference vs Predicted', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        if color_values is None or not is_categorical:
            ax.legend(loc='lower right')

        fig.tight_layout()

        canvas = FigureCanvasTkAgg(fig, plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add click handler for point identification
        def on_prediction_click(event):
            if event.inaxes != ax or event.xdata is None or event.ydata is None:
                return

            # Find nearest point to click
            click_x, click_y = event.xdata, event.ydata
            distances = np.sqrt((y_true - click_x)**2 + (y_pred - click_y)**2)
            nearest_idx = np.argmin(distances)

            # Only show annotation if click is reasonably close
            x_range = ax.get_xlim()[1] - ax.get_xlim()[0]
            y_range = ax.get_ylim()[1] - ax.get_ylim()[0]
            threshold = 0.1 * np.sqrt(x_range**2 + y_range**2)

            if distances[nearest_idx] < threshold:
                # Map CV index back to original specimen index
                if self.refined_cv_indices is not None:
                    original_idx = self.refined_cv_indices[nearest_idx]
                    y_actual = y_true[nearest_idx]
                    y_predicted = y_pred[nearest_idx]
                    residual = y_actual - y_predicted

                    extra_info = {
                        'Residual': residual
                    }

                    # Add color variable to extra_info if it's a metadata column
                    if color_by not in ['None', 'Y Value'] and color_values is not None:
                        color_val = color_values[nearest_idx]
                        if pd.notna(color_val):
                            if isinstance(color_val, (float, np.floating)):
                                extra_info[color_by] = f"{color_val:.4f}"
                            else:
                                extra_info[color_by] = str(color_val)
                        else:
                            extra_info[color_by] = "N/A"

                    info_text = self._format_specimen_info(original_idx, y_value=y_actual,
                                                          y_pred=y_predicted, extra_info=extra_info)
                    self._create_or_update_annotation(ax, y_actual, y_predicted, info_text, canvas)

        fig.canvas.mpl_connect('button_press_event', on_prediction_click)

        # Add export button
        self._add_plot_export_button(self.refine_plot_frame, fig, "cv_predictions")

    def _plot_classification_predictions(self):
        """Plot confusion matrix for classification."""
        if self.refined_y_true is None or self.refined_y_pred is None:
            return

        # Clear existing plot
        for widget in self.refine_plot_frame.winfo_children():
            widget.destroy()

        # Create confusion matrix
        cm = confusion_matrix(self.refined_y_true, self.refined_y_pred)

        # Get class labels (handle label encoder if present)
        if self.label_encoder is not None:
            class_labels = self.label_encoder.classes_
        else:
            class_labels = np.unique(np.concatenate([self.refined_y_true, self.refined_y_pred]))

        # Create figure
        fig = Figure(figsize=(8, 6))
        ax = fig.add_subplot(111)

        # Plot confusion matrix as heatmap
        im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
        fig.colorbar(im, ax=ax)

        # Set ticks and labels
        tick_marks = np.arange(len(class_labels))
        ax.set_xticks(tick_marks)
        ax.set_yticks(tick_marks)
        ax.set_xticklabels(class_labels, rotation=45, ha='right')
        ax.set_yticklabels(class_labels)

        # Add text annotations
        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, format(cm[i, j], 'd'),
                       ha="center", va="center",
                       color="white" if cm[i, j] > thresh else "black",
                       fontsize=12, fontweight='bold')

        ax.set_xlabel('Predicted Label', fontsize=10, fontweight='bold')
        ax.set_ylabel('True Label', fontsize=10, fontweight='bold')
        ax.set_title('Confusion Matrix', fontsize=12, fontweight='bold')
        fig.tight_layout()

        # Add to GUI
        canvas = FigureCanvasTkAgg(fig, self.refine_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add export button
        self._add_plot_export_button(self.refine_plot_frame, fig, "confusion_matrix")

        # Calculate and display metrics
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

        accuracy = accuracy_score(self.refined_y_true, self.refined_y_pred)

        # Handle binary vs multi-class
        n_classes = len(class_labels)
        if n_classes == 2:
            precision = precision_score(self.refined_y_true, self.refined_y_pred, average='binary', zero_division=0)
            recall = recall_score(self.refined_y_true, self.refined_y_pred, average='binary', zero_division=0)
            f1 = f1_score(self.refined_y_true, self.refined_y_pred, average='binary', zero_division=0)
        else:
            precision = precision_score(self.refined_y_true, self.refined_y_pred, average='weighted', zero_division=0)
            recall = recall_score(self.refined_y_true, self.refined_y_pred, average='weighted', zero_division=0)
            f1 = f1_score(self.refined_y_true, self.refined_y_pred, average='weighted', zero_division=0)

        # Display metrics in text box
        metrics_text = f"""
═══════════════════════════════════════════════════
CLASSIFICATION METRICS
═══════════════════════════════════════════════════
Accuracy:  {accuracy:.4f}
Precision: {precision:.4f}
Recall:    {recall:.4f}
F1 Score:  {f1:.4f}
═══════════════════════════════════════════════════
"""

        # Update or create metrics display
        if hasattr(self, 'refine_metrics_text'):
            self.refine_metrics_text.delete('1.0', tk.END)
            self.refine_metrics_text.insert('1.0', metrics_text)

    def _plot_residual_diagnostics(self):
        """Plot diagnostics - residuals for regression, ROC curves for classification."""
        if not HAS_MATPLOTLIB:
            return

        task_type = self.refined_config.get('task_type', 'regression') if hasattr(self, 'refined_config') and self.refined_config else 'regression'

        if task_type == 'classification':
            self._plot_classification_roc_curves()
        else:
            self._plot_regression_residual_diagnostics()

    def _plot_regression_residual_diagnostics(self):
        """Plot residual diagnostics for regression."""
        if not hasattr(self, 'refined_y_true') or not hasattr(self, 'refined_y_pred'):
            return

        from spectral_predict.diagnostics import compute_residuals, qq_plot_data

        # Clear existing plot
        for widget in self.residual_diagnostics_frame.winfo_children():
            widget.destroy()

        # Add control frame for color selection
        control_frame = ttk.Frame(self.residual_diagnostics_frame)
        control_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)

        ttk.Label(control_frame, text="Color by:", style='TLabel').pack(side='left', padx=5)

        color_options = self._get_available_color_variables()
        color_combo = ttk.Combobox(control_frame,
                                   textvariable=self.residual_color_var,
                                   values=color_options,
                                   width=20,
                                   state='readonly')
        color_combo.pack(side='left', padx=5)
        color_combo.bind('<<ComboboxSelected>>', lambda e: self._plot_residual_diagnostics())

        # Create plot frame
        plot_container = ttk.Frame(self.residual_diagnostics_frame)
        plot_container.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        y_true = self.refined_y_true
        y_pred = self.refined_y_pred

        # Compute residuals
        residuals, std_residuals = compute_residuals(y_true, y_pred)

        # Get color variable selection
        color_by = self.residual_color_var.get()

        # Determine coloring values - align with CV indices
        if color_by == 'Y Value':
            color_values = y_true.copy()
            is_categorical = self._is_categorical_target()
            color_label = 'Y Value'
        elif color_by == 'None':
            color_values = None
            is_categorical = False
            color_label = None
        else:
            # Metadata column - align with CV indices
            if self.ref is not None and color_by in self.ref.columns and hasattr(self, 'refined_cv_indices'):
                cv_specimen_ids = self.y.index[self.refined_cv_indices]
                try:
                    color_values = self.ref.loc[cv_specimen_ids, color_by].values
                    is_categorical = self._is_categorical_variable(color_by, color_values)
                    color_label = color_by
                except KeyError:
                    color_values = None
                    is_categorical = False
                    color_label = None
            else:
                color_values = None
                is_categorical = False
                color_label = None

        # Create 1x3 subplot figure
        fig = Figure(figsize=(12, 4))

        # Plot 1: Residuals vs Fitted
        ax1 = fig.add_subplot(131)
        self._apply_color_scheme(ax1, fig, y_pred, residuals, color_by,
                                alpha=0.6, edgecolors='black', linewidths=0.5, s=40)
        ax1.axhline(y=0, color='r', linestyle='--', linewidth=2)
        ax1.set_xlabel('Fitted Values', fontsize=10)
        ax1.set_ylabel('Residuals', fontsize=10)
        ax1.set_title('Residuals vs Fitted', fontsize=11, fontweight='bold')
        ax1.grid(True, alpha=0.3)

        # Plot 2: Residuals vs Index
        ax2 = fig.add_subplot(132)
        indices = np.arange(len(residuals))
        self._apply_color_scheme(ax2, fig, indices, residuals, color_by,
                                alpha=0.6, edgecolors='black', linewidths=0.5, s=40)
        ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)
        ax2.set_xlabel('Sample Index', fontsize=10)
        ax2.set_ylabel('Residuals', fontsize=10)
        ax2.set_title('Residuals vs Index', fontsize=11, fontweight='bold')
        ax2.grid(True, alpha=0.3)

        # Plot 3: Q-Q Plot
        ax3 = fig.add_subplot(133)
        theoretical_q, sample_q = qq_plot_data(residuals)
        ax3.scatter(theoretical_q, sample_q, alpha=0.6, edgecolors='black', linewidths=0.5, s=40)

        # Add reference line
        min_q = min(theoretical_q.min(), sample_q.min())
        max_q = max(theoretical_q.max(), sample_q.max())
        ax3.plot([min_q, max_q], [min_q, max_q], 'r--', linewidth=2)

        ax3.set_xlabel('Theoretical Quantiles', fontsize=10)
        ax3.set_ylabel('Sample Quantiles', fontsize=10)
        ax3.set_title('Q-Q Plot (Normality)', fontsize=11, fontweight='bold')
        ax3.grid(True, alpha=0.3)

        fig.tight_layout()

        # Embed in tkinter
        canvas = FigureCanvasTkAgg(fig, plot_container)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add click handlers for point identification
        def on_residual_vs_fitted_click(event):
            if event.inaxes != ax1 or event.xdata is None or event.ydata is None:
                return

            # Find nearest point
            click_x, click_y = event.xdata, event.ydata
            distances = np.sqrt((y_pred - click_x)**2 + (residuals - click_y)**2)
            nearest_idx = np.argmin(distances)

            # Check if click is reasonably close
            x_range = ax1.get_xlim()[1] - ax1.get_xlim()[0]
            y_range = ax1.get_ylim()[1] - ax1.get_ylim()[0]
            threshold = 0.1 * np.sqrt(x_range**2 + y_range**2)

            if distances[nearest_idx] < threshold and self.refined_cv_indices is not None:
                original_idx = self.refined_cv_indices[nearest_idx]
                y_actual = y_true[nearest_idx]
                y_predicted = y_pred[nearest_idx]
                residual_val = residuals[nearest_idx]

                extra_info = {
                    'Fitted': y_predicted,
                    'Residual': residual_val
                }

                # Add color variable info if applicable
                if color_values is not None and color_label:
                    color_val = color_values[nearest_idx]
                    if pd.notna(color_val):
                        extra_info[color_label] = color_val
                    else:
                        extra_info[color_label] = 'N/A'

                info_text = self._format_specimen_info(original_idx, y_value=y_actual,
                                                      y_pred=y_predicted, extra_info=extra_info)
                self._create_or_update_annotation(ax1, y_predicted, residual_val, info_text, canvas)

        def on_residual_vs_index_click(event):
            if event.inaxes != ax2 or event.xdata is None:
                return

            # Find nearest index
            bar_idx = int(round(event.xdata))
            if 0 <= bar_idx < len(residuals) and self.refined_cv_indices is not None:
                original_idx = self.refined_cv_indices[bar_idx]
                y_actual = y_true[bar_idx]
                y_predicted = y_pred[bar_idx]
                residual_val = residuals[bar_idx]

                extra_info = {
                    'Fitted': y_predicted,
                    'Residual': residual_val
                }

                # Add color variable info if applicable
                if color_values is not None and color_label:
                    color_val = color_values[bar_idx]
                    if pd.notna(color_val):
                        extra_info[color_label] = color_val
                    else:
                        extra_info[color_label] = 'N/A'

                info_text = self._format_specimen_info(original_idx, y_value=y_actual,
                                                      y_pred=y_predicted, extra_info=extra_info)
                self._create_or_update_annotation(ax2, bar_idx, residual_val, info_text, canvas)

        fig.canvas.mpl_connect('button_press_event', on_residual_vs_fitted_click)
        fig.canvas.mpl_connect('button_press_event', on_residual_vs_index_click)

        # Add export button
        self._add_plot_export_button(self.residual_diagnostics_frame, fig, "residual_diagnostics")

        # Add dynamic model assessment below the plots
        self._add_residual_assessment(residuals, std_residuals)

    def _plot_classification_roc_curves(self):
        """Plot ROC curves for classification."""
        if self.refined_y_true is None or self.refined_y_pred is None:
            return

        # Clear existing plot
        for widget in self.residual_diagnostics_frame.winfo_children():
            widget.destroy()

        # Check if probabilities are available
        if self.refined_y_proba is None:
            # Show message that ROC curves require predict_proba
            msg_label = ttk.Label(
                self.residual_diagnostics_frame,
                text="ROC curves require probability predictions.\nThis model does not support predict_proba().",
                font=('Arial', 10),
                justify=tk.CENTER
            )
            msg_label.pack(expand=True)
            return

        # Get class labels
        if self.label_encoder is not None:
            class_labels = self.label_encoder.classes_
        else:
            class_labels = np.unique(self.refined_y_true)

        n_classes = len(class_labels)

        # Create figure
        fig = Figure(figsize=(10, 6))
        ax = fig.add_subplot(111)

        if n_classes == 2:
            # Binary classification - single ROC curve
            fpr, tpr, _ = roc_curve(self.refined_y_true, self.refined_y_proba[:, 1])
            roc_auc = auc(fpr, tpr)

            ax.plot(fpr, tpr, color='darkorange', lw=2,
                    label=f'ROC curve (AUC = {roc_auc:.3f})')
            ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
            ax.set_xlabel('False Positive Rate', fontweight='bold')
            ax.set_ylabel('True Positive Rate', fontweight='bold')
            ax.set_title('ROC Curve - Binary Classification', fontweight='bold')
            ax.legend(loc="lower right")
            ax.grid(True, alpha=0.3)
        else:
            # Multi-class - one-vs-rest ROC curves
            from sklearn.preprocessing import label_binarize

            # Binarize labels
            y_true_bin = label_binarize(self.refined_y_true, classes=range(n_classes))

            # Plot ROC curve for each class
            colors = plt.cm.Set1(np.linspace(0, 1, n_classes))
            for i, (color, label) in enumerate(zip(colors, class_labels)):
                fpr, tpr, _ = roc_curve(y_true_bin[:, i], self.refined_y_proba[:, i])
                roc_auc = auc(fpr, tpr)
                ax.plot(fpr, tpr, color=color, lw=2,
                       label=f'{label} (AUC = {roc_auc:.3f})')

            ax.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--',
                   label='Random', alpha=0.3)
            ax.set_xlabel('False Positive Rate', fontweight='bold')
            ax.set_ylabel('True Positive Rate', fontweight='bold')
            ax.set_title('ROC Curves - Multi-class (One-vs-Rest)', fontweight='bold')
            ax.legend(loc="lower right", fontsize=8)
            ax.grid(True, alpha=0.3)

        fig.tight_layout()

        # Add to GUI
        canvas = FigureCanvasTkAgg(fig, self.residual_diagnostics_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        self._add_plot_export_button(self.residual_diagnostics_frame, fig, "roc_curves")

    def _add_residual_assessment(self, residuals, std_residuals):
        """Add a dynamic assessment box that evaluates residual quality."""
        # Create assessment frame
        assessment_frame = ttk.Frame(self.residual_diagnostics_frame)
        assessment_frame.pack(fill='x', padx=10, pady=(10, 5))

        # Analyze residuals
        assessment_lines = ["Model Assessment:"]
        issues = []

        # Check for outliers (residuals > 3 standard deviations)
        outlier_threshold = 3.0
        outlier_count = np.sum(np.abs(std_residuals) > outlier_threshold)
        if outlier_count > 0:
            issues.append(f"⚠ {outlier_count} potential outlier(s) detected (|residual| > 3σ)")
        else:
            assessment_lines.append("✓ No significant outliers detected")

        # Check for normality using Q-Q plot deviation
        # Simple check: compare quantiles at extremes
        from spectral_predict.diagnostics import qq_plot_data
        theoretical_q, sample_q = qq_plot_data(residuals)

        # Calculate deviation from diagonal at extremes (first and last 10%)
        n_check = max(1, len(theoretical_q) // 10)
        lower_dev = np.mean(np.abs(sample_q[:n_check] - theoretical_q[:n_check]))
        upper_dev = np.mean(np.abs(sample_q[-n_check:] - theoretical_q[-n_check:]))
        residual_std = np.std(residuals)

        # If deviation is more than 50% of std, flag it
        if lower_dev > 0.5 * residual_std or upper_dev > 0.5 * residual_std:
            issues.append("⚠ Q-Q plot shows deviation from normality at extremes")
        else:
            assessment_lines.append("✓ Residuals appear normally distributed")

        # Check for heteroscedasticity (changing variance)
        # Split residuals into lower and upper half by fitted values
        if hasattr(self, 'refined_y_pred'):
            y_pred = self.refined_y_pred
            sorted_indices = np.argsort(y_pred)
            n_half = len(residuals) // 2
            lower_half_var = np.var(residuals[sorted_indices[:n_half]])
            upper_half_var = np.var(residuals[sorted_indices[n_half:]])

            # If variance ratio is > 2, flag it
            var_ratio = max(lower_half_var, upper_half_var) / (min(lower_half_var, upper_half_var) + 1e-10)
            if var_ratio > 2.0:
                issues.append("⚠ Possible heteroscedasticity (non-constant variance)")
            else:
                assessment_lines.append("✓ Residual variance appears constant")

        # Add issues to assessment
        if issues:
            assessment_lines.extend(issues)

        # Create assessment text
        assessment_text = "\n".join(assessment_lines)

        # Choose background color based on issues
        if len(issues) == 0:
            bg_color = '#d4edda'  # Light green
        elif len(issues) <= 2:
            bg_color = '#fff3cd'  # Light yellow
        else:
            bg_color = '#f8d7da'  # Light red

        # Create label with colored background
        assessment_label = tk.Label(assessment_frame, text=assessment_text,
                                    bg=bg_color, fg='#000000',
                                    font=('TkDefaultFont', 9, 'bold'),
                                    justify='left', anchor='w',
                                    padx=15, pady=10, relief='solid', borderwidth=1)
        assessment_label.pack(fill='x')

    def _add_leverage_assessment(self, leverage, threshold_2p, threshold_3p, n_samples):
        """Add a dynamic assessment box that evaluates leverage distribution."""
        # Create assessment frame
        assessment_frame = ttk.Frame(self.leverage_plot_frame)
        assessment_frame.pack(fill='x', padx=10, pady=(10, 5))

        # Analyze leverage
        assessment_lines = ["Leverage Assessment:"]
        issues = []

        # Count high and moderate leverage points
        n_high = np.sum(leverage > threshold_3p)
        n_moderate = np.sum((leverage > threshold_2p) & (leverage <= threshold_3p))
        n_normal = n_samples - n_high - n_moderate

        # Calculate percentages
        pct_high = (n_high / n_samples) * 100
        pct_moderate = (n_moderate / n_samples) * 100
        pct_normal = (n_normal / n_samples) * 100

        # Check for concerning patterns
        if pct_high > 10:
            issues.append(f"⚠ {n_high} high-leverage points ({pct_high:.1f}%) - Consider investigating data quality")
        elif pct_high > 5:
            issues.append(f"⚠ {n_high} high-leverage points ({pct_high:.1f}%) - Some influential samples detected")
        elif n_high > 0:
            assessment_lines.append(f"✓ {n_high} high-leverage point(s) ({pct_high:.1f}%) - Within acceptable range")
        else:
            assessment_lines.append("✓ No high-leverage points detected")

        # Check moderate leverage
        if pct_moderate > 20:
            issues.append(f"⚠ {n_moderate} moderate-leverage points ({pct_moderate:.1f}%) - Higher than expected")
        elif n_moderate > 0:
            assessment_lines.append(f"✓ {n_moderate} moderate-leverage point(s) ({pct_moderate:.1f}%) - Normal distribution")

        # Overall assessment
        if pct_normal >= 80:
            assessment_lines.append(f"✓ {n_normal} samples ({pct_normal:.1f}%) have normal leverage - Good data distribution")
        elif pct_normal >= 70:
            assessment_lines.append(f"✓ {n_normal} samples ({pct_normal:.1f}%) have normal leverage - Acceptable")

        # Add issues to assessment
        if issues:
            assessment_lines.extend(issues)

        # Create assessment text
        assessment_text = "\n".join(assessment_lines)

        # Choose background color based on issues
        if len(issues) == 0:
            bg_color = '#d4edda'  # Light green
        elif pct_high <= 10:
            bg_color = '#fff3cd'  # Light yellow
        else:
            bg_color = '#f8d7da'  # Light red

        # Create label with colored background
        assessment_label = tk.Label(assessment_frame, text=assessment_text,
                                    bg=bg_color, fg='#000000',
                                    font=('TkDefaultFont', 9, 'bold'),
                                    justify='left', anchor='w',
                                    padx=15, pady=10, relief='solid', borderwidth=1)
        assessment_label.pack(fill='x')

    def _plot_leverage_diagnostics(self):
        """Plot diagnostics - leverage for regression, confidence for classification."""
        if not HAS_MATPLOTLIB:
            return

        task_type = self.refined_config.get('task_type', 'regression') if hasattr(self, 'refined_config') and self.refined_config else 'regression'

        if task_type == 'classification':
            self._plot_classification_confidence()
        else:
            self._plot_regression_leverage_diagnostics()

    def _plot_regression_leverage_diagnostics(self):
        """Plot leverage diagnostics for regression."""
        if not hasattr(self, 'refined_config'):
            return

        model_name = self.refined_config.get('model_name')

        # Leverage only meaningful for linear models (PLS, Ridge, Lasso, ElasticNet)
        if model_name not in ['PLS', 'Ridge', 'Lasso', 'ElasticNet']:
            return

        if not hasattr(self, 'refined_X_cv') or self.refined_X_cv is None:
            return  # Need X data for leverage calculation

        from spectral_predict.diagnostics import compute_leverage

        # Clear existing plot
        for widget in self.leverage_plot_frame.winfo_children():
            widget.destroy()

        # Compute leverage on the CV data
        X_data = self.refined_X_cv
        leverage, threshold_2p = compute_leverage(X_data)

        # Calculate 3p/n threshold manually
        n_samples, n_features = X_data.shape
        n_params = n_features + 1  # Include intercept
        threshold_3p = 3.0 * n_params / n_samples

        # Create figure
        fig = Figure(figsize=(10, 5))
        ax = fig.add_subplot(111)

        # Determine colors based on leverage thresholds
        colors = []
        for h in leverage:
            if h > threshold_3p:
                colors.append('red')  # High leverage
            elif h > threshold_2p:
                colors.append('orange')  # Moderate leverage
            else:
                colors.append('steelblue')  # Normal

        indices = np.arange(len(leverage))
        ax.scatter(indices, leverage, c=colors, alpha=0.7, edgecolors='black', linewidths=0.5, s=60)

        # Add threshold lines
        ax.axhline(y=threshold_2p, color='orange', linestyle='--', linewidth=2,
                   label=f'Moderate Leverage (2p/n = {threshold_2p:.3f})')
        ax.axhline(y=threshold_3p, color='red', linestyle='--', linewidth=2,
                   label=f'High Leverage (3p/n = {threshold_3p:.3f})')

        # Label high-leverage points
        high_leverage_indices = np.where(leverage > threshold_3p)[0]
        for idx in high_leverage_indices:
            ax.annotate(f'{idx}', (idx, leverage[idx]),
                       xytext=(5, 5), textcoords='offset points', fontsize=8)

        ax.set_xlabel('Sample Index', fontsize=11)
        ax.set_ylabel('Leverage (Hat Values)', fontsize=11)
        ax.set_title('Leverage Plot - Influential Samples', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.legend(loc='upper right')

        # Add info text
        n_high = np.sum(leverage > threshold_3p)
        n_moderate = np.sum((leverage > threshold_2p) & (leverage <= threshold_3p))
        info_text = f'High leverage: {n_high} samples\nModerate leverage: {n_moderate} samples'
        ax.text(0.02, 0.98, info_text, transform=ax.transAxes,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                fontsize=9, family='monospace')

        fig.tight_layout()

        canvas = FigureCanvasTkAgg(fig, self.leverage_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add click handler for point identification
        def on_leverage_click(event):
            if event.inaxes != ax or event.xdata is None or event.ydata is None:
                return

            # Find nearest point
            click_x, click_y = event.xdata, event.ydata
            distances = np.sqrt((indices - click_x)**2 + ((leverage - click_y) * 100)**2)  # Scale y for better matching
            nearest_idx = np.argmin(distances)

            # Check if click is reasonably close
            x_range = ax.get_xlim()[1] - ax.get_xlim()[0]
            threshold = 0.05 * x_range

            if abs(indices[nearest_idx] - click_x) < threshold and self.refined_cv_indices is not None:
                original_idx = self.refined_cv_indices[nearest_idx]
                y_value = self.refined_y_true[nearest_idx] if hasattr(self, 'refined_y_true') else None
                leverage_val = leverage[nearest_idx]

                # Determine leverage category
                if leverage_val > threshold_3p:
                    leverage_cat = 'High'
                elif leverage_val > threshold_2p:
                    leverage_cat = 'Moderate'
                else:
                    leverage_cat = 'Normal'

                extra_info = {
                    'Leverage': leverage_val,
                    'Category': leverage_cat
                }

                info_text = self._format_specimen_info(original_idx, y_value=y_value, extra_info=extra_info)
                self._create_or_update_annotation(ax, indices[nearest_idx], leverage_val, info_text, canvas)

        fig.canvas.mpl_connect('button_press_event', on_leverage_click)

        # Add export button
        self._add_plot_export_button(self.leverage_plot_frame, fig, "leverage_analysis")

        # Add dynamic leverage assessment below the plot
        self._add_leverage_assessment(leverage, threshold_2p, threshold_3p, n_samples)

    def _plot_classification_confidence(self):
        """Plot prediction confidence distribution for classification."""
        if self.refined_y_true is None or self.refined_y_pred is None:
            return

        # Clear existing plot
        for widget in self.leverage_plot_frame.winfo_children():
            widget.destroy()

        # Check if probabilities are available
        if self.refined_y_proba is None:
            msg_label = ttk.Label(
                self.leverage_plot_frame,
                text="Confidence analysis requires probability predictions.\nThis model does not support predict_proba().",
                font=('Arial', 10),
                justify=tk.CENTER
            )
            msg_label.pack(expand=True)
            return

        # Get maximum probability for each prediction (confidence)
        confidences = np.max(self.refined_y_proba, axis=1)

        # Identify correct vs incorrect predictions
        correct = (self.refined_y_true == self.refined_y_pred)

        # Create figure
        fig = Figure(figsize=(10, 6))
        ax = fig.add_subplot(111)

        # Plot histogram for correct and incorrect predictions
        ax.hist(confidences[correct], bins=30, alpha=0.7, color='green',
               label=f'Correct ({np.sum(correct)} samples)', edgecolor='black')
        ax.hist(confidences[~correct], bins=30, alpha=0.7, color='red',
               label=f'Incorrect ({np.sum(~correct)} samples)', edgecolor='black')

        ax.set_xlabel('Prediction Confidence', fontweight='bold')
        ax.set_ylabel('Frequency', fontweight='bold')
        ax.set_title('Prediction Confidence Distribution', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')

        # Add vertical lines for mean confidence
        mean_conf_correct = np.mean(confidences[correct]) if np.sum(correct) > 0 else 0
        mean_conf_incorrect = np.mean(confidences[~correct]) if np.sum(~correct) > 0 else 0

        if np.sum(correct) > 0:
            ax.axvline(mean_conf_correct, color='darkgreen', linestyle='--', lw=2,
                      label=f'Mean (correct): {mean_conf_correct:.3f}')
        if np.sum(~correct) > 0:
            ax.axvline(mean_conf_incorrect, color='darkred', linestyle='--', lw=2,
                      label=f'Mean (incorrect): {mean_conf_incorrect:.3f}')

        ax.legend()
        fig.tight_layout()

        # Add to GUI
        canvas = FigureCanvasTkAgg(fig, self.leverage_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        self._add_plot_export_button(self.leverage_plot_frame, fig, "confidence_distribution")

    def _run_refined_model(self):
        """Run the refined model with user-specified parameters."""
        if self.X is None or self.y is None:
            messagebox.showwarning("No Data", "Please load data first")
            return

        # Validate refinement parameters
        if not self._validate_refinement_parameters():
            return

        # Disable buttons during execution (both Configuration and Selection tabs)
        self.refine_run_button.config(state='disabled')
        self.refine_run_button_selection.config(state='disabled')
        self.refine_status.config(text="Running refined model...")

        # Run in thread
        thread = threading.Thread(target=self._run_refined_model_thread)
        thread.start()

    def _run_refined_model_thread(self):
        """Execute the refined model in a background thread."""
        try:
            from spectral_predict.models import get_model
            from spectral_predict.preprocess import SavgolDerivative, SNV
            from sklearn.model_selection import KFold, StratifiedKFold
            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
            from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score
            from sklearn.base import clone

            # Parse wavelength specification
            available_wl = self.X_original.columns.astype(float).values
            wl_spec_text = self.refine_wl_spec.get('1.0', 'end')
            selected_wl = self._parse_wavelength_spec(wl_spec_text, available_wl)

            if not selected_wl:
                raise ValueError("No valid wavelengths selected. Please check your wavelength specification.")

            # DIAGNOSTIC: Compare reconstructed wavelengths with original all_vars
            print(f"\nDEBUG: Checking wavelength reconstruction...")
            print(f"DEBUG: hasattr(self, 'loaded_model_config') = {hasattr(self, 'loaded_model_config')}")
            if hasattr(self, 'loaded_model_config'):
                print(f"DEBUG: self.loaded_model_config exists = {self.loaded_model_config is not None}")
                if self.loaded_model_config:
                    all_vars_str = self.loaded_model_config.get('all_vars', 'N/A')
                    print(f"DEBUG: all_vars_str = {all_vars_str[:100] if all_vars_str and all_vars_str != 'N/A' else all_vars_str}...")

            if hasattr(self, 'loaded_model_config') and self.loaded_model_config:
                all_vars_str = self.loaded_model_config.get('all_vars', 'N/A')
                if all_vars_str != 'N/A' and all_vars_str:
                    try:
                        # Parse original wavelengths from all_vars
                        # DON'T sort - preserve order to detect ordering differences!
                        original_wl = [float(x.strip()) for x in all_vars_str.split(',')]
                        reconstructed_wl = list(selected_wl)  # Preserve order as parsed

                        print(f"\n{'='*80}")
                        print(f"WAVELENGTH RECONSTRUCTION DIAGNOSTIC")
                        print(f"{'='*80}")
                        print(f"Original wavelengths (from Results all_vars): {len(original_wl)}")
                        print(f"Reconstructed wavelengths (from parsed spec): {len(reconstructed_wl)}")
                        print(f"First 10 original: {original_wl[:10]}")
                        print(f"First 10 reconstructed: {reconstructed_wl[:10]}")

                        # Check for exact match (INCLUDING ORDER)
                        if original_wl == reconstructed_wl:
                            print(f"✓ Perfect match - wavelengths AND ORDER are identical")
                        else:
                            # Find differences
                            missing = sorted(set(original_wl) - set(reconstructed_wl))
                            extra = sorted(set(reconstructed_wl) - set(original_wl))

                            # Check if just ordering differs (same set, different order)
                            same_set = (set(original_wl) == set(reconstructed_wl))

                            print(f"\n⚠️  WAVELENGTH MISMATCH DETECTED!")
                            print(f"{'='*80}")

                            if same_set:
                                print(f"⚠️  ORDERING DIFFERENCE (same wavelengths, different order)!")
                                print(f"  This will cause R² differences even though wavelengths match.")
                                print(f"\n  First 10 original order:      {original_wl[:10]}")
                                print(f"  First 10 reconstructed order: {reconstructed_wl[:10]}")

                                # Find first mismatch
                                for i, (orig, recon) in enumerate(zip(original_wl, reconstructed_wl)):
                                    if orig != recon:
                                        print(f"\n  First difference at position {i}:")
                                        print(f"    Original: {orig}")
                                        print(f"    Reconstructed: {recon}")
                                        break
                            else:
                                print(f"Missing from reconstruction: {len(missing)} wavelengths")
                                if missing:
                                    print(f"  First 10 missing: {missing[:10]}")
                                    if len(missing) > 10:
                                        print(f"  ... and {len(missing)-10} more")

                                print(f"\nExtra in reconstruction: {len(extra)} wavelengths")
                                if extra:
                                    print(f"  First 10 extra: {extra[:10]}")
                                    if len(extra) > 10:
                                        print(f"  ... and {len(extra)-10} more")

                            print(f"\n⚠️  This explains the R² difference between Results and Model Dev!")
                            print(f"{'='*80}")

                        print(f"{'='*80}\n")
                    except Exception as e:
                        print(f"DEBUG: Could not compare wavelengths: {e}")

            # Filter data source to selected wavelengths
            # Create mapping from float wavelengths to actual column names
            if self.X is not None:
                wavelength_columns = self.X.columns
            else:
                wavelength_columns = self.X_original.columns
            wl_to_col = {float(col): col for col in wavelength_columns}

            # Get the actual column names for selected wavelengths
            selected_cols = [wl_to_col[wl] for wl in selected_wl if wl in wl_to_col]

            if not selected_cols:
                raise ValueError(f"Could not find matching wavelengths. Selected: {len(selected_wl)}, Found: 0")

            # Determine how many folds we'll run so we can validate sample counts
            n_folds = self.refine_folds.get()

            # Validate CV folds match original training if available
            if hasattr(self, 'loaded_model_config') and self.loaded_model_config:
                # Check if we have CV fold information from original training
                original_folds = self.loaded_model_config.get('cv_folds', None)
                if original_folds is None and 'training_config' in self.loaded_model_config:
                    original_folds = self.loaded_model_config['training_config'].get('folds', None)

                if original_folds is not None:
                    if n_folds != original_folds:
                        print(f"\n⚠️  WARNING: CV FOLDS MISMATCH!")
                        print(f"  Original model trained with: {original_folds}-fold CV")
                        print(f"  Current refinement using: {n_folds}-fold CV")
                        print(f"  → This may cause R² differences from Results tab!")
                        print(f"  → Consider using {original_folds} folds to match original training\n")

            # Determine data source (respect current wavelength filter)
            if self.X is not None:
                X_source = self.X
            else:
                X_source = self.X_original

            # Align sample selection with the main analysis (respect excluded spectra)
            total_samples = len(X_source)
            excluded_indices = sorted(idx for idx in self.excluded_spectra if 0 <= idx < total_samples)
            if excluded_indices:
                excluded_set = set(excluded_indices)
                include_indices = [i for i in range(total_samples) if i not in excluded_set]
                if len(include_indices) < n_folds:
                    raise ValueError(
                        f"Only {len(include_indices)} samples remain after exclusions; "
                        f"{n_folds}-fold CV requires at least {n_folds} samples."
                    )
                print(f"DEBUG: Applying {len(excluded_indices)} excluded spectra for refinement "
                      f"({len(include_indices)} samples remain).")
                X_base_df = X_source.iloc[include_indices]
                y_series = self.y.iloc[include_indices]
            else:
                X_base_df = X_source
                y_series = self.y

            # Filter out validation set (if enabled) - CRITICAL FIX
            # This ensures Model Development uses the same data split as the main search
            if self.validation_enabled.get() and self.validation_indices:
                # Remove validation samples from training data
                initial_samples = len(X_base_df)
                X_base_df = X_base_df[~X_base_df.index.isin(self.validation_indices)]
                y_series = y_series[~y_series.index.isin(self.validation_indices)]

                n_val = len(self.validation_indices)
                n_removed = initial_samples - len(X_base_df)
                n_cal = len(X_base_df)

                if n_cal < n_folds:
                    raise ValueError(
                        f"Only {n_cal} calibration samples remain after validation set exclusion; "
                        f"{n_folds}-fold CV requires at least {n_folds} samples."
                    )

                print(f"DEBUG: Excluding {n_removed} validation samples from Model Development")
                print(f"DEBUG: Calibration: {n_cal} samples | Validation: {n_val} samples")
                print(f"DEBUG: This matches the data split used in the main search (Results tab)")

            # Add comprehensive diagnostic output for debugging R² discrepancies
            print(f"\n{'='*80}")
            print(f"MODEL DEVELOPMENT - DATASET STATE DIAGNOSTIC")
            print(f"{'='*80}")
            print(f"Configuration:")
            print(f"  CV Folds: {n_folds}")
            print(f"  Random State: 42 (fixed)")
            print(f"\nData Filtering:")
            print(f"  Total samples in dataset: {len(X_source)}")
            print(f"  Excluded samples: {len(excluded_indices) if excluded_indices else 0}")
            print(f"  Validation enabled: {self.validation_enabled.get()}")
            print(f"  Validation samples: {len(self.validation_indices) if self.validation_enabled.get() and self.validation_indices else 0}")
            print(f"  → Calibration samples for CV: {len(X_base_df)}")

            # Check if we have saved training configuration from the original search
            if hasattr(self, 'loaded_model_config') and self.loaded_model_config and 'training_config' in self.loaded_model_config:
                training_config = self.loaded_model_config['training_config']
                print(f"\nOriginal Training Configuration (from Results):")
                print(f"  CV Folds: {training_config.get('folds', 'NOT SAVED')}")
                print(f"  Calibration samples: {training_config.get('n_samples_used', 'NOT SAVED')}")
                print(f"  Excluded count: {training_config.get('excluded_count', 'NOT SAVED')}")
                print(f"  Validation count: {training_config.get('validation_count', 'NOT SAVED')}")

                # Check for mismatches
                if 'n_samples_used' in training_config:
                    original_samples = training_config['n_samples_used']
                    current_samples = len(X_base_df)
                    if original_samples != current_samples:
                        print(f"\n⚠️  WARNING: SAMPLE COUNT MISMATCH!")
                        print(f"  Original training used: {original_samples} samples")
                        print(f"  Current refinement uses: {current_samples} samples")
                        print(f"  Difference: {current_samples - original_samples:+d} samples")
                        print(f"  → This WILL cause R² values to differ from Results tab!")
            else:
                print(f"\n⚠️  Note: Training configuration not saved in results")
                print(f"  (Model was saved before configuration tracking was added)")
                print(f"  Cannot verify if dataset state matches original training")

            print(f"{'='*80}\n")

            wl_summary = f"{len(selected_wl)} wavelengths ({selected_wl[0]:.1f} to {selected_wl[-1]:.1f} nm)"

            # Get user-selected preprocessing method and map to build_preprocessing_pipeline format
            preprocess = self.refine_preprocess.get()

            # Get window size (check custom first)
            if self.refine_window_custom.get().strip():
                try:
                    window = int(self.refine_window_custom.get().strip())
                except ValueError:
                    self._log_progress(f"Warning: Invalid custom window size '{self.refine_window_custom.get()}', using selected radio button value")
                    window = self.refine_window.get()
            else:
                window = self.refine_window.get()

            # Map GUI preprocessing names to search.py format
            preprocess_name_map = {
                'raw': 'raw',
                'snv': 'snv',
                'sg1': 'deriv',
                'sg2': 'deriv',
                'snv_sg1': 'snv_deriv',
                'snv_sg2': 'snv_deriv',
                'deriv_snv': 'deriv_snv'
            }

            # Helper function to determine polyorder from derivative order
            def get_polyorder_from_deriv(deriv_value):
                """Determine polynomial order from derivative order."""
                if deriv_value == 0:
                    return 2  # No derivative
                elif deriv_value == 1:
                    return 2  # 1st derivative needs poly order 2
                elif deriv_value == 2:
                    return 3  # 2nd derivative needs poly order 3
                else:
                    print(f"WARNING: Unexpected deriv value {deriv_value}, using polyorder=2")
                    return 2

            # Default derivative orders for GUI preprocessing methods
            deriv_map = {
                'raw': 0,
                'snv': 0,
                'sg1': 1,
                'sg2': 2,
                'snv_sg1': 1,
                'snv_sg2': 2,
                'deriv_snv': None  # Ambiguous - must be determined from config
            }

            preprocess_name = preprocess_name_map.get(preprocess, 'raw')

            # Determine derivative order and polyorder
            # Priority: Use actual derivative order from loaded config, otherwise use defaults
            deriv = None
            polyorder = None

            if self.selected_model_config is not None:
                config_deriv = self.selected_model_config.get('Deriv', None)
                if config_deriv is not None and not pd.isna(config_deriv):
                    # Use the actual derivative order from config
                    deriv = int(config_deriv)
                    polyorder = get_polyorder_from_deriv(deriv)
                    print(f"DEBUG: Using deriv={deriv}, polyorder={polyorder} from loaded config")
                else:
                    print(f"WARNING: No 'Deriv' value in loaded config")

            # If we couldn't get from config, use defaults
            if deriv is None:
                default_deriv = deriv_map.get(preprocess, 0)
                if default_deriv is None:
                    # This is deriv_snv without config - we can't determine the derivative order
                    print(f"\n⚠️  CRITICAL WARNING: Cannot determine derivative order for 'deriv_snv'!")
                    print(f"  'deriv_snv' can be either 1st or 2nd derivative")
                    print(f"  Without config data, assuming 1st derivative (may be wrong!)")
                    print(f"  R² WILL BE INCORRECT if this assumption is wrong!")
                    deriv = 1  # Assume 1st derivative as safer default
                    polyorder = 2
                else:
                    deriv = default_deriv
                    polyorder = get_polyorder_from_deriv(deriv)
                    if self.selected_model_config is not None:
                        print(f"INFO: Using default deriv={deriv}, polyorder={polyorder} for '{preprocess}'")
            else:
                # Already set from config above
                pass

            # CRITICAL FIX: Detect if we have derivative preprocessing + variable subset
            # This matches the behavior in search.py (lines 434-449)
            is_derivative = preprocess in ['sg1', 'sg2', 'snv_sg1', 'snv_sg2', 'deriv_snv']
            base_full_vars = len(X_base_df.columns)
            if self.selected_model_config is not None:
                cfg_full_vars = self.selected_model_config.get('full_vars')
                if cfg_full_vars is not None and not pd.isna(cfg_full_vars):
                    try:
                        base_full_vars = int(cfg_full_vars)
                    except (TypeError, ValueError):
                        pass
            is_subset = len(selected_wl) < base_full_vars
            # CRITICAL FIX: Use PATH A ONLY for derivative + subset (non-contiguous wavelengths)
            # - Derivative subsets need full spectral context for correct Savitzky-Golay windows
            # - Full-spectrum derivatives should use PATH B to avoid data leakage in CV
            # - This matches search.py behavior (skip_preprocessing=True only for derivative subsets)
            use_full_spectrum_preprocessing = is_derivative and is_subset

            # Debug output for preprocessing parameters
            print(f"\nDEBUG: Preprocessing Parameters:")
            print(f"  Method: {preprocess}")
            print(f"  Derivative order: {deriv}")
            print(f"  Polyorder: {polyorder}")
            print(f"  Window size: {window}")

            if use_full_spectrum_preprocessing:
                print(f"DEBUG: Derivative + subset detected. Using PATH A (preprocess full spectrum, then subset).")
                print(f"DEBUG: This preserves derivative context for non-contiguous wavelengths.")
                print(f"DEBUG: Preprocessing {len(X_base_df.columns)} wavelengths → subsetting to {len(selected_wl)} wavelengths")
            else:
                if is_derivative:
                    print(f"DEBUG: Full-spectrum derivative detected. Using PATH B (preprocess inside CV).")
                    print(f"DEBUG: This prevents data leakage by fitting preprocessing per-fold.")
                    print(f"DEBUG: Processing {len(selected_wl)} wavelengths")
                else:
                    print(f"DEBUG: Raw/SNV preprocessing. Using PATH B (standard pipeline).")
                    print(f"DEBUG: Processing {len(selected_wl)} wavelengths")

            # Get user-selected task type
            task_type = self.refine_task_type.get()

            # Get user-selected model
            model_name = self.refine_model_type.get()

            # Extract hyperparameters from loaded model config (if available)
            # This ensures we reproduce the exact same model that was selected from results
            n_components = 10  # Default fallback
            if self.selected_model_config is not None and 'LVs' in self.selected_model_config:
                lvs_value = self.selected_model_config.get('LVs')
                if not pd.isna(lvs_value):
                    n_components = int(lvs_value)
                    print(f"DEBUG: Using n_components={n_components} from loaded model config")

            model = get_model(
                model_name,
                task_type=task_type,
                n_components=n_components,  # Use exact n_components from original model
                max_n_components=self.max_n_components.get(),
                max_iter=self.refine_max_iter.get()
            )

            # Reapply tuned hyperparameters from the search results when available
            params_from_search = {}
            if self.selected_model_config is not None:
                raw_params = self.selected_model_config.get('Params')
                if isinstance(raw_params, str) and raw_params.strip():
                    try:
                        parsed = ast.literal_eval(raw_params)
                        if isinstance(parsed, dict):
                            params_from_search = parsed
                    except (ValueError, SyntaxError) as parse_err:
                        print(f"WARNING: Could not parse saved Params '{raw_params}': {parse_err}")

            if params_from_search:
                try:
                    # DIAGNOSTIC: Capture parameters BEFORE set_params for ALL models
                    if True:  # Apply to ALL models
                        print(f"\n{'='*80}")
                        print(f"DIAGNOSTIC - {model_name} Model Development (BEFORE set_params)")
                        print(f"{'='*80}")
                        try:
                            before_params = model.get_params()
                            print(f"Default {model_name} parameters before loading:")
                            for key in sorted(before_params.keys()):
                                print(f"  {key}: {before_params[key]}")
                        except Exception as e:
                            print(f"ERROR capturing before params: {e}")
                        print(f"{'='*80}\n")

                    model.set_params(**params_from_search)
                    print(f"DEBUG: Applied saved search parameters: {params_from_search}")

                    # DIAGNOSTIC: Capture parameters AFTER set_params for ALL models
                    if True:  # Apply to ALL models including PLS
                        print(f"\n{'='*80}")
                        print(f"DIAGNOSTIC - {model_name} Model Development (AFTER set_params)")
                        print(f"{'='*80}")
                        try:
                            after_params = model.get_params()
                            print(f"ALL {model_name} parameters after loading:")
                            for key in sorted(after_params.keys()):
                                print(f"  {key}: {after_params[key]}")
                            print(f"\nParams loaded from CSV:")
                            print(f"  {params_from_search}")

                            # VALIDATION: Check for parameter mismatches
                            param_mismatches = []
                            params_not_loaded = []

                            # Check if loaded params match actual params
                            for key, expected_val in params_from_search.items():
                                actual_val = after_params.get(key)
                                if actual_val != expected_val:
                                    param_mismatches.append(
                                        f"  - {key}: expected={expected_val}, got={actual_val}"
                                    )

                            # Check for critical params based on model type
                            critical_params_by_model = {
                                "XGBoost": ['n_estimators', 'learning_rate', 'max_depth', 'random_state', 'tree_method'],
                                "LightGBM": ['n_estimators', 'learning_rate', 'num_leaves', 'random_state', 'verbose'],
                                "CatBoost": ['iterations', 'learning_rate', 'depth', 'random_state'],
                                "Ridge": ['alpha', 'solver', 'tol', 'max_iter'],
                                "Lasso": ['alpha', 'selection', 'tol', 'max_iter'],
                                "ElasticNet": ['alpha', 'l1_ratio', 'selection', 'tol', 'max_iter'],
                                "RandomForest": ['n_estimators', 'max_depth', 'min_samples_split', 'random_state'],
                                "PLS": ['n_components', 'scale', 'max_iter', 'tol']
                            }

                            critical_params = critical_params_by_model.get(model_name, [])
                            missing_critical = []

                            for key in critical_params:
                                if key not in params_from_search and key in after_params:
                                    missing_critical.append(
                                        f"  - {key}: using default={after_params[key]} (CRITICAL for {model_name})"
                                    )

                            # Also check other potentially important params
                            other_params = ['subsample', 'colsample_bytree', 'reg_alpha', 'reg_lambda',
                                          'min_samples_leaf', 'bootstrap', 'criterion']
                            for key in other_params:
                                if key not in critical_params and key not in params_from_search and key in after_params:
                                    params_not_loaded.append(
                                        f"  - {key}: using default={after_params[key]}"
                                    )

                            if param_mismatches:
                                print(f"\n⚠️  WARNING: Parameter mismatches detected!")
                                for mismatch in param_mismatches:
                                    print(mismatch)
                                print(f"⚠️  This may cause R² differences from Results tab!")

                            if missing_critical:
                                print(f"\n⚠️  CRITICAL WARNING: Missing critical parameters for {model_name}!")
                                print(f"⚠️  These parameters significantly affect model behavior and R² values:")
                                for missing in missing_critical:
                                    print(missing)
                                print(f"⚠️  R² WILL differ from Results tab due to missing critical parameters!")

                            if params_not_loaded:
                                print(f"\n⚠️  WARNING: Some parameters not in saved results!")
                                for missing in params_not_loaded:
                                    print(missing)
                                print(f"⚠️  This may cause R² differences from Results tab!")

                            print(f"{'='*80}\n")
                        except Exception as e:
                            print(f"ERROR capturing after params: {e}\n")

                    # Add warning for NeuralBoosted
                    elif model_name == "NeuralBoosted":
                        if params_from_search.get('early_stopping', False):
                            print(f"\n⚠️  NOTE: NeuralBoosted with early_stopping=True")
                            print(f"   R² may vary slightly (±0.01-0.02) from Results tab due to")
                            print(f"   validation split differences between CV and full dataset.\n")

                except Exception as e:
                    print(f"WARNING: Failed to apply saved parameters {params_from_search}: {e}")

            # Apply hyperparameters from UI widgets (overrides loaded params if present)
            # This allows users to adjust hyperparameters in the GUI after loading from Results tab
            try:
                ui_params = self._collect_refine_hyperparams(model_name)

                if ui_params:
                    print(f"\n{'='*80}")
                    print(f"HYPERPARAMETERS FROM UI (Model Development Tab)")
                    print(f"{'='*80}")
                    print(f"Applying {len(ui_params)} hyperparameters from UI widgets:")
                    for key in sorted(ui_params.keys()):
                        print(f"  {key}: {ui_params[key]}")
                    print(f"\nNote: UI values override loaded parameters from Results tab")
                    print(f"{'='*80}\n")

                    model.set_params(**ui_params)
                    print(f"DEBUG: Successfully applied {len(ui_params)} UI hyperparameters to {model_name} model")

                    # Final parameter verification
                    final_params = model.get_params()
                    print(f"\n{'='*80}")
                    print(f"FINAL {model_name} PARAMETERS (After UI Application)")
                    print(f"{'='*80}")
                    for key in sorted(final_params.keys()):
                        # Highlight parameters that came from UI
                        if key in ui_params:
                            print(f"  {key}: {final_params[key]} ← from UI")
                        else:
                            print(f"  {key}: {final_params[key]}")
                    print(f"{'='*80}\n")
                else:
                    print(f"DEBUG: No UI hyperparameters collected for {model_name} (using defaults or loaded params)")

            except Exception as e:
                print(f"WARNING: Failed to apply UI hyperparameters: {e}")
                import traceback
                traceback.print_exc()

            # Build preprocessing pipeline and prepare data
            from spectral_predict.preprocess import build_preprocessing_pipeline
            from sklearn.pipeline import Pipeline

            # Handle categorical labels for classification (must happen BEFORE creating y_array)
            # This matches the logic in search.py lines 127-143
            local_label_encoder = None
            if task_type == "classification":
                # Check if labels are non-numeric (text labels like "Clean", "Contaminated", etc.)
                if y_series.dtype == object or not np.issubdtype(y_series.dtype, np.number):
                    from sklearn.preprocessing import LabelEncoder
                    local_label_encoder = LabelEncoder()
                    y_original = y_series.copy()  # Keep original for logging
                    y_series_encoded = local_label_encoder.fit_transform(y_series)
                    # Convert back to Series to maintain index
                    y_series = pd.Series(y_series_encoded, index=y_series.index)

                    # Log the label mapping
                    label_mapping = dict(zip(local_label_encoder.classes_,
                                            local_label_encoder.transform(local_label_encoder.classes_)))
                    print(f"\n{'='*70}")
                    print(f"CATEGORICAL LABEL ENCODING (Model Development)")
                    print(f"{'='*70}")
                    print(f"Detected non-numeric classification labels.")
                    print(f"Encoding mapping:")
                    for label, code in sorted(label_mapping.items(), key=lambda x: x[1]):
                        print(f"  '{label}' -> {code}")
                    print(f"{'='*70}\n")

            # Prepare cross-validation
            y_array = y_series.values
            if task_type == "regression":
                cv = KFold(n_splits=n_folds, shuffle=True, random_state=42)
            else:
                cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)

            if use_full_spectrum_preprocessing:
                # === PATH A: Derivative + Subset (matches search.py lines 434-449) ===
                # 1. Build preprocessing pipeline WITHOUT model
                prep_steps = build_preprocessing_pipeline(
                    preprocess_name,
                    deriv,
                    window,
                    polyorder
                )
                prep_pipeline = Pipeline(prep_steps)

                # 2. Preprocess FULL spectrum (all wavelengths)
                X_full = X_base_df.values

                # DIAGNOSTIC: Fingerprint BEFORE preprocessing
                import hashlib
                X_pre_hash = hashlib.md5(X_full.tobytes()).hexdigest()[:8]
                print(f"\nDEBUG: PRE-PREPROCESSING FINGERPRINT:")
                print(f"  X_full hash (before preprocessing): {X_pre_hash}")
                print(f"  X_full shape: {X_full.shape}")
                print(f"  X_full[0,:5] (first spectrum, first 5 wavelengths): {X_full[0,:5]}")

                print(f"\nDEBUG: Preprocessing full spectrum ({X_full.shape[1]} wavelengths)...")
                X_full_preprocessed = prep_pipeline.fit_transform(X_full)

                # DIAGNOSTIC: Fingerprint AFTER preprocessing
                X_post_hash = hashlib.md5(X_full_preprocessed.tobytes()).hexdigest()[:8]
                print(f"\nDEBUG: POST-PREPROCESSING FINGERPRINT:")
                print(f"  X_preprocessed hash (after {preprocess_name}): {X_post_hash}")
                print(f"  X_preprocessed shape: {X_full_preprocessed.shape}")
                print(f"  X_preprocessed[0,:5] (first spectrum, first 5 values): {X_full_preprocessed[0,:5]}")

                # 3. Find indices of selected wavelengths in original data
                all_wavelengths = X_base_df.columns.astype(float).values
                wavelength_indices = []
                for wl in selected_wl:
                    idx = np.where(np.abs(all_wavelengths - wl) < 0.01)[0]
                    if len(idx) > 0:
                        wavelength_indices.append(idx[0])

                # 4. Subset the PREPROCESSED data (not raw!)
                X_work = X_full_preprocessed[:, wavelength_indices]
                print(f"DEBUG: Subsetted to {X_work.shape[1]} wavelengths after preprocessing.")
                print(f"DEBUG: This preserves derivative context from full spectrum.")

                # 5. Build pipeline with ONLY the model (skip preprocessing - already done!)
                # For PLS-DA, we need PLS + LogisticRegression
                if model_name == "PLS-DA" and task_type == "classification":
                    from sklearn.linear_model import LogisticRegression
                    pipe_steps = [
                        ('pls', model),
                        ('lr', LogisticRegression(max_iter=1000, random_state=42))
                    ]
                else:
                    pipe_steps = [('model', model)]
                pipe = Pipeline(pipe_steps)

                print(f"DEBUG: Pipeline steps: {[name for name, _ in pipe_steps]} (preprocessing already applied)")

            else:
                # === PATH B: Raw/SNV or Full-Spectrum (existing behavior) ===
                # Subset raw data first, then preprocess inside CV
                X_work = X_base_df[selected_cols].values

                # Build full pipeline with preprocessing + model
                pipe_steps = build_preprocessing_pipeline(
                    preprocess_name,
                    deriv,
                    window,
                    polyorder
                )

                # For PLS-DA, we need PLS + LogisticRegression
                if model_name == "PLS-DA" and task_type == "classification":
                    from sklearn.linear_model import LogisticRegression
                    pipe_steps.append(('pls', model))
                    pipe_steps.append(('lr', LogisticRegression(max_iter=1000, random_state=42)))
                else:
                    pipe_steps.append(('model', model))
                pipe = Pipeline(pipe_steps)

                print(f"DEBUG: Pipeline steps: {[name for name, _ in pipe_steps]} (preprocessing inside CV)")

            # DATA FINGERPRINT - verify same data as search.py
            import hashlib
            X_hash = hashlib.md5(X_work.tobytes()).hexdigest()[:8]
            y_hash = hashlib.md5(y_array.tobytes()).hexdigest()[:8]
            print(f"\n{'='*80}")
            print(f"DATA FINGERPRINT (Model Development)")
            print(f"{'='*80}")
            print(f"X fingerprint: {X_hash}")
            print(f"y fingerprint: {y_hash}")
            print(f"X shape: {X_work.shape}")
            print(f"y shape: {y_array.shape}")
            print(f"First 5 y values: {y_array[:5].tolist()}")
            print(f"Last 5 y values: {y_array[-5:].tolist()}")
            print(f"y range: [{y_array.min():.2f}, {y_array.max():.2f}]")
            print(f"{'='*80}\n")

            # Collect metrics for each fold
            fold_metrics = []
            all_y_true = []
            all_y_pred = []
            all_y_proba = []  # Store prediction probabilities for classification
            all_cv_indices = []  # Store CV sample indices for specimen ID mapping
            X_raw = X_work  # For derivative+subset, this is preprocessed; for others, it's raw

            for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X_raw, y_array)):
                # Clone ENTIRE PIPELINE for this fold (not just model)
                pipe_fold = clone(pipe)

                # Split data
                X_train, X_test = X_raw[train_idx], X_raw[test_idx]
                y_train, y_test = y_array[train_idx], y_array[test_idx]

                # Fit pipeline (preprocessing + model) and predict
                pipe_fold.fit(X_train, y_train)
                y_pred = pipe_fold.predict(X_test)

                # Store predictions for plotting
                all_y_true.extend(y_test)
                all_y_pred.extend(y_pred)
                all_cv_indices.extend(test_idx)  # Track indices for specimen ID mapping

                # Store prediction probabilities if available (for classification)
                if hasattr(pipe_fold, 'predict_proba'):
                    y_proba = pipe_fold.predict_proba(X_test)
                    all_y_proba.append(y_proba)
                elif 'model' in pipe_fold.named_steps and hasattr(pipe_fold.named_steps['model'], 'predict_proba'):
                    y_proba = pipe_fold.named_steps['model'].predict_proba(X_test)
                    all_y_proba.append(y_proba)
                elif 'lr' in pipe_fold.named_steps and hasattr(pipe_fold.named_steps['lr'], 'predict_proba'):
                    # For PLS-DA, LogisticRegression is named 'lr'
                    y_proba = pipe_fold.named_steps['lr'].predict_proba(X_test)
                    all_y_proba.append(y_proba)

                if task_type == "regression":
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    r2 = r2_score(y_test, y_pred)
                    mae = mean_absolute_error(y_test, y_pred)
                    fold_metrics.append({"rmse": rmse, "r2": r2, "mae": mae})

                    # FOLD-LEVEL DIAGNOSTIC
                    print(f"\n{'='*80}")
                    print(f"FOLD {fold_idx+1}/{n_folds} DIAGNOSTIC (Model Development)")
                    print(f"{'='*80}")
                    print(f"Train indices: n={len(train_idx)}, first 10: {train_idx[:10].tolist()}")
                    print(f"Test indices:  n={len(test_idx)}, all: {test_idx.tolist()}")
                    print(f"Test y values (first 5): {y_test[:5].tolist()}")
                    print(f"Predictions (first 5):   {y_pred[:5].tolist()}")
                    print(f"Test y values (last 5):  {y_test[-5:].tolist() if len(y_test) >= 5 else y_test.tolist()}")
                    print(f"Predictions (last 5):    {y_pred[-5:].tolist() if len(y_pred) >= 5 else y_pred.tolist()}")
                    print(f"y_test range: [{y_test.min():.2f}, {y_test.max():.2f}]")
                    print(f"y_pred range: [{y_pred.min():.2f}, {y_pred.max():.2f}]")
                    print(f"Fold RMSE: {rmse:.4f}")
                    print(f"Fold R²:   {r2:.4f}")
                    print(f"Fold MAE:  {mae:.4f}")
                    print(f"{'='*80}\n")
                else:
                    acc = accuracy_score(y_test, y_pred)
                    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                    fold_metrics.append({"accuracy": acc, "precision": prec, "recall": rec, "f1": f1})

            # Compute mean and std across folds
            results = {}
            if task_type == "regression":
                results['rmse_mean'] = np.mean([m['rmse'] for m in fold_metrics])
                results['rmse_std'] = np.std([m['rmse'] for m in fold_metrics])
                results['r2_mean'] = np.mean([m['r2'] for m in fold_metrics])
                results['r2_std'] = np.std([m['r2'] for m in fold_metrics])
                results['mae_mean'] = np.mean([m['mae'] for m in fold_metrics])
                results['mae_std'] = np.std([m['mae'] for m in fold_metrics])

                # Compute regional performance (quartile-based) for consensus predictions
                all_y_true_arr = np.array(all_y_true)
                all_y_pred_arr = np.array(all_y_pred)

                # Compute quartiles based on true values
                quartiles = np.percentile(all_y_true_arr, [25, 50, 75])

                # Compute RMSE for each quartile region
                regional_rmse = {}
                for i, (lower, upper) in enumerate([
                    (-np.inf, quartiles[0]),  # Q1
                    (quartiles[0], quartiles[1]),  # Q2
                    (quartiles[1], quartiles[2]),  # Q3
                    (quartiles[2], np.inf)  # Q4
                ]):
                    mask = (all_y_true_arr >= lower) & (all_y_true_arr < upper if i < 3 else all_y_true_arr >= lower)
                    if mask.sum() > 0:
                        regional_rmse[f'Q{i+1}'] = np.sqrt(mean_squared_error(
                            all_y_true_arr[mask], all_y_pred_arr[mask]
                        ))
                    else:
                        regional_rmse[f'Q{i+1}'] = np.nan

                # Store regional performance for model saving
                results['regional_rmse'] = regional_rmse
                results['y_quartiles'] = quartiles.tolist()
            else:
                results['accuracy_mean'] = np.mean([m['accuracy'] for m in fold_metrics])
                results['accuracy_std'] = np.std([m['accuracy'] for m in fold_metrics])
                results['precision_mean'] = np.mean([m['precision'] for m in fold_metrics])
                results['precision_std'] = np.std([m['precision'] for m in fold_metrics])
                results['recall_mean'] = np.mean([m['recall'] for m in fold_metrics])
                results['recall_std'] = np.std([m['recall'] for m in fold_metrics])
                results['f1_mean'] = np.mean([m['f1'] for m in fold_metrics])
                results['f1_std'] = np.std([m['f1'] for m in fold_metrics])

            # Format results with detailed diagnostics
            if task_type == "regression":
                # Add comparison to loaded model if available
                loaded_r2 = "N/A"
                r2_diff = "N/A"
                if self.selected_model_config is not None and 'R2' in self.selected_model_config:
                    loaded_r2_value = self.selected_model_config.get('R2')
                    if not pd.isna(loaded_r2_value):
                        loaded_r2 = f"{loaded_r2_value:.4f}"
                        r2_diff_value = results['r2_mean'] - loaded_r2_value
                        r2_diff = f"{r2_diff_value:+.4f}"

                # Add reproducibility notes for sensitive models
                reproducibility_note = ""
                if model_name in ["XGBoost", "LightGBM", "CatBoost"]:
                    # Check if there's a significant difference
                    if r2_diff != "N/A" and loaded_r2 != "N/A":
                        abs_diff = abs(results['r2_mean'] - float(loaded_r2))
                        if abs_diff > 0.001:
                            reproducibility_note = f"""
⚠️  REPRODUCIBILITY NOTE ({model_name}):
  Small differences are expected if parameter capture was incomplete.
  Expected variance: ±0.001 to ±0.005
  Your difference: {abs_diff:.4f}
  {'✓ Within expected range' if abs_diff <= 0.01 else '⚠️  Larger than expected - check parameter capture'}
"""
                elif model_name == "NeuralBoosted":
                    if r2_diff != "N/A" and loaded_r2 != "N/A":
                        abs_diff = abs(results['r2_mean'] - float(loaded_r2))
                        reproducibility_note = f"""
ℹ️  REPRODUCIBILITY NOTE (NeuralBoosted):
  With early_stopping=True, some variance is expected due to
  validation split differences between CV folds and full dataset.
  Expected variance: ±0.01 to ±0.02
  Your difference: {abs_diff:.4f}
  {'✓ Within expected range' if abs_diff <= 0.03 else '⚠️  Larger than expected - investigate'}
"""

                results_text = f"""Refined Model Results:

Cross-Validation Performance ({self.refine_folds.get()} folds):
  RMSE: {results['rmse_mean']:.4f} ± {results['rmse_std']:.4f}
  R²: {results['r2_mean']:.4f} ± {results['r2_std']:.4f}
  MAE: {results['mae_mean']:.4f} ± {results['mae_std']:.4f}

COMPARISON TO LOADED MODEL:
  Original R² (from Results tab): {loaded_r2}
  Refined R² (just computed):     {results['r2_mean']:.4f}
  Difference:                     {r2_diff}
{reproducibility_note}
Configuration:
  Model: {model_name}
  Task Type: {task_type}
  Preprocessing: {preprocess}
  Window Size: {window}
  Wavelengths: {wl_summary}
  Features: {len(selected_wl)}
  Samples: {X_raw.shape[0]}
  CV Folds: {self.refine_folds.get()}
  n_components: {n_components}

DEBUG INFO:
  Loaded LVs from config: {self.selected_model_config.get('LVs', 'N/A') if self.selected_model_config else 'N/A'}
  Loaded n_vars from config: {self.selected_model_config.get('n_vars', 'N/A') if self.selected_model_config else 'N/A'}
  Loaded Preprocessing: {self.selected_model_config.get('Preprocess', 'N/A') if self.selected_model_config else 'N/A'}
  Loaded Deriv: {self.selected_model_config.get('Deriv', 'N/A') if self.selected_model_config else 'N/A'}
  Loaded Window: {self.selected_model_config.get('Window', 'N/A') if self.selected_model_config else 'N/A'}
  Processing Path: {'Full-spectrum preprocessing (derivative+subset fix)' if use_full_spectrum_preprocessing else 'Standard (subset then preprocess)'}

NOTE: {'Derivative + subset detected! Using full-spectrum preprocessing to match search.py behavior and preserve derivative context.' if use_full_spectrum_preprocessing else ''}
"""
            else:
                results_text = f"""Refined Model Results:

Cross-Validation Performance ({self.refine_folds.get()} folds):
  Accuracy: {results['accuracy_mean']:.4f} ± {results['accuracy_std']:.4f}
  Precision: {results['precision_mean']:.4f} ± {results['precision_std']:.4f}
  Recall: {results['recall_mean']:.4f} ± {results['recall_std']:.4f}
  F1 Score: {results['f1_mean']:.4f} ± {results['f1_std']:.4f}

Configuration:
  Model: {model_name}
  Task Type: {task_type}
  Preprocessing: {preprocess}
  Window Size: {window}
  Wavelengths: {wl_summary}
  Features: {len(selected_wl)}
  Samples: {X_raw.shape[0]}
  CV Folds: {self.refine_folds.get()}
"""

            # Fit final pipeline on full dataset for model persistence
            # Clone the pipeline and fit on all data
            final_pipe = clone(pipe)
            final_pipe.fit(X_raw, y_array)

            # Extract model and preprocessor from pipeline for saving
            # For PLS-DA, save the entire pipeline (PLS + LogisticRegression)
            if model_name == "PLS-DA" and task_type == "classification":
                # Save entire PLS-DA pipeline (both PLS and LogisticRegression)
                final_model = final_pipe
            elif 'model' in final_pipe.named_steps:
                final_model = final_pipe.named_steps['model']
            else:
                # Fallback: save the entire pipeline
                final_model = final_pipe

            # Build preprocessor from pipeline steps (excluding the model)
            if use_full_spectrum_preprocessing:
                # For derivative + subset: preprocessor was already fitted on full spectrum
                # We need to save that preprocessor, not create a new one
                final_preprocessor = prep_pipeline  # Already fitted
                print("DEBUG: Using full-spectrum preprocessor (already fitted)")
            elif model_name == "PLS-DA" and task_type == "classification":
                # For PLS-DA: preprocessing steps are before PLS, PLS+LR are the model
                if len(pipe_steps) > 2:  # Has preprocessing + PLS + LR
                    final_preprocessor = Pipeline(pipe_steps[:-2])  # All steps except PLS and LR
                    final_preprocessor.fit(X_raw)  # Fit on raw data
                    print("DEBUG: Fitting preprocessor on subset data (PLS-DA)")
                else:
                    final_preprocessor = None
                    print("DEBUG: No preprocessor for PLS-DA (raw data)")
            elif len(pipe_steps) > 1:  # Has preprocessing steps
                final_preprocessor = Pipeline(pipe_steps[:-1])  # All steps except model
                final_preprocessor.fit(X_raw)  # Fit on raw data
                print("DEBUG: Fitting preprocessor on subset data")
            else:
                final_preprocessor = None
                print("DEBUG: No preprocessor (raw data)")

            # Store the fitted model and metadata for later saving
            self.refined_model = final_model
            self.refined_preprocessor = final_preprocessor
            self.refined_wavelengths = list(selected_wl)
            self.refined_performance = results
            self.refined_label_encoder = local_label_encoder  # Store for decoding predictions
            self.refined_X_train = X_raw  # Store training data for applicability domain
            self.refined_config = {
                'model_name': model_name,
                'task_type': task_type,
                'preprocessing': preprocess,
                'window': window,
                'n_vars': len(selected_wl),
                'n_samples': X_raw.shape[0],
                'cv_folds': n_folds,
                'use_full_spectrum_preprocessing': use_full_spectrum_preprocessing
            }

            # Store predictions for plotting
            self.refined_y_true = np.array(all_y_true)
            self.refined_y_pred = np.array(all_y_pred)
            self.refined_cv_indices = np.array(all_cv_indices)  # Store CV indices for specimen ID mapping

            # CRITICAL FIX: Store the actual specimen IDs that correspond to the CV data
            # This is needed because refined_cv_indices are relative to the filtered data (without validation set)
            # but self.y.index contains all samples (including validation set)
            self.refined_specimen_ids = y_series.index[all_cv_indices].tolist()

            # Store prediction probabilities if available
            if all_y_proba:
                self.refined_y_proba = np.concatenate(all_y_proba, axis=0)
            else:
                self.refined_y_proba = None

            # Store X data for leverage diagnostics
            self.refined_X_cv = X_raw

            # Store full wavelengths for derivative + subset case
            if use_full_spectrum_preprocessing:
                self.refined_full_wavelengths = list(all_wavelengths)
            else:
                self.refined_full_wavelengths = None

            # Update UI
            self.root.after(0, lambda: self._update_refined_results(results_text))

            # Switch to Results subtab to show the results
            self.root.after(0, lambda: self.model_dev_notebook.select(2))  # Results subtab (index 2, Features removed)

        except Exception as e:
            import traceback
            error_msg = traceback.format_exc()
            error_text = f"Error running refined model:\n\n{e}\n\n{error_msg}"
            self.root.after(0, lambda: self._update_refined_results(error_text, is_error=True))

    def _update_refined_results(self, results_text, is_error=False):
        """Update the refined results display."""
        self.refine_results_text.config(state='normal')
        self.refine_results_text.delete('1.0', tk.END)
        self.refine_results_text.insert('1.0', results_text)
        self.refine_results_text.config(state='disabled')

        # Re-enable buttons (both Configuration and Selection tabs)
        self.refine_run_button.config(state='normal')
        self.refine_run_button_selection.config(state='normal')

        if is_error:
            self.refine_status.config(text="✗ Error running refined model")
            self.refine_save_button.config(state='disabled')
            self.refine_save_button_results.config(state='disabled')
            messagebox.showerror("Error", "Failed to run refined model. See results area for details.")
        else:
            self.refine_status.config(text="✓ Refined model complete")
            # Enable Save Model button after successful run
            self.refine_save_button.config(state='normal')
            self.refine_save_button_results.config(state='normal')
            # Plot the predictions
            self._plot_refined_predictions()
            # Plot diagnostic plots
            self._plot_residual_diagnostics()
            self._plot_leverage_diagnostics()
            # Refined model complete - plots displayed

    def _save_refined_model(self):
        """Save the current refined model to a .dasp file."""
        # Check if model has been trained
        if self.refined_model is None:
            messagebox.showerror(
                "No Model Trained",
                "Please run a refined model first before saving.\n\n"
                "Click 'Run Refined Model' to train a model, then you can save it."
            )
            return

        try:
            from spectral_predict.model_io import save_model
            from datetime import datetime

            # Ask for save location
            # Create prefix: C/R for Classification/Regression + number of variables
            task_prefix = 'C' if self.refined_config['task_type'] == 'classification' else 'R'
            n_vars = self.refined_config['n_vars']
            default_name = f"{task_prefix}{n_vars}_model_{self.refined_config['model_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.dasp"

            # Get initial directory from spectral data path
            initial_dir = None
            if self.spectral_data_path.get():
                data_path = Path(self.spectral_data_path.get())
                # If it's a file, get its parent directory; if it's a directory, use it
                initial_dir = str(data_path.parent if data_path.is_file() else data_path)

            filepath = filedialog.asksaveasfilename(
                defaultextension=".dasp",
                filetypes=[("DASP Model", "*.dasp"), ("All files", "*.*")],
                initialfile=default_name,
                initialdir=initial_dir,
                title="Save Trained Model"
            )

            if not filepath:
                return  # User cancelled

            # Build comprehensive metadata
            metadata = {
                'model_name': self.refined_config['model_name'],
                'task_type': self.refined_config['task_type'],
                'preprocessing': self.refined_config['preprocessing'],
                'window': self.refined_config['window'],
                'wavelengths': self.refined_wavelengths,
                'n_vars': self.refined_config['n_vars'],
                'n_samples': self.refined_config['n_samples'],
                'cv_folds': self.refined_config['cv_folds'],
                'performance': {},
                'use_full_spectrum_preprocessing': self.refined_config.get('use_full_spectrum_preprocessing', False),
                'full_wavelengths': self.refined_full_wavelengths,  # All wavelengths for derivative+subset
                # Validation set metadata
                'validation_set_enabled': self.validation_enabled.get(),
                'validation_indices': list(self.validation_indices) if self.validation_indices else [],
                'validation_size': len(self.validation_indices) if self.validation_indices else 0,
                'validation_algorithm': self.validation_algorithm.get() if self.validation_enabled.get() else None
            }

            # Add performance metrics based on task type
            if self.refined_config['task_type'] == 'regression':
                metadata['performance'] = {
                    'RMSE': self.refined_performance.get('rmse_mean'),
                    'RMSE_std': self.refined_performance.get('rmse_std'),
                    'R2': self.refined_performance.get('r2_mean'),
                    'R2_std': self.refined_performance.get('r2_std'),
                    'MAE': self.refined_performance.get('mae_mean'),
                    'MAE_std': self.refined_performance.get('mae_std')
                }
                # Add regional performance for consensus predictions
                if 'regional_rmse' in self.refined_performance:
                    metadata['regional_rmse'] = self.refined_performance['regional_rmse']
                if 'y_quartiles' in self.refined_performance:
                    metadata['y_quartiles'] = self.refined_performance['y_quartiles']
            else:  # classification
                metadata['performance'] = {
                    'Accuracy': self.refined_performance.get('accuracy_mean'),
                    'Accuracy_std': self.refined_performance.get('accuracy_std'),
                    'Precision': self.refined_performance.get('precision_mean'),
                    'Precision_std': self.refined_performance.get('precision_std'),
                    'Recall': self.refined_performance.get('recall_mean'),
                    'Recall_std': self.refined_performance.get('recall_std'),
                    'F1': self.refined_performance.get('f1_mean'),
                    'F1_std': self.refined_performance.get('f1_std')
                }

            # Save the model
            # Use refined_label_encoder if available (from Model Development tab),
            # otherwise fallback to global label_encoder (from Results tab)
            label_encoder_to_save = getattr(self, 'refined_label_encoder', None) or self.label_encoder

            # Prepare CV data for uncertainty estimation
            cv_residuals = None
            cv_predictions = None
            cv_actuals = None

            if hasattr(self, 'refined_y_true') and hasattr(self, 'refined_y_pred'):
                cv_actuals = np.array(self.refined_y_true)
                cv_predictions = np.array(self.refined_y_pred)

                if self.refined_config['task_type'] == 'regression':
                    # For regression: residuals = predictions - actuals
                    cv_residuals = cv_predictions - cv_actuals
                elif self.refined_config['task_type'] == 'classification':
                    # For classification: store probabilities if available
                    if hasattr(self, 'refined_y_proba'):
                        cv_residuals = np.array(self.refined_y_proba)  # Store probabilities as "residuals"

            # Get training data for applicability domain (if available)
            X_train = getattr(self, 'refined_X_train', None)
            if X_train is not None:
                print(f"DEBUG: Passing X_train to save_model (shape: {X_train.shape})")
            else:
                print("DEBUG: No X_train available - model will not have applicability domain data")

            save_model(
                model=self.refined_model,
                preprocessor=self.refined_preprocessor,
                metadata=metadata,
                filepath=filepath,
                label_encoder=label_encoder_to_save,
                cv_residuals=cv_residuals,
                cv_predictions=cv_predictions,
                cv_actuals=cv_actuals,
                X_train=X_train
            )

            # Model saved successfully - update status
            self.refine_status.config(text=f"✓ Model saved to {Path(filepath).name}")

        except Exception as e:
            import traceback
            error_msg = traceback.format_exc()
            messagebox.showerror(
                "Save Error",
                f"Failed to save model:\n\n{str(e)}\n\nSee console for details."
            )
            print(f"Error saving model:\n{error_msg}")

    def _format_wavelengths_as_spec(self, wavelengths):
        """
        Format a list of wavelengths into a compact specification string.
        Groups consecutive wavelengths into ranges.

        Example: [1500, 1501, 1502, 1505, 1506, 1510]
                 -> "1500.0-1502.0, 1505.0-1506.0, 1510.0"
        """
        # Enhanced validation
        if wavelengths is None:
            print("WARNING: _format_wavelengths_as_spec received None")
            return ""

        # Convert numpy array to list if needed
        if hasattr(wavelengths, 'tolist'):
            wavelengths = wavelengths.tolist()

        # Ensure it's a list
        if not isinstance(wavelengths, list):
            try:
                wavelengths = list(wavelengths)
            except Exception as e:
                print(f"ERROR: Cannot convert wavelengths to list: {e}")
                return ""

        if len(wavelengths) == 0:
            print("WARNING: _format_wavelengths_as_spec received empty list")
            return ""

        wavelengths = sorted(list(set(wavelengths)))  # Remove duplicates and sort

        # Group consecutive wavelengths (within 1.5 nm)
        ranges = []
        start = wavelengths[0]
        end = wavelengths[0]

        for i in range(1, len(wavelengths)):
            if wavelengths[i] - end <= 1.5:  # Consecutive
                end = wavelengths[i]
            else:
                # Save the range
                if abs(end - start) < 0.1:  # Single wavelength
                    ranges.append(f"{start:.1f}")
                else:  # Range
                    ranges.append(f"{start:.1f}-{end:.1f}")
                start = wavelengths[i]
                end = wavelengths[i]

        # Don't forget the last range
        if abs(end - start) < 0.1:
            ranges.append(f"{start:.1f}")
        else:
            ranges.append(f"{start:.1f}-{end:.1f}")

        return ", ".join(ranges)

    def _parse_wavelength_spec(self, spec_text, available_wavelengths):
        """
        Parse wavelength specification string into list of wavelengths.

        Format: "1920, 1930-1940, 1950, 1960-2000"
        - Individual wavelengths: 1920, 1950
        - Ranges: 1930-1940, 1960-2000
        - Comments: Lines starting with # are ignored

        Returns list of wavelengths that exist in available_wavelengths.
        """
        selected = []
        spec_text = spec_text.strip()

        if not spec_text:
            return list(available_wavelengths)  # Return all if empty

        # Remove comment lines (lines starting with #)
        lines = spec_text.split('\n')
        clean_lines = [line for line in lines if not line.strip().startswith('#')]
        spec_text = ' '.join(clean_lines)

        # Split by commas
        parts = [p.strip() for p in spec_text.split(',')]

        for part in parts:
            if '-' in part and not part.startswith('-'):
                # Range specification
                try:
                    start, end = part.split('-')
                    start_wl = float(start.strip())
                    end_wl = float(end.strip())

                    # Find wavelengths in this range
                    for wl in available_wavelengths:
                        if start_wl <= wl <= end_wl:
                            selected.append(wl)
                except ValueError:
                    # Invalid range, skip
                    continue
            else:
                # Individual wavelength
                try:
                    wl = float(part.strip())
                    # Find closest wavelength in available
                    if wl in available_wavelengths:
                        selected.append(wl)
                    else:
                        # Find closest match
                        closest = min(available_wavelengths, key=lambda x: abs(x - wl))
                        if abs(closest - wl) < 5:  # Within 5 nm tolerance
                            selected.append(closest)
                except ValueError:
                    # Invalid wavelength, skip
                    continue

        # Remove duplicates while preserving order from available_wavelengths
        # This matches the order used during search/training (stored in all_vars)
        # DO NOT sort - sorting changes feature order and breaks R² reproducibility!
        selected_set = set(selected)
        selected = [wl for wl in available_wavelengths if wl in selected_set]
        return selected

    def _parse_range_specification(self, spec_string, param_name="parameter", is_float=False):
        """
        Parse flexible range specifications for hyperparameters.

        Supports:
        - Single values: "150" → [150]
        - Lists: "50, 100, 200" → [50, 100, 200]
        - Ranges: "50-200 step 50" → [50, 100, 150, 200]
        - Mixed: "10, 50-100 step 25, 200" → [10, 50, 75, 100, 200]
        - None values: "None, 10, 20" → [None, 10, 20]
        - Float values: "0.01, 0.1, 1.0" → [0.01, 0.1, 1.0]
        - String values: "relu, tanh, sigmoid" → ['relu', 'tanh', 'sigmoid']

        Args:
            spec_string: String specification to parse
            param_name: Parameter name for error messages (default: "parameter")
            is_float: If True, parse as floats; if False, parse as ints (default: False)

        Returns:
            list: Parsed values (sorted, unique, appropriate type)

        Raises:
            ValueError: On invalid syntax or unparseable values
        """
        # Handle empty/whitespace-only strings
        if not spec_string or not spec_string.strip():
            return []

        spec_string = spec_string.strip()
        values = []
        has_strings = False

        # Split by commas to handle mixed formats
        segments = [seg.strip() for seg in spec_string.split(',')]

        for segment in segments:
            if not segment:
                continue

            try:
                # Check if this is a range specification with "step"
                if ' step ' in segment.lower():
                    # Parse range: "start-end step increment"
                    parts = segment.lower().split(' step ')
                    if len(parts) != 2:
                        raise ValueError(f"Invalid range syntax in '{segment}'. Expected format: 'start-end step increment'")

                    range_part = parts[0].strip()
                    step_part = parts[1].strip()

                    # Parse the range bounds
                    # Use regex to properly handle negative numbers
                    import re
                    # Match pattern: optional_minus + digits + dash + optional_minus + digits
                    # This handles cases like: "10-20", "-10-20", "10--20", "-10--20"
                    match = re.match(r'^(-?\d+(?:\.\d+)?)-(-?\d+(?:\.\d+)?)$', range_part)

                    if not match:
                        raise ValueError(f"Invalid range syntax in '{segment}'. Expected 'start-end' format")

                    start_str = match.group(1)
                    end_str = match.group(2)

                    # Parse the numeric values
                    try:
                        if is_float:
                            start = float(start_str)
                            end = float(end_str)
                            step = float(step_part)
                        else:
                            start = int(start_str)
                            end = int(end_str)
                            step = int(step_part)
                    except ValueError as e:
                        raise ValueError(f"Could not parse numeric values in '{segment}': {e}")

                    # Validate step
                    if step == 0:
                        raise ValueError(f"Step cannot be zero in '{segment}'")

                    if step < 0:
                        raise ValueError(f"Step must be positive in '{segment}' (found {step})")

                    # Generate range values
                    if start <= end:
                        current = start
                        while current <= end:
                            values.append(current)
                            if is_float:
                                # Use round to avoid floating point precision issues
                                current = round(current + step, 10)
                            else:
                                current = current + step
                    else:
                        # Descending range
                        current = start
                        while current >= end:
                            values.append(current)
                            if is_float:
                                current = round(current - step, 10)
                            else:
                                current = current - step

                # Check if it's None
                elif segment.lower() == 'none':
                    values.append(None)

                # Check if it's a string value (contains letters and is not "None")
                elif any(c.isalpha() for c in segment) and segment.lower() != 'none':
                    # String value - keep as is
                    values.append(segment)
                    has_strings = True

                # Try to parse as numeric value
                else:
                    try:
                        if is_float:
                            values.append(float(segment))
                        else:
                            values.append(int(segment))
                    except ValueError:
                        raise ValueError(f"Could not parse '{segment}' as {'float' if is_float else 'int'} for {param_name}")

            except ValueError as e:
                # Re-raise with parameter context
                raise ValueError(f"Error parsing {param_name} specification: {e}")

        # Remove duplicates while preserving None values
        if has_strings:
            # For string values, preserve order and remove duplicates
            seen = set()
            unique_values = []
            for v in values:
                if v not in seen:
                    seen.add(v)
                    unique_values.append(v)
            return unique_values
        else:
            # For numeric values, sort and remove duplicates
            # Separate None from numeric values
            none_values = [v for v in values if v is None]
            numeric_values = [v for v in values if v is not None]

            # Remove duplicates and sort numeric values
            numeric_values = sorted(list(set(numeric_values)))

            # Combine: None values first, then sorted numeric values
            result = (none_values[:1] if none_values else []) + numeric_values
            return result

    def _create_parameter_grid_control(self, parent, param_name, param_label,
                                       checkbox_values, default_checked=None,
                                       is_float=False, allow_string_values=False,
                                       help_text=None):
        """
        Create unified hyperparameter control with checkboxes + custom entry.

        Args:
            parent: Parent widget (Frame or LabelFrame)
            param_name: Parameter name (e.g., 'n_estimators')
            param_label: Display label (e.g., 'Number of Trees')
            checkbox_values: List of common values to show as checkboxes
            default_checked: List of values to check by default (optional)
            is_float: If True, parse custom entry as floats
            allow_string_values: If True, allow non-numeric values
            help_text: Tooltip text (optional)

        Returns:
            dict: {
                'checkboxes': {value: BooleanVar},
                'custom_entry': StringVar,
                'frame': Frame,
                'label': Label
            }
        """
        # Create main container frame
        control_frame = tk.Frame(parent, bg=self.colors['card_bg'])
        control_frame.pack(fill='x', padx=5, pady=5)

        # Create header row with label and custom entry
        header_frame = tk.Frame(control_frame, bg=self.colors['card_bg'])
        header_frame.pack(fill='x', pady=(0, 5))

        # Label on left
        label = ttk.Label(header_frame, text=param_label,
                         font=('Arial', 10, 'bold'))
        label.pack(side='left', padx=(0, 10))

        # Add help tooltip if provided
        if help_text:
            CreateToolTip(label, text=help_text, delay=500)

        # Custom entry on right
        custom_var = tk.StringVar()
        custom_entry = ttk.Entry(header_frame, textvariable=custom_var, width=30)
        custom_entry.pack(side='right')

        # Add placeholder text
        placeholder = "e.g., 10, 50-200 step 50, 500"
        custom_entry.insert(0, placeholder)
        custom_entry.config(foreground='gray')

        def on_entry_focus_in(event):
            if custom_var.get() == placeholder:
                custom_entry.delete(0, 'end')
                custom_entry.config(foreground=self.colors['text'])

        def on_entry_focus_out(event):
            if not custom_var.get():
                custom_entry.insert(0, placeholder)
                custom_entry.config(foreground='gray')

        custom_entry.bind('<FocusIn>', on_entry_focus_in)
        custom_entry.bind('<FocusOut>', on_entry_focus_out)

        # Create checkbox row
        checkbox_frame = tk.Frame(control_frame, bg=self.colors['card_bg'])
        checkbox_frame.pack(fill='x')

        # Create checkboxes for common values
        checkbox_vars = {}
        for i, value in enumerate(checkbox_values):
            var = tk.BooleanVar()

            # Check if this value should be checked by default
            if default_checked and value in default_checked:
                var.set(True)

            # Create checkbox with appropriate label
            if value is None:
                label_text = "None"
            elif isinstance(value, str):
                label_text = str(value)
            elif is_float:
                label_text = f"{value:.3g}"  # Smart float formatting
            else:
                label_text = str(value)

            cb = ttk.Checkbutton(checkbox_frame, text=label_text,
                               variable=var, style='TCheckbutton')
            cb.pack(side='left', padx=5)

            checkbox_vars[value] = var

        # Return control dict
        return {
            'checkboxes': checkbox_vars,
            'custom_entry': custom_var,
            'frame': control_frame,
            'label': label,
            'entry_widget': custom_entry,
            'placeholder': placeholder
        }

    def _extract_parameter_values(self, control_dict, param_name, is_float=False,
                                  allow_string_values=False):
        """
        Extract final parameter list from control dict.

        Combines checked checkbox values + parsed custom entry values.

        Args:
            control_dict: Dict returned by _create_parameter_grid_control
            param_name: Parameter name for error messages
            is_float: If True, expect floats
            allow_string_values: If True, allow string values

        Returns:
            list: Sorted, unique parameter values

        Raises:
            ValueError: If parsing fails
        """
        values = []

        # Step 1: Collect checked checkbox values
        checkboxes = control_dict.get('checkboxes', {})
        for value, var in checkboxes.items():
            if var.get():  # If checkbox is checked
                values.append(value)

        # Step 2: Parse custom entry
        custom_var = control_dict.get('custom_entry')
        if custom_var:
            custom_text = custom_var.get().strip()

            # Skip if it's the placeholder text
            placeholder = control_dict.get('placeholder', '')
            if custom_text and custom_text != placeholder:
                try:
                    # Use the existing _parse_range_specification method
                    parsed_values = self._parse_range_specification(
                        custom_text,
                        param_name=param_name,
                        is_float=is_float
                    )
                    values.extend(parsed_values)
                except ValueError as e:
                    # Re-raise with more context
                    raise ValueError(f"Error parsing custom values for {param_name}: {e}")

        # Step 3: Validate that we have at least one value
        if not values:
            raise ValueError(f"No values specified for {param_name}. Please select at least one checkbox or enter custom values.")

        # Step 4: Remove duplicates and sort
        if allow_string_values or any(isinstance(v, str) for v in values):
            # For string values, preserve order and remove duplicates
            seen = set()
            unique_values = []
            for v in values:
                if v not in seen:
                    seen.add(v)
                    unique_values.append(v)
            return unique_values
        else:
            # For numeric values, separate None from numeric values
            none_values = [v for v in values if v is None]
            numeric_values = [v for v in values if v is not None]

            # Remove duplicates and sort numeric values
            numeric_values = sorted(list(set(numeric_values)))

            # Combine: None values first, then sorted numeric values
            result = (none_values[:1] if none_values else []) + numeric_values
            return result

    def _preview_wavelength_selection(self):
        """Preview the wavelength selection with a plot."""
        if self.X_original is None:
            messagebox.showwarning("No Data", "Please load data first to preview wavelength selection.")
            return

        # Get available wavelengths
        available_wl = self.X_original.columns.astype(float).values

        # Parse wavelength specification
        spec_text = self.refine_wl_spec.get('1.0', 'end')
        try:
            selected_wl = self._parse_wavelength_spec(spec_text, available_wl)
        except Exception as e:
            messagebox.showerror("Parse Error", f"Error parsing wavelength specification:\n{e}")
            return

        if not selected_wl:
            messagebox.showwarning("No Wavelengths", "No valid wavelengths found in specification.")
            return

        # Create preview window
        preview_window = tk.Toplevel(self.root)
        preview_window.title("Wavelength Selection Preview")
        preview_window.geometry("800x500")

        # Info text
        info_text = f"Selected {len(selected_wl)} wavelengths out of {len(available_wl)} available"
        ttk.Label(preview_window, text=info_text, font=('Arial', 12, 'bold')).pack(pady=10)

        # Create plot
        if HAS_MATPLOTLIB:
            fig = Figure(figsize=(10, 4))
            ax = fig.add_subplot(111)

            # Create binary indicator (1 = selected, 0 = not selected)
            selected_set = set(selected_wl)
            indicators = [1 if wl in selected_set else 0 for wl in available_wl]

            # Plot
            ax.fill_between(available_wl, 0, indicators, alpha=0.3, color='blue', label='Selected')
            ax.plot(available_wl, indicators, 'b-', linewidth=0.5)
            ax.set_xlabel('Wavelength (nm)', fontsize=10)
            ax.set_ylabel('Selected', fontsize=10)
            ax.set_title(f'Wavelength Selection: {len(selected_wl)}/{len(available_wl)} wavelengths', fontsize=12)
            ax.set_ylim(-0.1, 1.1)
            ax.grid(True, alpha=0.3)
            ax.legend()

            # Embed plot
            canvas = FigureCanvasTkAgg(fig, master=preview_window)
            canvas.draw()
            canvas.get_tk_widget().pack(fill='both', expand=True, padx=10, pady=10)

            # Add export button to preview window
            self._add_plot_export_button(preview_window, fig, "wavelength_selection")

        # Show wavelength list in text box
        list_frame = ttk.LabelFrame(preview_window, text="Selected Wavelengths", padding="10")
        list_frame.pack(fill='both', expand=True, padx=10, pady=10)

        wl_text = tk.Text(list_frame, height=5, font=('Consolas', 9), wrap=tk.WORD,
                         bg=self.colors['panel'], fg=self.colors['text'],
                         relief='flat', borderwidth=0,
                         selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        wl_text.pack(fill='both', expand=True)

        # Format wavelengths nicely
        wl_str = ', '.join([f"{wl:.1f}" for wl in selected_wl[:50]])  # Show first 50
        if len(selected_wl) > 50:
            wl_str += f", ... ({len(selected_wl) - 50} more)"
        wl_text.insert('1.0', wl_str)
        wl_text.config(state='disabled')

        # Close button
        ttk.Button(preview_window, text="Close", command=preview_window.destroy, style='Modern.TButton').pack(pady=10)


    def _update_wavelength_count(self, event=None):
        """Update wavelength count display in real-time."""
        try:
            if self.X_original is None:
                self.refine_wl_count_label.config(text="Wavelengths: Data not loaded")
                return

            wl_spec_text = self.refine_wl_spec.get('1.0', 'end')
            available_wl = self.X_original.columns.astype(float).values
            selected_wl = self._parse_wavelength_spec(wl_spec_text, available_wl)

            count = len(selected_wl)
            if count > 0:
                range_text = f"({selected_wl[0]:.1f} - {selected_wl[-1]:.1f} nm)"
                self.refine_wl_count_label.config(text=f"Wavelengths: {count} selected {range_text}")
            else:
                self.refine_wl_count_label.config(text="Wavelengths: 0 selected (check specification)")
        except Exception as e:
            self.refine_wl_count_label.config(text="Wavelengths: Invalid specification")
            print(f"DEBUG: Error updating wavelength count: {e}")

    def _apply_wl_preset(self, preset_type):
        """Apply wavelength preset."""
        if self.X_original is None:
            # No data loaded - preset cannot be applied
            return

        wavelengths = self.X_original.columns.astype(float).values

        if preset_type == 'all':
            selected = wavelengths
        elif preset_type == 'nir':
            selected = [wl for wl in wavelengths if wl >= 780]
        elif preset_type == 'visible':
            selected = [wl for wl in wavelengths if 400 <= wl <= 780]
        else:
            return

        if len(selected) == 0:
            # No wavelengths in this range - preset not applied
            return

        wl_spec = self._format_wavelengths_as_spec(list(selected))
        self.refine_wl_spec.delete('1.0', 'end')
        self.refine_wl_spec.insert('1.0', wl_spec)
        self._update_wavelength_count()

    def _custom_range_dialog(self):
        """Show dialog for custom wavelength range."""
        dialog = tk.Toplevel(self.root)
        dialog.title("Custom Wavelength Range")
        dialog.geometry("350x180")
        dialog.transient(self.root)
        dialog.grab_set()

        ttk.Label(dialog, text="Start wavelength (nm):", padding=5).pack(pady=5)
        start_var = tk.StringVar()
        ttk.Entry(dialog, textvariable=start_var, width=20).pack()

        ttk.Label(dialog, text="End wavelength (nm):", padding=5).pack(pady=5)
        end_var = tk.StringVar()
        ttk.Entry(dialog, textvariable=end_var, width=20).pack()

        def apply_range():
            try:
                start = float(start_var.get())
                end = float(end_var.get())

                if self.X_original is None:
                    messagebox.showwarning("No Data", "Please load data first")
                    dialog.destroy()
                    return

                wavelengths = self.X_original.columns.astype(float).values
                selected = [wl for wl in wavelengths if start <= wl <= end]

                if len(selected) == 0:
                    messagebox.showwarning("No Wavelengths",
                                          f"No wavelengths found in range {start}-{end} nm")
                    return

                wl_spec = self._format_wavelengths_as_spec(selected)
                self.refine_wl_spec.delete('1.0', 'end')
                self.refine_wl_spec.insert('1.0', wl_spec)
                self._update_wavelength_count()
                dialog.destroy()

            except ValueError:
                messagebox.showerror("Invalid Input", "Please enter valid numbers")

        button_frame = ttk.Frame(dialog)
        button_frame.pack(pady=10)
        ttk.Button(button_frame, text="Apply", command=apply_range, style='Modern.TButton').pack(side='left', padx=5)
        ttk.Button(button_frame, text="Cancel", command=dialog.destroy, style='Modern.TButton').pack(side='left', padx=5)

    # === Tab 8: Model Prediction Methods ===

    def _create_tab8_model_prediction(self):
        """Create Tab 8: Model Prediction - Load models and make predictions on new data."""
        self.tab8 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab8, text='  🔮 Model Prediction  ')

        # Create notebook for subtabs
        self.prediction_notebook = ttk.Notebook(self.tab8)
        self.prediction_notebook.pack(fill='both', expand=True)

        # Create subtabs
        self._create_tab8a_setup()
        self._create_tab8b_results()

    def _create_tab8a_setup(self):
        """Subtab 8A: Setup - Load models and data, configure predictions."""
        # Create subtab frame with scrolling
        tab8a = ttk.Frame(self.prediction_notebook, style='TFrame')
        self.prediction_notebook.add(tab8a, text='  ⚙️ Setup  ')

        # Create scrollable content
        canvas = tk.Canvas(tab8a, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab8a, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab8", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # Instructions
        ttk.Label(content_frame,
            text="Load saved .dasp model files and apply them to new spectral data for predictions.",
            style='Caption.TLabel').grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 20))
        row += 1

        # === Step 1: Load Models ===
        step1_frame = ttk.LabelFrame(content_frame, text="Step 1: Load Saved Models", padding="20")
        step1_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), padx=20, pady=10)
        row += 1

        # Button frame for load and clear
        button_frame1 = ttk.Frame(step1_frame)
        button_frame1.grid(row=0, column=0, columnspan=2, pady=5)

        ttk.Button(button_frame1, text="📂 Load Model File(s)",
                   command=self._load_model_for_prediction, style='Modern.TButton').pack(side='left', padx=5)
        ttk.Button(button_frame1, text="🗑️ Clear All Models",
                   command=self._clear_loaded_models, style='Modern.TButton').pack(side='left', padx=5)

        # Loaded models display
        ttk.Label(step1_frame, text="Loaded Models:", style='Subheading.TLabel').grid(
            row=1, column=0, columnspan=2, sticky=tk.W, pady=(10, 5))

        models_text_frame = ttk.Frame(step1_frame)
        models_text_frame.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)

        self.loaded_models_text = tk.Text(models_text_frame, height=8, width=90,
                                          font=('Consolas', 9),
                                          bg=self.colors['panel'], fg=self.colors['text'],
                                          wrap=tk.WORD, state='disabled',
                                          relief='flat', borderwidth=0,
                                          selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        self.loaded_models_text.pack(side='left', fill='both', expand=True)

        models_scrollbar = ttk.Scrollbar(models_text_frame, orient='vertical',
                                        command=self.loaded_models_text.yview)
        models_scrollbar.pack(side='right', fill='y')
        self.loaded_models_text.config(yscrollcommand=models_scrollbar.set)

        # === Step 2: Load Data ===
        step2_frame = ttk.LabelFrame(content_frame, text="Step 2: Load Data for Prediction", padding="20")
        step2_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), padx=20, pady=10)
        row += 1

        # Data source selection
        ttk.Label(step2_frame, text="Data Source:", style='Subheading.TLabel').grid(
            row=0, column=0, sticky=tk.W, pady=5)

        self.pred_data_source = tk.StringVar(value='directory')
        source_frame = ttk.Frame(step2_frame)
        source_frame.grid(row=1, column=0, columnspan=2, sticky=tk.W, pady=5)

        ttk.Radiobutton(source_frame, text="Directory (ASD/SPC)",
                       variable=self.pred_data_source, value='directory',
                       command=self._on_pred_source_change).pack(side='left', padx=5)
        ttk.Radiobutton(source_frame, text="CSV File",
                       variable=self.pred_data_source, value='csv',
                       command=self._on_pred_source_change).pack(side='left', padx=5)
        ttk.Radiobutton(source_frame, text="Use Pre-Selected Validation Set 🔬",
                       variable=self.pred_data_source, value='validation',
                       command=self._on_pred_source_change).pack(side='left', padx=5)

        # File path entry
        self.pred_path_label = ttk.Label(step2_frame, text="Path:", style='Caption.TLabel')
        self.pred_path_label.grid(row=2, column=0, sticky=tk.W, pady=(10, 5))

        path_frame = ttk.Frame(step2_frame)
        path_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)

        self.pred_data_path = tk.StringVar()
        self.pred_path_entry = ttk.Entry(path_frame, textvariable=self.pred_data_path, width=60)
        self.pred_path_entry.pack(side='left', fill='x', expand=True)
        self.pred_browse_button = ttk.Button(path_frame, text="Browse...",
                   command=self._browse_prediction_data, style='Modern.TButton')
        self.pred_browse_button.pack(side='left', padx=5)

        # Load button and status
        button_frame2 = ttk.Frame(step2_frame)
        button_frame2.grid(row=4, column=0, columnspan=2, pady=10)

        ttk.Button(button_frame2, text="📊 Load Data",
                   command=self._load_prediction_data, style='Modern.TButton').pack(side='left', padx=5)

        self.pred_data_status = ttk.Label(step2_frame, text="No data loaded", style='Caption.TLabel')
        self.pred_data_status.grid(row=5, column=0, columnspan=2, pady=5)

        # === Step 3: Run Predictions ===
        step3_frame = ttk.LabelFrame(content_frame, text="Step 3: Make Predictions", padding="20")
        step3_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), padx=20, pady=10)
        row += 1

        button_frame3 = ttk.Frame(step3_frame)
        button_frame3.grid(row=0, column=0, columnspan=2, pady=5)

        self._create_accent_button(button_frame3, "🚀 Run All Models",
                                    self._run_predictions).pack(side='left', padx=5)
        ttk.Button(button_frame3, text="📥 Export to CSV",
                   command=self._export_predictions, style='Modern.TButton').pack(side='left', padx=5)

        # Progress bar
        self.pred_progress = ttk.Progressbar(step3_frame, mode='determinate', length=400)
        self.pred_progress.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)

        # Status label
        self.pred_status = ttk.Label(step3_frame, text="Ready", style='Caption.TLabel')
        self.pred_status.grid(row=2, column=0, columnspan=2)

    def _create_tab8b_results(self):
        """Subtab 8B: Results - View prediction results and statistics."""
        # Create subtab frame
        tab8b = ttk.Frame(self.prediction_notebook, style='TFrame')
        self.prediction_notebook.add(tab8b, text='  📊 Results  ')

        # Create scrollable content
        canvas = tk.Canvas(tab8b, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(tab8b, orient="vertical", command=canvas.yview)
        content_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        content_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab8b", canvas))
        canvas.create_window((0, 0), window=content_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        row = 0

        # === View Results ===
        step4_frame = ttk.LabelFrame(content_frame, text="Prediction Results", padding="20")
        step4_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), padx=20, pady=10)
        content_frame.grid_rowconfigure(row, weight=1)
        row += 1

        # Predictions table
        ttk.Label(step4_frame, text="Prediction Results:", style='Subheading.TLabel').grid(
            row=0, column=0, sticky=tk.W, pady=(0, 5))

        tree_frame = ttk.Frame(step4_frame)
        tree_frame.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        step4_frame.grid_rowconfigure(1, weight=1)
        step4_frame.grid_columnconfigure(0, weight=1)

        self.predictions_tree = ttk.Treeview(tree_frame, height=12, show='headings')
        self.predictions_tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))

        # Scrollbars for treeview
        vsb = ttk.Scrollbar(tree_frame, orient="vertical", command=self.predictions_tree.yview)
        vsb.grid(row=0, column=1, sticky=(tk.N, tk.S))
        self.predictions_tree.configure(yscrollcommand=vsb.set)

        hsb = ttk.Scrollbar(tree_frame, orient="horizontal", command=self.predictions_tree.xview)
        hsb.grid(row=1, column=0, sticky=(tk.W, tk.E))
        self.predictions_tree.configure(xscrollcommand=hsb.set)

        tree_frame.grid_rowconfigure(0, weight=1)
        tree_frame.grid_columnconfigure(0, weight=1)

        # Statistics display
        ttk.Label(step4_frame, text="Statistics:", style='Subheading.TLabel').grid(
            row=2, column=0, sticky=tk.W, pady=(15, 5))

        stats_text_frame = ttk.Frame(step4_frame)
        stats_text_frame.grid(row=3, column=0, sticky=(tk.W, tk.E), pady=5)

        self.pred_stats_text = tk.Text(stats_text_frame, height=10, width=90,
                                       font=('Consolas', 9),
                                       bg=self.colors['panel'], fg=self.colors['text'],
                                       wrap=tk.WORD, state='disabled',
                                       relief='flat', borderwidth=0,
                                       selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        self.pred_stats_text.pack(side='left', fill='both', expand=True)

        stats_scrollbar = ttk.Scrollbar(stats_text_frame, orient='vertical',
                                       command=self.pred_stats_text.yview)
        stats_scrollbar.pack(side='right', fill='y')
        self.pred_stats_text.config(yscrollcommand=stats_scrollbar.set)

        # Consensus information display
        ttk.Label(step4_frame, text="Consensus Details:", style='Subheading.TLabel').grid(
            row=4, column=0, sticky=tk.W, pady=(15, 5))

        consensus_info_frame = ttk.Frame(step4_frame)
        consensus_info_frame.grid(row=5, column=0, sticky=(tk.W, tk.E), pady=5)

        self.consensus_info_text = tk.Text(consensus_info_frame, height=8, width=90,
                                           font=('Consolas', 9),
                                           bg=self.colors['panel'], fg=self.colors['text'],
                                           wrap=tk.WORD, state='disabled',
                                           relief='flat', borderwidth=0,
                                           selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
        self.consensus_info_text.pack(side='left', fill='both', expand=True)

        consensus_scrollbar = ttk.Scrollbar(consensus_info_frame, orient='vertical',
                                           command=self.consensus_info_text.yview)
        consensus_scrollbar.pack(side='right', fill='y')
        self.consensus_info_text.config(yscrollcommand=consensus_scrollbar.set)

        # === Prediction Uncertainty ===
        ttk.Label(step4_frame, text="Prediction Uncertainty:", style='Subheading.TLabel').grid(
            row=6, column=0, sticky=tk.W, pady=(15, 5))

        uncertainty_frame = ttk.Frame(step4_frame)
        uncertainty_frame.grid(row=7, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)

        # Uncertainty table with scrollbars
        uncertainty_tree_frame = ttk.Frame(uncertainty_frame)
        uncertainty_tree_frame.pack(fill='both', expand=True)

        self.uncertainty_tree = ttk.Treeview(uncertainty_tree_frame, height=10, show='headings')
        self.uncertainty_tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))

        unc_vsb = ttk.Scrollbar(uncertainty_tree_frame, orient="vertical", command=self.uncertainty_tree.yview)
        unc_vsb.grid(row=0, column=1, sticky=(tk.N, tk.S))
        self.uncertainty_tree.configure(yscrollcommand=unc_vsb.set)

        unc_hsb = ttk.Scrollbar(uncertainty_tree_frame, orient="horizontal", command=self.uncertainty_tree.xview)
        unc_hsb.grid(row=1, column=0, sticky=(tk.W, tk.E))
        self.uncertainty_tree.configure(xscrollcommand=unc_hsb.set)

        uncertainty_tree_frame.grid_rowconfigure(0, weight=1)
        uncertainty_tree_frame.grid_columnconfigure(0, weight=1)

        # Placeholder for uncertainty
        self.uncertainty_placeholder = ttk.Label(uncertainty_frame,
                                                 text="Run predictions to see uncertainty estimates (prediction intervals for regression, confidence scores for classification)",
                                                 style='Caption.TLabel')
        # Don't pack yet - will show/hide dynamically

        # === Step 5: Prediction Plots (Only for Validation Set) ===
        ttk.Label(step4_frame, text="Prediction Plots (Validation Set Only):", style='Subheading.TLabel').grid(
            row=8, column=0, sticky=tk.W, pady=(15, 5))

        prediction_plots_frame = ttk.Frame(step4_frame)
        prediction_plots_frame.grid(row=9, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=5)
        step4_frame.grid_rowconfigure(9, weight=1)

        self.prediction_plots_frame = ttk.Frame(prediction_plots_frame)
        self.prediction_plots_frame.pack(fill='both', expand=True)

        # Placeholder text
        self.pred_plot_placeholder = ttk.Label(self.prediction_plots_frame,
                                               text="Load validation set and run predictions to see Reference vs Predicted plots for all models",
                                               style='Caption.TLabel')
        self.pred_plot_placeholder.pack(pady=20)

    def _load_model_for_prediction(self):
        """Browse and load one or more .dasp model files (individual or ensemble)."""
        filepaths = filedialog.askopenfilenames(
            title="Select DASP Model File(s)",
            filetypes=[("DASP Model", "*.dasp"), ("All files", "*.*")]
        )

        if not filepaths:
            return

        try:
            from spectral_predict.model_io import load_model, load_ensemble
            import zipfile

            # Load each model
            loaded_count = 0
            failed_models = []

            for filepath in filepaths:
                try:
                    # Check if this is an ensemble file
                    is_ensemble = False
                    try:
                        with zipfile.ZipFile(filepath, 'r') as zf:
                            if 'ensemble_config.json' in zf.namelist():
                                is_ensemble = True
                    except:
                        pass

                    if is_ensemble:
                        # Load as ensemble
                        ensemble_dict = load_ensemble(filepath)

                        # Create model_dict format compatible with existing code
                        model_dict = {
                            'model': ensemble_dict['ensemble'],
                            'metadata': ensemble_dict['metadata'],
                            'preprocessor': None,  # Ensembles have preprocessing in base models
                            'filepath': filepath,
                            'filename': Path(filepath).name,
                            'is_ensemble': True,
                            'ensemble_type': ensemble_dict['config']['ensemble_type'],
                            'ensemble_name': ensemble_dict['config']['ensemble_name'],
                            'model_names': ensemble_dict['model_names'],
                            'base_model_dicts': ensemble_dict.get('base_model_dicts', [])  # For applicability domain
                        }
                    else:
                        # Load as individual model
                        model_dict = load_model(filepath)

                        # Add file information
                        model_dict['filepath'] = filepath
                        model_dict['filename'] = Path(filepath).name
                        model_dict['is_ensemble'] = False

                    # Add to loaded models list
                    self.loaded_models.append(model_dict)
                    loaded_count += 1

                except Exception as e:
                    failed_models.append((Path(filepath).name, str(e)))

            # Update display
            self._update_loaded_models_display()

            # Show appropriate message
            if loaded_count > 0 and not failed_models:
                # Models loaded successfully - display updated
                pass
            elif loaded_count > 0 and failed_models:
                error_msg = f"Successfully loaded {loaded_count} model(s), but {len(failed_models)} failed:\n\n"
                for name, error in failed_models[:3]:  # Show first 3 failures
                    error_msg += f"- {name}: {error}\n"
                if len(failed_models) > 3:
                    error_msg += f"... and {len(failed_models) - 3} more"
                messagebox.showwarning("Partial Success", error_msg)
            else:
                error_msg = f"Failed to load all {len(failed_models)} model(s):\n\n"
                for name, error in failed_models[:3]:
                    error_msg += f"- {name}: {error}\n"
                if len(failed_models) > 3:
                    error_msg += f"... and {len(failed_models) - 3} more"
                messagebox.showerror("Load Error", error_msg)

        except Exception as e:
            messagebox.showerror("Load Error",
                f"Unexpected error during model loading:\n{str(e)}")

    def _update_loaded_models_display(self):
        """Update the list of loaded models display."""
        self.loaded_models_text.config(state='normal')
        self.loaded_models_text.delete('1.0', 'end')

        if not self.loaded_models:
            self.loaded_models_text.insert('1.0', "No models loaded. Click 'Load Model File(s)' to add models.")
        else:
            for i, model_dict in enumerate(self.loaded_models, 1):
                filename = model_dict.get('filename', 'Unknown')

                # Check if this is an ensemble
                if model_dict.get('is_ensemble', False):
                    # Ensemble display
                    ensemble_name = model_dict.get('ensemble_name', 'Ensemble')
                    ensemble_type = model_dict.get('ensemble_type', 'unknown')
                    model_names = model_dict.get('model_names', [])
                    metadata = model_dict.get('metadata', {})

                    # Performance metrics
                    r2 = metadata.get('r2', 'N/A')
                    rmse = metadata.get('rmse', 'N/A')

                    # Format metrics
                    if isinstance(r2, (int, float)):
                        r2_str = f"{r2:.4f}"
                    else:
                        r2_str = str(r2)

                    if isinstance(rmse, (int, float)):
                        rmse_str = f"{rmse:.4f}"
                    else:
                        rmse_str = str(rmse)

                    # Build display text
                    text = f"[{i}] 🎯 ENSEMBLE: {filename}\n"
                    text += f"    Type: {ensemble_name}  |  Method: {ensemble_type}\n"
                    text += f"    Base Models: {', '.join(model_names)}\n"
                    text += f"    R²: {r2_str}  |  RMSE: {rmse_str}\n"
                    text += f"    Path: {model_dict.get('filepath', 'Unknown')}\n"
                    text += "\n"
                else:
                    # Individual model display
                    metadata = model_dict['metadata']

                    # Extract key information
                    model_name = metadata.get('model_name', 'Unknown')
                    preprocessing = metadata.get('preprocessing', 'Unknown')
                    n_vars = metadata.get('n_vars', 'Unknown')

                    # Performance metrics
                    perf = metadata.get('performance', {})
                    r2 = perf.get('R2', perf.get('R2_cv', 'N/A'))
                    rmse = perf.get('RMSE', perf.get('RMSE_cv', 'N/A'))

                    # Format R2 and RMSE
                    if isinstance(r2, (int, float)):
                        r2_str = f"{r2:.4f}"
                    else:
                        r2_str = str(r2)

                    if isinstance(rmse, (int, float)):
                        rmse_str = f"{rmse:.4f}"
                    else:
                        rmse_str = str(rmse)

                    # Build display text
                    text = f"[{i}] {filename}\n"
                    text += f"    Model: {model_name}  |  Preprocessing: {preprocessing}\n"
                    text += f"    R²: {r2_str}  |  RMSE: {rmse_str}  |  Variables: {n_vars}\n"
                    text += f"    Path: {model_dict.get('filepath', 'Unknown')}\n"
                    text += "\n"

                self.loaded_models_text.insert('end', text)

        self.loaded_models_text.config(state='disabled')

    def _clear_loaded_models(self):
        """Clear all loaded models and prediction results."""
        if self.loaded_models:
            response = messagebox.askyesno("Confirm Clear",
                f"Clear all {len(self.loaded_models)} loaded model(s)?\n\n"
                "This will also clear any prediction results.")
            if response:
                # Clear models
                self.loaded_models = []
                self._update_loaded_models_display()

                # Clear prediction results
                self.predictions_df = None
                self.predictions_model_map = {}
                self.consensus_info = {}

                # Clear predictions treeview
                for item in self.predictions_tree.get_children():
                    self.predictions_tree.delete(item)

                # Clear statistics display
                self.pred_stats_text.config(state='normal')
                self.pred_stats_text.delete('1.0', 'end')
                self.pred_stats_text.config(state='disabled')

                # Clear consensus info display
                self.consensus_info_text.config(state='normal')
                self.consensus_info_text.delete('1.0', 'end')
                self.consensus_info_text.config(state='disabled')

                # Models and results cleared
        # else: No models to clear

    def _browse_prediction_data(self):
        """Browse for spectral data directory or CSV file."""
        source = self.pred_data_source.get()

        if source == 'directory':
            path = filedialog.askdirectory(title="Select Spectral Data Directory")
        else:  # csv
            path = filedialog.askopenfilename(
                title="Select CSV File",
                filetypes=[("CSV files", "*.csv"), ("All files", "*.*")]
            )

        if path:
            self.pred_data_path.set(path)

    def _on_pred_source_change(self):
        """Handle data source radio button change - enable/disable path widgets."""
        source = self.pred_data_source.get()

        if source == 'validation':
            # Disable path entry and browse button when validation set is selected
            self.pred_path_entry.config(state='disabled')
            self.pred_browse_button.config(state='disabled')
            self.pred_data_path.set("(Using pre-selected validation set)")
        else:
            # Enable path entry and browse button for directory and CSV options
            self.pred_path_entry.config(state='normal')
            self.pred_browse_button.config(state='normal')
            if self.pred_data_path.get() == "(Using pre-selected validation set)":
                self.pred_data_path.set("")

    def _load_prediction_data(self):
        """Load spectral data for predictions."""
        source = self.pred_data_source.get()

        # Handle validation set separately
        if source == 'validation':
            if self.validation_X is None or self.validation_y is None:
                messagebox.showerror("No Validation Set",
                    "No validation set has been created yet.\n\n"
                    "Please go to the Analysis Configuration tab and create a validation set first.")
                return

            try:
                self.prediction_data = self.validation_X.copy()

                # Update status
                n_samples = len(self.prediction_data)
                n_wavelengths = len(self.prediction_data.columns)

                self.pred_data_status.config(
                    text=f"✓ Loaded validation set: {n_samples} spectra with {n_wavelengths} wavelengths"
                )
                # Validation set loaded - status updated
                return

            except Exception as e:
                messagebox.showerror("Load Error",
                    f"Failed to load validation set:\n{str(e)}")
                self.pred_data_status.config(text="Load failed")
                return

        # For directory and CSV options, need a path
        path_str = self.pred_data_path.get()

        if not path_str:
            messagebox.showerror("No Path", "Please select a data source first.")
            return

        path = Path(path_str)

        if not path.exists():
            messagebox.showerror("Path Error", f"Path does not exist:\n{path_str}")
            return

        try:
            from spectral_predict.io import (read_asd_dir, read_spc_dir, read_csv_spectra,
                                             read_jcamp_dir, read_ascii_spectra)

            if source == 'directory':
                # Try to detect file type
                asd_files = sorted(list(path.glob("*.asd")))
                spc_files = sorted(list(path.glob("*.spc")))
                jcamp_files = sorted(list(path.glob("*.jdx")) + list(path.glob("*.dx")) + list(path.glob("*.JDX")) + list(path.glob("*.DX")))
                ascii_files = sorted(list(path.glob("*.dpt")) + list(path.glob("*.dat")) + list(path.glob("*.asc")) +
                               list(path.glob("*.DPT")) + list(path.glob("*.DAT")) + list(path.glob("*.ASC")))

                if asd_files:
                    self.pred_status.config(text="Loading ASD files...")
                    self.root.update()
                    self.prediction_data, _ = read_asd_dir(str(path))  # Unpack tuple, discard metadata
                elif spc_files:
                    self.pred_status.config(text="Loading SPC files...")
                    self.root.update()
                    self.prediction_data, _ = read_spc_dir(str(path))  # Unpack tuple, discard metadata
                elif jcamp_files:
                    self.pred_status.config(text="Loading JCAMP-DX files...")
                    self.root.update()
                    self.prediction_data, _ = read_jcamp_dir(str(path))  # Unpack tuple, discard metadata
                elif ascii_files:
                    self.pred_status.config(text="Loading ASCII files...")
                    self.root.update()
                    self.prediction_data, _ = read_ascii_spectra(str(path))  # Unpack tuple, discard metadata
                else:
                    messagebox.showerror("No Files",
                        "No supported spectral files found in the selected directory.\n"
                        "Supported formats: ASD, SPC, JCAMP-DX (.jdx/.dx), ASCII (.dpt/.dat/.asc)")
                    return
            else:  # csv
                self.pred_status.config(text="Loading CSV file...")
                self.root.update()
                self.prediction_data, _ = read_csv_spectra(str(path))  # Unpack tuple, discard metadata

            # Update status
            n_samples = len(self.prediction_data)
            n_wavelengths = len(self.prediction_data.columns)

            self.pred_data_status.config(
                text=f"✓ Loaded {n_samples} spectra with {n_wavelengths} wavelengths"
            )
            # Data loaded - status updated

        except Exception as e:
            messagebox.showerror("Load Error",
                f"Failed to load data:\n{str(e)}")
            self.pred_data_status.config(text="Load failed")

    def _run_predictions(self):
        """Apply all loaded models to prediction data."""
        # Validate inputs
        if not self.loaded_models:
            messagebox.showerror("No Models",
                "Please load at least one model first.")
            return

        if self.prediction_data is None:
            messagebox.showerror("No Data",
                "Please load prediction data first.")
            return

        try:
            from spectral_predict.model_io import predict_with_model, predict_with_uncertainty

            # Initialize results dataframe
            results = pd.DataFrame()
            results['Sample'] = self.prediction_data.index

            # Add metadata columns if available (from self.ref)
            if self.ref is not None and hasattr(self.ref, 'columns') and len(self.ref.columns) > 0:
                # Add metadata columns for prediction samples
                for col in self.ref.columns:
                    try:
                        # Check if indices match before adding
                        if all(idx in self.ref.index for idx in self.prediction_data.index):
                            results[col] = self.ref.loc[self.prediction_data.index, col].values
                        else:
                            print(f"Warning: Metadata column '{col}' indices don't match prediction data - skipping")
                    except Exception as e:
                        print(f"Warning: Could not add metadata column '{col}': {str(e)}")

            # Add actual values column if using validation set
            if self.pred_data_source.get() == 'validation' and self.validation_y is not None:
                # Align actual values with prediction samples
                actual_values = self.validation_y.loc[results['Sample']].values

                # For classification models, predictions are already decoded text labels
                # (e.g., "Clean", "Contaminated"), so keep actual values as text too
                # Do NOT encode - this ensures actual and predicted are in the same format
                results['Actual'] = actual_values

            # Clear and initialize model map and uncertainty storage
            self.predictions_model_map = {}
            self.predictions_uncertainty = {}  # Store uncertainty data for each model

            # Setup progress bar
            self.pred_progress['maximum'] = len(self.loaded_models)
            self.pred_progress['value'] = 0

            # Apply each model
            successful_models = 0
            for i, model_dict in enumerate(self.loaded_models):
                metadata = model_dict['metadata']
                is_ensemble = model_dict.get('is_ensemble', False)

                # Derive a descriptive model name for display / column headers
                if is_ensemble:
                    # Prefer human-friendly ensemble name, then type
                    ensemble_name = model_dict.get('ensemble_name') or metadata.get('ensemble_name')
                    ensemble_type = model_dict.get('ensemble_type') or metadata.get('ensemble_type')
                    display_name = ensemble_name or ensemble_type or 'Ensemble'
                    model_name = f"Ensemble {display_name}"
                    preprocessing = metadata.get('preprocessing', 'raw')
                else:
                    model_name = metadata.get('model_name', 'Unknown')
                    preprocessing = metadata.get('preprocessing', 'raw')

                filename = model_dict.get('filename', f'Model_{i+1}')

                # Update status
                self.pred_status.config(text=f"Running {filename}...")
                self.root.update()

                try:
                    # Make predictions with uncertainty
                    pred_result = predict_with_uncertainty(
                        model_dict,
                        self.prediction_data,
                        validate_wavelengths=True
                    )

                    predictions = pred_result['predictions']
                    uncertainty = pred_result['uncertainty']
                    has_uncertainty = pred_result['has_uncertainty']
                    applicability_domain = pred_result.get('applicability_domain', {})
                    has_applicability_domain = pred_result.get('has_applicability_domain', False)

                    # Debug output
                    print(f"DEBUG: Model {filename} - has_applicability_domain: {has_applicability_domain}")
                    if has_applicability_domain:
                        print(f"  - PCA distance range: {applicability_domain['pca_distance'].min():.3f} - {applicability_domain['pca_distance'].max():.3f}")
                        print(f"  - Status counts: {dict(zip(*np.unique(applicability_domain['distance_status'], return_counts=True)))}")
                    print(f"  - has_uncertainty: {has_uncertainty}, keys: {list(uncertainty.keys())}")

                    # Store predictions with descriptive column name
                    if preprocessing in (None, '', 'unknown', 'raw'):
                        col_name = f"{model_name}"
                    else:
                        col_name = f"{model_name}_{preprocessing}"

                    # Handle duplicate column names
                    counter = 1
                    original_col_name = col_name
                    while col_name in results.columns:
                        col_name = f"{original_col_name}_{counter}"
                        counter += 1

                    results[col_name] = predictions

                    # Store mapping to model metadata
                    self.predictions_model_map[col_name] = metadata

                    # Store uncertainty data if available
                    if has_uncertainty:
                        self.predictions_uncertainty[col_name] = uncertainty

                    # Store applicability domain data if available
                    if has_applicability_domain:
                        if not hasattr(self, 'predictions_applicability'):
                            self.predictions_applicability = {}
                        self.predictions_applicability[col_name] = applicability_domain

                    successful_models += 1

                except Exception as e:
                    error_msg = f"Model '{filename}' failed:\n{str(e)}"
                    print(error_msg)  # Log to console
                    # Continue with other models

                # Update progress
                self.pred_progress['value'] = i + 1
                self.root.update()

            # Compute consensus predictions if we have multiple models
            if successful_models > 1:
                results = self._add_consensus_predictions(results)

            # Store results
            self.predictions_df = results

            # Display results
            self._display_predictions()
            self._display_consensus_info()
            self._display_uncertainty()

            # Update status
            if successful_models == len(self.loaded_models):
                self.pred_status.config(text=f"✓ Complete! {successful_models} models applied successfully.")
            else:
                failed = len(self.loaded_models) - successful_models
                self.pred_status.config(
                    text=f"⚠ Complete with warnings: {successful_models} succeeded, {failed} failed."
                )
            # Predictions complete - status updated

            # Auto-advance to Results tab
            self.prediction_notebook.select(1)

        except Exception as e:
            messagebox.showerror("Prediction Error",
                f"An error occurred during predictions:\n{str(e)}")
            self.pred_status.config(text="Error occurred")

    def _add_consensus_predictions(self, results_df):
        """
        Add consensus prediction columns to the results dataframe.

        Computes two types of consensus:
        1. Simple quality-weighted: Average weighted by model R²
        2. Regional quartile-based: Weights vary by prediction range

        NOTE: Consensus predictions are only computed for REGRESSION models.
        Classification models return categorical text labels that cannot be averaged.

        Parameters
        ----------
        results_df : pd.DataFrame
            DataFrame with 'Sample' column and prediction columns

        Returns
        -------
        pd.DataFrame
            DataFrame with added consensus columns
        """
        # Get prediction columns - exclude Sample, Actual, and metadata columns
        metadata_cols = list(self.ref.columns) if self.ref is not None else []

        # Initial filtering - exclude known non-prediction columns
        excluded_cols = {'Sample', 'Actual'} | set(metadata_cols)
        pred_cols = [col for col in results_df.columns if col not in excluded_cols]

        # Additional safety: Filter out non-numeric columns
        numeric_pred_cols = []
        for col in pred_cols:
            if pd.api.types.is_numeric_dtype(results_df[col]):
                numeric_pred_cols.append(col)
            else:
                print(f"Warning: Excluding non-numeric column '{col}' from consensus predictions")
        pred_cols = numeric_pred_cols

        if len(pred_cols) < 2:
            return results_df  # Need at least 2 models for consensus

        # Check if any models are classification models - consensus only works for regression
        has_classification = False
        for col in pred_cols:
            if col in self.predictions_model_map:
                metadata = self.predictions_model_map[col]
                task_type = metadata.get('task_type', 'regression')
                if task_type == 'classification':
                    has_classification = True
                    break

        # Skip consensus for classification models (can't average categorical predictions)
        if has_classification:
            print("\nSkipping consensus predictions: Classification models detected.")
            print("Consensus predictions are only computed for regression models.")
            return results_df

        # Extract model performance metadata
        model_r2 = {}
        model_regional_rmse = {}
        model_quartiles = {}

        for col in pred_cols:
            if col in self.predictions_model_map:
                metadata = self.predictions_model_map[col]

                # Get R² for quality weighting
                if 'performance' in metadata and 'R2' in metadata['performance']:
                    r2 = metadata['performance']['R2']
                    if r2 is not None:
                        model_r2[col] = r2

                # Get regional performance for regional consensus
                if 'regional_rmse' in metadata and 'y_quartiles' in metadata:
                    model_regional_rmse[col] = metadata['regional_rmse']
                    model_quartiles[col] = metadata['y_quartiles']

        # Filter out models that are much worse than the best
        # Use the median R² and filter models that are outliers below it
        if len(model_r2) > 1:
            r2_values = sorted(model_r2.values(), reverse=True)
            best_r2 = r2_values[0]
            median_r2 = np.median(r2_values)

            # Strategy: Keep models within 0.05 of the median (or best if there are only 2-3 models)
            # This way if you have [0.92, 0.92, 0.92, 0.87], median=0.92, threshold=0.87
            # So the 0.87 model gets excluded
            if len(r2_values) <= 3:
                # For small sets, use tight threshold from best
                threshold = best_r2 - 0.05
            else:
                # For larger sets, use median-based threshold
                threshold = median_r2 - 0.05

            # Filter out poor performers
            filtered_model_r2 = {col: r2 for col, r2 in model_r2.items() if r2 >= threshold}

            # Capture consensus info for display
            self.consensus_info['quality'] = {
                'threshold': threshold,
                'best_r2': best_r2,
                'median_r2': median_r2,
                'included': {},
                'excluded': {}
            }

            # Compute weights for included models
            total_weight = sum(filtered_model_r2.values())
            for col, r2 in filtered_model_r2.items():
                weight = r2 / total_weight
                self.consensus_info['quality']['included'][col] = {
                    'r2': r2,
                    'weight': weight
                }

            # Store excluded models
            if len(filtered_model_r2) < len(model_r2):
                excluded = set(model_r2.keys()) - set(filtered_model_r2.keys())
                excluded_r2 = {col: model_r2[col] for col in excluded}
                print(f"\nConsensus filtering: Best R²={best_r2:.3f}, Median R²={median_r2:.3f}, threshold={threshold:.3f}")
                print(f"Excluded {len(excluded)} poor model(s):")
                for col, r2 in excluded_r2.items():
                    print(f"  - {col}: R²={r2:.3f}")
                    self.consensus_info['quality']['excluded'][col] = {
                        'r2': r2,
                        'reason': f"R² < {threshold:.3f}"
                    }

            model_r2 = filtered_model_r2

        # Compute simple quality-weighted consensus (fully vectorized)
        if len(model_r2) > 0:
            # Get all model columns as a 2D numpy array: (n_samples, n_models)
            model_cols = list(model_r2.keys())

            # Validate all columns are numeric before numpy operations
            for col in model_cols:
                if not pd.api.types.is_numeric_dtype(results_df[col]):
                    raise ValueError(f"Cannot compute consensus: Column '{col}' contains non-numeric data")

            model_data = results_df[model_cols].values

            # Create normalized weight array
            weights = np.array([model_r2[col] for col in model_cols])
            weights = weights / weights.sum()

            # Vectorized weighted sum: (n_samples, n_models) @ (n_models,) = (n_samples,)
            consensus_simple = model_data @ weights

            results_df['Consensus_Quality_Weighted'] = consensus_simple

        # Compute regional quartile-based consensus
        if len(model_regional_rmse) > 0 and len(model_quartiles) > 0:
            # Capture regional consensus info for display
            ref_quartiles = list(model_quartiles.values())[0]
            self.consensus_info['regional'] = {
                'quartiles': ref_quartiles,
                'models': list(model_regional_rmse.keys()),
                'regional_rmse': model_regional_rmse
            }

            # Vectorized regional consensus computation
            regional_cols = list(model_regional_rmse.keys())

            # Validate all columns are numeric before numpy operations
            for col in regional_cols:
                if not pd.api.types.is_numeric_dtype(results_df[col]):
                    raise ValueError(f"Cannot compute regional consensus: Column '{col}' contains non-numeric data")

            regional_data = results_df[regional_cols].values  # (n_samples, n_models)

            # Compute median predictions for all samples at once
            median_preds = np.median(regional_data, axis=1)  # (n_samples,)

            # Assign quartiles to all samples (vectorized)
            quartile_indices = np.zeros(len(results_df), dtype=int)
            quartile_indices[median_preds >= ref_quartiles[2]] = 3  # Q4
            quartile_indices[(median_preds >= ref_quartiles[1]) & (median_preds < ref_quartiles[2])] = 2  # Q3
            quartile_indices[(median_preds >= ref_quartiles[0]) & (median_preds < ref_quartiles[1])] = 1  # Q2
            quartile_indices[median_preds < ref_quartiles[0]] = 0  # Q1

            quartile_names = ['Q1', 'Q2', 'Q3', 'Q4']

            # Build RMSE matrix for weighting: (4 quartiles, n_models)
            rmse_matrix = np.zeros((4, len(regional_cols)))
            for j, col in enumerate(regional_cols):
                regional_rmse = model_regional_rmse[col]
                for i, q_name in enumerate(quartile_names):
                    if q_name in regional_rmse and not np.isnan(regional_rmse[q_name]):
                        rmse_matrix[i, j] = regional_rmse[q_name]
                    else:
                        rmse_matrix[i, j] = np.inf  # Invalid RMSE

            # Compute weights: inverse RMSE squared (vectorized)
            # Shape: (4 quartiles, n_models)
            weight_matrix = 1.0 / (rmse_matrix ** 2 + 1e-10)
            weight_matrix[np.isinf(rmse_matrix)] = 0  # Zero weight for invalid

            # Normalize weights per quartile
            weight_sums = weight_matrix.sum(axis=1, keepdims=True)
            weight_sums[weight_sums == 0] = 1  # Avoid division by zero
            weight_matrix = weight_matrix / weight_sums

            # Apply weights based on each sample's quartile assignment (fully vectorized)
            # Get the weight vector for each sample based on its quartile
            # Shape: (n_samples, n_models)
            sample_weights = weight_matrix[quartile_indices, :]

            # Compute weighted sum for each sample
            consensus_regional = (regional_data * sample_weights).sum(axis=1)

            # Handle cases where all weights are zero (fallback to median)
            no_valid_weights = sample_weights.sum(axis=1) == 0
            consensus_regional[no_valid_weights] = median_preds[no_valid_weights]

            results_df['Consensus_Regional'] = consensus_regional

        return results_df

    def _display_predictions(self):
        """Display predictions in treeview table."""
        # Clear existing items
        for item in self.predictions_tree.get_children():
            self.predictions_tree.delete(item)

        if self.predictions_df is None or self.predictions_df.empty:
            return

        # Set columns
        columns = list(self.predictions_df.columns)
        self.predictions_tree['columns'] = columns

        # Detect if this is a classification task by checking prediction columns
        # Classification predictions will have string/text labels (not just numeric)
        is_classification = False
        prediction_cols = [col for col in columns if col not in ['Sample', 'Actual']]

        if prediction_cols:
            # Sample first prediction column to detect type
            first_pred_col = prediction_cols[0]
            sample_values = self.predictions_df[first_pred_col].dropna()
            if len(sample_values) > 0:
                # Check if values are strings (excluding numeric strings that look like floats)
                first_val = sample_values.iloc[0]
                if isinstance(first_val, str):
                    try:
                        float(first_val)
                        # It's a numeric string, likely regression
                        is_classification = False
                    except (ValueError, TypeError):
                        # It's a text label, classification
                        is_classification = True

        # Configure column headings and widths
        for col in columns:
            self.predictions_tree.heading(col, text=col)
            if col == 'Sample':
                self.predictions_tree.column(col, width=150, anchor='w')
            elif is_classification and col != 'Sample':
                # Wider columns for text labels in classification
                self.predictions_tree.column(col, width=150, anchor='center')
            else:
                self.predictions_tree.column(col, width=120, anchor='e')

        # Populate rows
        for idx, row in self.predictions_df.iterrows():
            values = []
            for col in columns:
                val = row[col]
                # Format numeric values only for regression (not classification text labels)
                if isinstance(val, (int, float)) and col != 'Sample':
                    values.append(f"{val:.4f}")
                else:
                    values.append(str(val))

            self.predictions_tree.insert('', 'end', values=values)

        # Calculate and display statistics
        self._update_prediction_statistics()

        # Plot predictions if validation set is used
        self._plot_prediction_results()

    def _display_uncertainty(self):
        """Display prediction uncertainty for each model."""
        # Clear existing items
        for item in self.uncertainty_tree.get_children():
            self.uncertainty_tree.delete(item)

        # Check if we have any uncertainty data
        if not hasattr(self, 'predictions_uncertainty') or not self.predictions_uncertainty:
            # Show placeholder
            self.uncertainty_placeholder.pack(pady=20)
            return

        # Hide placeholder, show tree
        self.uncertainty_placeholder.pack_forget()

        # Get sample names
        sample_names = self.predictions_df['Sample'].values

        # Determine if we have regression or classification by checking first model with uncertainty
        first_model = list(self.predictions_uncertainty.keys())[0]
        first_uncertainty = self.predictions_uncertainty[first_model]
        is_classification = 'probabilities' in first_uncertainty

        if is_classification:
            # === CLASSIFICATION: Show probabilities and confidence ===
            # Build columns: Sample | Model | Predicted | Confidence | Prob(Class1) | Prob(Class2) | ...
            class_names = first_uncertainty.get('class_names', [])

            columns = ['Sample', 'Model', 'Predicted', 'Confidence%']
            if class_names:
                columns.extend([f'P({cls})' for cls in class_names])

            self.uncertainty_tree['columns'] = columns

            # Configure column headings and widths
            for col in columns:
                self.uncertainty_tree.heading(col, text=col)
                if col == 'Sample':
                    self.uncertainty_tree.column(col, width=150, anchor='w')
                elif col == 'Model':
                    self.uncertainty_tree.column(col, width=180, anchor='w')
                elif col == 'Predicted':
                    self.uncertainty_tree.column(col, width=120, anchor='center')
                else:
                    self.uncertainty_tree.column(col, width=100, anchor='e')

            # Populate rows for each model
            for model_name, uncertainty in self.predictions_uncertainty.items():
                if 'probabilities' not in uncertainty:
                    continue

                probabilities = uncertainty['probabilities']
                confidence = uncertainty['confidence']
                class_names = uncertainty.get('class_names', [])

                # Get predictions for this model
                predictions = self.predictions_df[model_name].values

                for i, sample in enumerate(sample_names):
                    row_values = [
                        str(sample),
                        model_name,
                        str(predictions[i]),
                        f"{confidence[i]*100:.2f}"
                    ]

                    # Add class probabilities
                    if class_names:
                        for j in range(len(class_names)):
                            row_values.append(f"{probabilities[i, j]*100:.2f}")

                    # Color-code by confidence
                    item_id = self.uncertainty_tree.insert('', 'end', values=row_values)
                    if confidence[i] >= 0.9:
                        self.uncertainty_tree.item(item_id, tags=('high_conf',))
                    elif confidence[i] >= 0.7:
                        self.uncertainty_tree.item(item_id, tags=('med_conf',))
                    else:
                        self.uncertainty_tree.item(item_id, tags=('low_conf',))

            # Configure tags for color-coding
            self.uncertainty_tree.tag_configure('high_conf', foreground='#2ecc71')  # Green
            self.uncertainty_tree.tag_configure('med_conf', foreground='#f39c12')  # Orange
            self.uncertainty_tree.tag_configure('low_conf', foreground='#e74c3c')   # Red

        else:
            # === REGRESSION: Show RMSECV and applicability domain ===
            print("DEBUG: Displaying regression uncertainty table")
            print(f"  - Number of models with uncertainty: {len(self.predictions_uncertainty)}")
            if hasattr(self, 'predictions_applicability'):
                print(f"  - Number of models with applicability domain: {len(self.predictions_applicability)}")
            else:
                print(f"  - No predictions_applicability attribute found")

            # Build columns: Sample | Model | Prediction | Model RMSECV | Distance | Variance | Status
            columns = ['Sample', 'Model', 'Prediction', 'Model RMSECV', 'PCA Distance', 'Tree Var', 'Status']
            self.uncertainty_tree['columns'] = columns

            # Configure column headings and widths
            for col in columns:
                self.uncertainty_tree.heading(col, text=col)
                if col == 'Sample':
                    self.uncertainty_tree.column(col, width=150, anchor='w')
                elif col == 'Model':
                    self.uncertainty_tree.column(col, width=180, anchor='w')
                elif col == 'Status':
                    self.uncertainty_tree.column(col, width=120, anchor='center')
                else:
                    self.uncertainty_tree.column(col, width=100, anchor='e')

            # Get applicability domain data if available
            has_applicability = hasattr(self, 'predictions_applicability') and self.predictions_applicability

            # Populate rows for each model
            for model_name in self.predictions_uncertainty.keys():
                uncertainty = self.predictions_uncertainty[model_name]

                # Get RMSECV (model-level metric)
                rmsecv = uncertainty.get('rmsecv', None)
                tree_variance = uncertainty.get('tree_variance', None)

                # Get applicability domain data for this model
                ad_data = None
                if has_applicability and model_name in self.predictions_applicability:
                    ad_data = self.predictions_applicability[model_name]

                # Get predictions for this model
                predictions = self.predictions_df[model_name].values

                for i, sample in enumerate(sample_names):
                    # Base values
                    row_values = [
                        str(sample),
                        model_name,
                        f"{predictions[i]:.4f}",
                        f"{rmsecv:.4f}" if rmsecv is not None else "N/A",
                    ]

                    # Add PCA distance if available
                    if ad_data and 'pca_distance' in ad_data:
                        pca_dist = ad_data['pca_distance'][i]
                        row_values.append(f"{pca_dist:.3f}")
                    else:
                        row_values.append("N/A")

                    # Add tree variance if available (Random Forest only)
                    if tree_variance is not None:
                        row_values.append(f"{tree_variance[i]:.4f}")
                    else:
                        row_values.append("N/A")

                    # Add status if available
                    if ad_data and 'distance_status' in ad_data:
                        status = ad_data['distance_status'][i]
                        status_display = {'good': '✓ Good', 'caution': '⚠ Caution', 'extrapolation': '⚠️ Extrap'}
                        row_values.append(status_display.get(status, status))
                    else:
                        row_values.append("N/A")

                    # Color-code by applicability domain status
                    item_id = self.uncertainty_tree.insert('', 'end', values=row_values)

                    if ad_data and 'distance_status' in ad_data:
                        status = ad_data['distance_status'][i]
                        if status == 'good':
                            self.uncertainty_tree.item(item_id, tags=('high_conf',))
                        elif status == 'caution':
                            self.uncertainty_tree.item(item_id, tags=('med_conf',))
                        else:  # extrapolation
                            self.uncertainty_tree.item(item_id, tags=('low_conf',))
                    else:
                        # No AD data - neutral color
                        pass

            # Configure tags for color-coding
            self.uncertainty_tree.tag_configure('high_conf', foreground='#2ecc71')  # Green
            self.uncertainty_tree.tag_configure('med_conf', foreground='#f39c12')  # Orange
            self.uncertainty_tree.tag_configure('low_conf', foreground='#e74c3c')   # Red

    def _update_prediction_statistics(self):
        """Calculate and display prediction statistics."""
        if self.predictions_df is None or self.predictions_df.empty:
            return

        self.pred_stats_text.config(state='normal')
        self.pred_stats_text.delete('1.0', 'end')

        # Check if we're using validation set (has actual y values)
        is_validation = (self.pred_data_source.get() == 'validation' and
                        self.validation_y is not None)

        if is_validation:
            stats_text = "🔬 VALIDATION SET RESULTS\n"
            stats_text += "=" * 60 + "\n"
            stats_text += f"Algorithm: {self.validation_algorithm.get()}\n"
            stats_text += f"Validation Samples: {len(self.validation_y)}\n"
            stats_text += "=" * 60 + "\n\n"
        else:
            stats_text = "Prediction Statistics:\n"
            stats_text += "=" * 60 + "\n\n"

        # Calculate stats for each prediction column - exclude Sample, Actual, and metadata columns
        metadata_cols = list(self.ref.columns) if self.ref is not None else []
        prediction_cols = [col for col in self.predictions_df.columns
                          if col not in ['Sample', 'Actual'] + metadata_cols]

        if not prediction_cols:
            stats_text += "No prediction columns found.\n"
        else:
            for col in prediction_cols:
                values = self.predictions_df[col].dropna()

                if len(values) > 0:
                    stats_text += f"{col}:\n"

                    # Detect if this is a classification model
                    is_classification = False
                    if col in self.predictions_model_map:
                        metadata = self.predictions_model_map[col]
                        task_type = metadata.get('task_type', None)

                        # Check metadata first
                        if task_type == 'classification':
                            is_classification = True
                        # Also check if values are strings (text labels)
                        elif len(values) > 0 and isinstance(values.iloc[0], str):
                            try:
                                float(values.iloc[0])
                                is_classification = False
                            except (ValueError, TypeError):
                                is_classification = True

                    # Add variable information from model metadata
                    if col in self.predictions_model_map:
                        metadata = self.predictions_model_map[col]
                        n_vars = metadata.get('n_vars', 'Unknown')
                        wavelengths = metadata.get('wavelengths', None)

                        stats_text += f"  • Variables: {n_vars}\n"

                        # Format wavelengths if available and not too many
                        if wavelengths is not None:
                            wl_spec = self._format_wavelengths_as_spec(wavelengths)
                            if wl_spec and len(wl_spec) < 500:  # Only show if reasonable length
                                stats_text += f"  • Wavelengths: {wl_spec}\n"

                    # If validation set, calculate performance metrics
                    if is_validation:
                        try:
                            # Get actual values aligned with predictions
                            y_true = self.validation_y.loc[self.predictions_df['Sample']].values
                            y_pred = values.values

                            if is_classification:
                                # Use classification metrics
                                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

                                accuracy = accuracy_score(y_true, y_pred)
                                stats_text += f"  ✓ Accuracy:     {accuracy:.4f}\n"

                                # For multi-class, use weighted average
                                unique_classes = np.unique(np.concatenate([y_true, y_pred]))
                                if len(unique_classes) > 2:
                                    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
                                    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
                                    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
                                    stats_text += f"  ✓ Precision:    {precision:.4f} (weighted)\n"
                                    stats_text += f"  ✓ Recall:       {recall:.4f} (weighted)\n"
                                    stats_text += f"  ✓ F1 Score:     {f1:.4f} (weighted)\n"
                                else:
                                    # Binary classification
                                    precision = precision_score(y_true, y_pred, zero_division=0)
                                    recall = recall_score(y_true, y_pred, zero_division=0)
                                    f1 = f1_score(y_true, y_pred, zero_division=0)
                                    stats_text += f"  ✓ Precision:    {precision:.4f}\n"
                                    stats_text += f"  ✓ Recall:       {recall:.4f}\n"
                                    stats_text += f"  ✓ F1 Score:     {f1:.4f}\n"

                                stats_text += f"  • Samples:      {len(y_true)}\n"
                                stats_text += f"  • Classes:      {len(unique_classes)}\n"

                                # Show class distribution
                                unique_pred, counts_pred = np.unique(y_pred, return_counts=True)
                                stats_text += f"  • Pred Distribution:\n"
                                for cls, cnt in zip(unique_pred, counts_pred):
                                    stats_text += f"      {cls}: {cnt} ({cnt/len(y_pred)*100:.1f}%)\n"

                            else:
                                # Use regression metrics
                                from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

                                # Calculate metrics
                                r2 = r2_score(y_true, y_pred)
                                rmse = np.sqrt(mean_squared_error(y_true, y_pred))
                                mae = mean_absolute_error(y_true, y_pred)

                                stats_text += f"  ✓ R² Score:     {r2:.4f}\n"
                                stats_text += f"  ✓ RMSE:         {rmse:.4f}\n"
                                stats_text += f"  ✓ MAE:          {mae:.4f}\n"
                                stats_text += f"  • Samples:      {len(y_true)}\n"
                                stats_text += f"  • Pred Mean:    {y_pred.mean():.4f}\n"
                                stats_text += f"  • Actual Mean:  {y_true.mean():.4f}\n"
                                stats_text += f"  • Pred Range:   [{y_pred.min():.4f}, {y_pred.max():.4f}]\n"
                                stats_text += f"  • Actual Range: [{y_true.min():.4f}, {y_true.max():.4f}]\n"

                        except Exception as e:
                            stats_text += f"  ⚠ Could not calculate validation metrics: {e}\n"
                            if is_classification:
                                stats_text += f"  • Count:  {len(values)}\n"
                                unique_vals, counts = np.unique(values, return_counts=True)
                                stats_text += f"  • Classes: {len(unique_vals)}\n"
                                stats_text += f"  • Distribution:\n"
                                for val, cnt in zip(unique_vals, counts):
                                    stats_text += f"      {val}: {cnt} ({cnt/len(values)*100:.1f}%)\n"
                            else:
                                stats_text += f"  • Count:  {len(values)}\n"
                                stats_text += f"  • Mean:   {values.mean():.4f}\n"
                                stats_text += f"  • Std:    {values.std():.4f}\n"
                    else:
                        # Regular prediction statistics (no validation)
                        if is_classification:
                            stats_text += f"  Count: {len(values)}\n"
                            unique_vals, counts = np.unique(values, return_counts=True)
                            stats_text += f"  Classes: {len(unique_vals)}\n"
                            stats_text += f"  Distribution:\n"
                            for val, cnt in zip(unique_vals, counts):
                                stats_text += f"    {val}: {cnt} ({cnt/len(values)*100:.1f}%)\n"
                        else:
                            stats_text += f"  Count: {len(values)}\n"
                            stats_text += f"  Mean:  {values.mean():.4f}\n"
                            stats_text += f"  Std:   {values.std():.4f}\n"
                            stats_text += f"  Min:   {values.min():.4f}\n"
                            stats_text += f"  Max:   {values.max():.4f}\n"
                            stats_text += f"  Median:{values.median():.4f}\n"

                    stats_text += "\n"

        self.pred_stats_text.insert('1.0', stats_text)
        self.pred_stats_text.config(state='disabled')

    def _plot_prediction_results(self):
        """Plot reference vs predicted for all models (validation set only)."""
        if not HAS_MATPLOTLIB:
            return

        # Only plot if using validation set
        is_validation = (self.pred_data_source.get() == 'validation' and
                        self.validation_y is not None)

        if not is_validation or self.predictions_df is None or self.predictions_df.empty:
            # Hide placeholder if it exists
            if hasattr(self, 'pred_plot_placeholder'):
                self.pred_plot_placeholder.pack_forget()
            return

        # Hide placeholder
        if hasattr(self, 'pred_plot_placeholder'):
            self.pred_plot_placeholder.pack_forget()

        # Clear existing plots
        for widget in self.prediction_plots_frame.winfo_children():
            widget.destroy()

        # Add control frame for color selection
        control_frame = ttk.Frame(self.prediction_plots_frame)
        control_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)

        ttk.Label(control_frame, text="Color by:", style='TLabel').pack(side='left', padx=5)

        color_options = self._get_available_color_variables()
        color_combo = ttk.Combobox(control_frame,
                                   textvariable=self.pred_results_color_var,
                                   values=color_options,
                                   width=20,
                                   state='readonly')
        color_combo.pack(side='left', padx=5)
        color_combo.bind('<<ComboboxSelected>>', lambda e: self._plot_prediction_results())

        # Create plot frame
        plot_container = ttk.Frame(self.prediction_plots_frame)
        plot_container.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Get prediction columns - exclude Sample, Actual, Consensus_*, and metadata columns
        metadata_cols = list(self.ref.columns) if self.ref is not None else []
        prediction_cols = [col for col in self.predictions_df.columns
                          if col not in ['Sample', 'Actual'] + metadata_cols
                          and not col.startswith('Consensus_')]

        if not prediction_cols:
            return

        # Get actual values
        try:
            y_true = self.validation_y.loc[self.predictions_df['Sample']].values
        except Exception as e:
            print(f"Error getting validation y values: {e}")
            return

        # Get color variable selection
        color_by = self.pred_results_color_var.get()

        # Determine coloring values - align with prediction samples
        if color_by == 'Y Value':
            color_values = y_true.copy()
            is_categorical = self._is_categorical_target()
            color_label = 'Y Value'
        elif color_by == 'None':
            color_values = None
            is_categorical = False
            color_label = None
        else:
            # Metadata column - align with prediction sample IDs
            if self.ref is not None and color_by in self.ref.columns:
                sample_ids = self.predictions_df['Sample'].values
                try:
                    color_values = self.ref.loc[sample_ids, color_by].values
                    is_categorical = self._is_categorical_variable(color_by, color_values)
                    color_label = color_by
                except KeyError:
                    color_values = None
                    is_categorical = False
                    color_label = None
            else:
                color_values = None
                is_categorical = False
                color_label = None

        # Determine grid size for subplots
        n_models = len(prediction_cols)
        n_cols = min(3, n_models)  # Max 3 columns
        n_rows = (n_models + n_cols - 1) // n_cols

        # Create figure with subplots
        fig = Figure(figsize=(6*n_cols, 5*n_rows))

        # Helper function to apply coloring to a subplot
        def add_colored_scatter(ax, y_true_data, y_pred_data, color_vals, is_cat):
            if color_vals is None:
                ax.scatter(y_true_data, y_pred_data, alpha=0.6, edgecolors='black', linewidths=0.5, s=50, color='steelblue')
            elif is_cat:
                unique_vals = np.unique(color_vals[pd.notna(color_vals)])
                n_vals = len(unique_vals)
                colors = plt.cm.tab10(np.linspace(0, 1, 10)) if n_vals <= 10 else plt.cm.tab20(np.linspace(0, 1, 20))
                color_map = {val: colors[i % len(colors)] for i, val in enumerate(unique_vals)}
                for val in unique_vals:
                    mask = color_vals == val
                    if np.any(mask):
                        ax.scatter(y_true_data[mask], y_pred_data[mask], c=[color_map[val]], alpha=0.6,
                                 edgecolors='black', linewidths=0.5, s=50, label=str(val))
                nan_mask = pd.isna(color_vals)
                if np.any(nan_mask):
                    ax.scatter(y_true_data[nan_mask], y_pred_data[nan_mask], c='lightgray', alpha=0.6,
                             edgecolors='black', linewidths=0.5, s=50, label='N/A')
            else:
                # Convert to numeric if needed (handles string numeric values)
                numeric_color_vals = pd.to_numeric(color_vals, errors='coerce')
                valid_mask = pd.notna(numeric_color_vals)
                if np.any(valid_mask):
                    ax.scatter(y_true_data[valid_mask], y_pred_data[valid_mask], c=numeric_color_vals[valid_mask],
                             cmap='viridis', alpha=0.6, edgecolors='black', linewidths=0.5, s=50)
                nan_mask = ~valid_mask
                if np.any(nan_mask):
                    ax.scatter(y_true_data[nan_mask], y_pred_data[nan_mask], c='lightgray', alpha=0.6,
                             edgecolors='black', linewidths=0.5, s=50, label='N/A')

        # Plot each model
        for idx, col in enumerate(prediction_cols):
            ax = fig.add_subplot(n_rows, n_cols, idx + 1)

            y_pred = self.predictions_df[col].values

            # Apply colored scatter
            add_colored_scatter(ax, y_true, y_pred, color_values, is_categorical)

            # 1:1 line
            min_val = min(y_true.min(), y_pred.min())
            max_val = max(y_true.max(), y_pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='1:1 Line')

            # Calculate statistics
            from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
            r2 = r2_score(y_true, y_pred)
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            mae = mean_absolute_error(y_true, y_pred)

            # Add statistics text box
            stats_text = f'R² = {r2:.4f}\nRMSE = {rmse:.4f}\nMAE = {mae:.4f}'
            ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                    fontsize=9, family='monospace')

            ax.set_xlabel('Reference Values', fontsize=10)
            ax.set_ylabel('Predicted Values', fontsize=10)
            ax.set_title(col, fontsize=11, fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend(loc='lower right', fontsize=8)

        fig.tight_layout()

        # Add to GUI
        canvas = FigureCanvasTkAgg(fig, plot_container)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add click handler for point identification (on all subplots)
        def on_click(event):
            if event.inaxes is None or event.xdata is None or event.ydata is None:
                return

            # Find which subplot was clicked
            subplot_idx = None
            for idx in range(n_models):
                ax = fig.axes[idx]
                if event.inaxes == ax:
                    subplot_idx = idx
                    break

            if subplot_idx is None:
                return

            ax = fig.axes[subplot_idx]
            col = prediction_cols[subplot_idx]
            y_pred = self.predictions_df[col].values

            # Find nearest point to click
            click_x, click_y = event.xdata, event.ydata
            distances = np.sqrt((y_true - click_x)**2 + (y_pred - click_y)**2)
            nearest_idx = np.argmin(distances)

            # Only show annotation if click is reasonably close
            x_range = ax.get_xlim()[1] - ax.get_xlim()[0]
            y_range = ax.get_ylim()[1] - ax.get_ylim()[0]
            threshold = 0.1 * np.sqrt(x_range**2 + y_range**2)

            if distances[nearest_idx] < threshold:
                sample_name = self.predictions_df['Sample'].iloc[nearest_idx]
                y_actual = y_true[nearest_idx]
                y_predicted = y_pred[nearest_idx]
                residual = y_actual - y_predicted

                info_text = f"Sample: {sample_name}\nActual: {y_actual:.4f}\nPredicted: {y_predicted:.4f}\nResidual: {residual:.4f}"

                # Add color variable info if applicable
                if color_values is not None and color_label:
                    color_val = color_values[nearest_idx]
                    if pd.notna(color_val):
                        info_text += f"\n{color_label}: {color_val}"
                    else:
                        info_text += f"\n{color_label}: N/A"

                self._create_or_update_annotation(ax, y_actual, y_predicted, info_text, canvas)

        fig.canvas.mpl_connect('button_press_event', on_click)

        # Add export button
        self._add_plot_export_button(self.prediction_plots_frame, fig, "validation_predictions")

    def _display_consensus_info(self):
        """Display detailed information about consensus predictions."""
        self.consensus_info_text.config(state='normal')
        self.consensus_info_text.delete('1.0', 'end')

        if not self.consensus_info:
            self.consensus_info_text.insert('1.0', "No consensus predictions available.\n")
            self.consensus_info_text.config(state='disabled')
            return

        info_text = ""

        # Quality-Weighted Consensus
        if 'quality' in self.consensus_info:
            quality_info = self.consensus_info['quality']
            info_text += "="*80 + "\n"
            info_text += "QUALITY-WEIGHTED CONSENSUS\n"
            info_text += "="*80 + "\n\n"

            if 'included' in quality_info and quality_info['included']:
                info_text += f"Included Models ({len(quality_info['included'])}):\n"
                for model_name, model_data in quality_info['included'].items():
                    r2 = model_data.get('r2', 0)
                    weight = model_data.get('weight', 0) * 100  # Convert to percentage
                    info_text += f"  • {model_name}\n"
                    info_text += f"      R² = {r2:.4f}, Weight = {weight:.1f}%\n"
                info_text += "\n"

            if 'excluded' in quality_info and quality_info['excluded']:
                info_text += f"Excluded Models ({len(quality_info['excluded'])}):\n"
                for model_name, model_data in quality_info['excluded'].items():
                    r2 = model_data.get('r2', 0)
                    reason = model_data.get('reason', 'Unknown')
                    info_text += f"  • {model_name}\n"
                    info_text += f"      R² = {r2:.4f}, Reason: {reason}\n"
                info_text += "\n"

            if 'threshold' in quality_info:
                info_text += f"Filtering Threshold: R² >= {quality_info['threshold']:.4f}\n"
            if 'best_r2' in quality_info:
                info_text += f"Best R²: {quality_info['best_r2']:.4f}\n"
            if 'median_r2' in quality_info:
                info_text += f"Median R²: {quality_info['median_r2']:.4f}\n"
            info_text += "\n"

        # Regional Consensus
        if 'regional' in self.consensus_info:
            regional_info = self.consensus_info['regional']
            info_text += "="*80 + "\n"
            info_text += "REGIONAL CONSENSUS (Quartile-Based)\n"
            info_text += "="*80 + "\n\n"

            if 'quartiles' in regional_info:
                quartiles = regional_info['quartiles']
                info_text += "Quartile Boundaries:\n"
                info_text += f"  Q1: y <= {quartiles[0]:.4f}\n"
                info_text += f"  Q2: {quartiles[0]:.4f} < y <= {quartiles[1]:.4f}\n"
                info_text += f"  Q3: {quartiles[1]:.4f} < y <= {quartiles[2]:.4f}\n"
                info_text += f"  Q4: y > {quartiles[2]:.4f}\n\n"

            if 'models' in regional_info and regional_info['models']:
                info_text += f"Models Used ({len(regional_info['models'])} total):\n"
                for model_name in regional_info['models']:
                    info_text += f"  • {model_name}\n"
                info_text += "\n"

            if 'regional_rmse' in regional_info:
                info_text += "Per-Quartile Performance (RMSE):\n"
                for model_name, rmse_dict in regional_info['regional_rmse'].items():
                    info_text += f"  {model_name}:\n"
                    for quartile, rmse in rmse_dict.items():
                        info_text += f"      {quartile}: {rmse:.4f}\n"
                info_text += "\n"

            info_text += "Note: For each prediction, the model weight is based on its\n"
            info_text += "performance in the quartile that the prediction value falls into.\n"
            info_text += "Models with lower RMSE in that quartile get higher weight.\n"

        if not info_text:
            info_text = "No consensus prediction details available.\n"

        self.consensus_info_text.insert('1.0', info_text)
        self.consensus_info_text.config(state='disabled')

    def _export_predictions(self):
        """Export predictions to CSV file."""
        if self.predictions_df is None or self.predictions_df.empty:
            messagebox.showerror("No Predictions",
                "No predictions to export. Run predictions first.")
            return

        # Generate default filename with timestamp
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        default_filename = f"predictions_{timestamp}.csv"

        # Get initial directory from spectral data path
        initial_dir = None
        if self.spectral_data_path.get():
            data_path = Path(self.spectral_data_path.get())
            initial_dir = str(data_path.parent if data_path.is_file() else data_path)

        # Ask user for save location
        filepath = filedialog.asksaveasfilename(
            title="Export Predictions",
            defaultextension=".csv",
            filetypes=[
                ("CSV files", "*.csv"),
                ("Excel files", "*.xlsx"),
                ("All files", "*.*")
            ],
            initialfile=default_filename,
            initialdir=initial_dir
        )

        if not filepath:
            return

        try:
            # Export based on file extension
            filepath_obj = Path(filepath)
            if filepath_obj.suffix.lower() in ['.xlsx', '.xls']:
                self.predictions_df.to_excel(filepath, index=False, engine='xlsxwriter')
            else:
                self.predictions_df.to_csv(filepath, index=False)
            # Predictions exported successfully

        except Exception as e:
            messagebox.showerror("Export Error",
                f"Failed to export predictions:\n{str(e)}")

    # REMOVED - Instrument Lab functionality deprecated
    # def _create_tab9_instrument_lab(self):
    #     """Tab 9: Instrument Lab - Instrument characterization and registry."""
    #     self.tab9 = ttk.Frame(self.notebook, style='TFrame')
    #     self.notebook.add(self.tab9, text='  🔬 Instrument Lab  ')
    #
    #     # Create scrollable content
    #     canvas = tk.Canvas(self.tab9, bg=self.colors['bg'], highlightthickness=0)
    #     scrollbar = ttk.Scrollbar(self.tab9, orient="vertical", command=canvas.yview)
    #     scrollable_frame = ttk.Frame(canvas, style='TFrame')
    #
    #     scrollable_frame.bind(
    #         "<Configure>",
    #         lambda e: self._debounced_configure_scrollregion("tab8", canvas)
    #     )
    #
    #     canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
    #     canvas.configure(yscrollcommand=scrollbar.set)
    #
    #     canvas.pack(side="left", fill="both", expand=True)
    #     scrollbar.pack(side="right", fill="y")
    #
    #     main_frame = ttk.Frame(scrollable_frame, style='TFrame', padding="30")
    #     main_frame.pack(fill='both', expand=True)
    #
    #     # Title
    #     title_label = ttk.Label(main_frame, text="Instrument Lab",
    #                            style='Title.TLabel')
    #     title_label.pack(pady=(0, 10))
    #
    #     desc_label = ttk.Label(main_frame,
    #                           text="Characterize instruments and manage instrument registry",
    #                           style='Caption.TLabel')
    #     desc_label.pack(pady=(0, 20))
    #
    #     # === SECTION A: Load & Characterize Instrument ===
    #     section_a = ttk.LabelFrame(main_frame, text="Load & Characterize Instrument",
    #                                padding="15", style='Card.TFrame')
    #     section_a.pack(fill='x', pady=(0, 15))
    #
    #     # Instrument ID
    #     id_frame = ttk.Frame(section_a)
    #     id_frame.pack(fill='x', pady=5)
    #     ttk.Label(id_frame, text="Instrument ID:", style='CardLabel.TLabel').pack(side='left', padx=(0, 10))
    #     self.inst_id_entry = ttk.Entry(id_frame, width=30)
    #     self.inst_id_entry.pack(side='left')
    #
    #     # Data directory
    #     dir_frame = ttk.Frame(section_a)
    #     dir_frame.pack(fill='x', pady=5)
    #     ttk.Label(dir_frame, text="Data Directory:", style='CardLabel.TLabel').pack(side='left', padx=(0, 10))
    #     self.inst_data_path = tk.StringVar()
    #     ttk.Entry(dir_frame, textvariable=self.inst_data_path, width=50).pack(side='left', padx=(0, 10))
    #     ttk.Button(dir_frame, text="Browse", command=self._browse_instrument_data, style='Modern.TButton').pack(side='left')
    #
    #     # Load & Characterize button
    #     self._create_accent_button(section_a, "Load & Characterize",
    #                                 self._load_and_characterize_instrument).pack(pady=10)
    #
    #     # === SECTION B: Instrument Summary ===
    #     section_b = ttk.LabelFrame(main_frame, text="Instrument Summary",
    #                                padding="15", style='Card.TFrame')
    #     section_b.pack(fill='x', pady=(0, 15))
    #
    #     self.inst_summary_text = tk.Text(section_b, height=8, width=80, wrap='word',
    #                                      bg=self.colors['panel'], fg=self.colors['text'],
    #                                      relief='flat', borderwidth=0,
    #                                      selectbackground=self.colors['accent'], selectforeground=self.colors['text_inverse'])
    #     self.inst_summary_text.pack(fill='x')
    #     self.inst_summary_text.insert('1.0', "No instrument loaded")
    #     self.inst_summary_text.config(state='disabled')
    #
    #     # === SECTION C: Instrument Registry ===
    #     section_c = ttk.LabelFrame(main_frame, text="Instrument Registry",
    #                                padding="15", style='Card.TFrame')
    #     section_c.pack(fill='both', expand=True, pady=(0, 15))
    #
    #     # Registry table
    #     columns = ('ID', 'Vendor', 'Model', 'Δλ_med', 'Roughness', 'Detail Score')
    #     self.inst_registry_tree = ttk.Treeview(section_c, columns=columns, show='headings', height=10)
    #
    #     for col in columns:
    #         self.inst_registry_tree.heading(col, text=col)
    #         self.inst_registry_tree.column(col, width=120)
    #
    #     self.inst_registry_tree.pack(fill='both', expand=True)
    #
    #     # Registry buttons
    #     reg_btn_frame = ttk.Frame(section_c)
    #     reg_btn_frame.pack(fill='x', pady=(10, 0))
    #
    #     ttk.Button(reg_btn_frame, text="Save Registry",
    #               command=self._save_instrument_registry, style='Modern.TButton').pack(side='left', padx=5)
    #     ttk.Button(reg_btn_frame, text="Load Registry",
    #               command=self._load_instrument_registry, style='Modern.TButton').pack(side='left', padx=5)
    #     ttk.Button(reg_btn_frame, text="Delete Selected",
    #               command=self._delete_selected_instrument, style='Modern.TButton').pack(side='left', padx=5)

    # REMOVED - Instrument Lab functionality deprecated
    # def _browse_instrument_data(self):
    #     """Browse for instrument data directory."""
    #     directory = filedialog.askdirectory(title="Select Instrument Data Directory")
    #     if directory:
    #         self.inst_data_path.set(directory)

    # REMOVED - Instrument Lab functionality deprecated
    # def _load_and_characterize_instrument(self):
    #     """Load instrument data and compute characteristics."""
    #     if not HAS_CALIBRATION_TRANSFER:
    #         messagebox.showerror("Module Not Available",
    #             "Calibration transfer modules are not available.")
    #         return
    #
    #     inst_id = self.inst_id_entry.get().strip()
    #     data_path = self.inst_data_path.get()
    #
    #     if not inst_id:
    #         messagebox.showerror("Error", "Please enter an Instrument ID")
    #         return
    #
    #     if not data_path:
    #         messagebox.showerror("Error", "Please select a data directory")
    #         return
    #
    #     try:
    #         # Load spectral data from directory (reuse existing loader logic)
    #         from spectral_predict.io import (read_asd_dir, read_csv_spectra, read_spc_dir,
    #                                          read_jcamp_dir, read_ascii_spectra)
    #
    #         data_path_obj = Path(data_path)
    #
    #         # Try different formats in order
    #         data = None
    #
    #         # Check file types
    #         asd_files = list(data_path_obj.glob("*.asd"))
    #         spc_files = list(data_path_obj.glob("*.spc"))
    #         jcamp_files = list(data_path_obj.glob("*.jdx")) + list(data_path_obj.glob("*.dx"))
    #         ascii_files = (list(data_path_obj.glob("*.dpt")) + list(data_path_obj.glob("*.dat")) +
    #                       list(data_path_obj.glob("*.asc")))
    #
    #         try:
    #             if asd_files:
    #                 data, _ = read_asd_dir(str(data_path_obj))
    #             elif spc_files:
    #                 data, _ = read_spc_dir(str(data_path_obj))
    #             elif jcamp_files:
    #                 data, _ = read_jcamp_dir(str(data_path_obj))
    #             elif ascii_files:
    #                 data, _ = read_ascii_spectra(str(data_path_obj))
    #             else:
    #                 # Try CSV format as fallback
    #                 data, _ = read_csv_spectra(str(data_path_obj))
    #
    #             wavelengths = data.columns.astype(float).values
    #             X = data.values
    #         except Exception as e:
    #             raise ValueError(f"Could not load spectral data: {e}")
    #
    #         # Characterize instrument
    #         profile = characterize_instrument(
    #             instrument_id=inst_id,
    #             wavelengths=wavelengths,
    #             spectra=X,
    #             vendor="",
    #             model="",
    #             description=f"Loaded from {data_path}"
    #         )
    #
    #         # Store in registry
    #         self.instrument_profiles[inst_id] = profile
    #         self.current_instrument_data = (wavelengths, X)
    #         # Store spectral data persistently for use in Calibration Transfer
    #         self.instrument_spectral_data[inst_id] = (wavelengths, X)
    #
    #         # Update summary
    #         self._update_instrument_summary(profile)
    #
    #         # Update registry table
    #         self._update_registry_table()
    #
    #         # Update calibration transfer registry view
    #         self._refresh_ct_registry()
    #
    #         messagebox.showinfo("Success",
    #             f"Instrument '{inst_id}' characterized successfully!\n\n"
    #             f"Wavelength range: {wavelengths.min():.1f} - {wavelengths.max():.1f} nm\n"
    #             f"Channels: {len(wavelengths)}\n"
    #             f"Detail score: {profile.detail_score:.4f}")
    #
    #         # Clear fields for next entry
    #         self.inst_id_entry.delete(0, 'end')
    #         self.inst_data_path.set('')
    #
    #     except Exception as e:
    #         messagebox.showerror("Error",
    #             f"Failed to load and characterize instrument:\n{str(e)}")

    # REMOVED - Instrument Lab functionality deprecated
    # def _update_instrument_summary(self, profile):
    #     """Update the instrument summary text widget."""
    #     self.inst_summary_text.config(state='normal')
    #     self.inst_summary_text.delete('1.0', 'end')
    #
    #     # Check if data is interpolated
    #     interp_flag = " [INTERPOLATED]" if profile.is_interpolated else ""
    #
    #     summary = f"""Instrument ID: {profile.instrument_id}{interp_flag}
# Vendor: {profile.vendor or 'N/A'}
# Model: {profile.model or 'N/A'}
# Description: {profile.description or 'N/A'}
#
# Wavelength Range: {profile.wavelengths.min():.1f} - {profile.wavelengths.max():.1f} nm
# Number of Channels: {len(profile.wavelengths)}
#
# Data-Driven Metrics:
#   Median Wavelength Spacing (Δλ_med): {profile.delta_lambda_med:.4f} nm
#   Roughness (R): {profile.roughness_R:.6f}
#   Detail Score (R/Δλ): {profile.detail_score:.4f}
#
# Peak-Based Resolution Metrics:
#   Average Peak Count: {profile.peak_count if profile.peak_count is not None else 'N/A'}
#   Average Peak FWHM: {f'{profile.avg_peak_fwhm:.4f} nm' if profile.avg_peak_fwhm else 'N/A'}
#   Average Peak Sharpness: {f'{profile.avg_peak_sharpness:.4f}' if profile.avg_peak_sharpness else 'N/A'}
#
# Interpretation: {"Higher detail/resolution" if profile.detail_score > 0.01 else "Lower detail/resolution"}
# Note: {"Uniform wavelength spacing detected - data appears interpolated" if profile.is_interpolated else "Non-uniform spacing - likely native measurements"}
# """
    #     self.inst_summary_text.insert('1.0', summary)
    #     self.inst_summary_text.config(state='disabled')

    def _update_registry_table(self):
        """Update the instrument registry table."""
        # Clear existing items
        for item in self.inst_registry_tree.get_children():
            self.inst_registry_tree.delete(item)

        # Add instruments sorted by detail score
        if self.instrument_profiles:
            ranked = rank_instruments_by_detail(self.instrument_profiles)
            for inst_id in ranked:
                profile = self.instrument_profiles[inst_id]
                self.inst_registry_tree.insert('', 'end', values=(
                    profile.instrument_id,
                    profile.vendor or 'N/A',
                    profile.model or 'N/A',
                    f"{profile.delta_lambda_med:.4f}",
                    f"{profile.roughness_R:.6f}",
                    f"{profile.detail_score:.4f}"
                ))

    # REMOVED - Instrument Lab functionality deprecated
    # def _save_instrument_registry(self):
    #     """Save instrument registry to JSON file."""
    #     if not self.instrument_profiles:
    #         messagebox.showwarning("Warning", "No instruments to save")
    #         return
    #
    #     # Create filename with timestamp and instrument names
    #     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    #     inst_names = "_".join(list(self.instrument_profiles.keys())[:3])  # Use first 3 instrument names
    #     if len(self.instrument_profiles) > 3:
    #         inst_names += f"_plus{len(self.instrument_profiles)-3}more"
    #
    #     # Sanitize filename (remove invalid characters)
    #     inst_names = "".join(c if c.isalnum() or c in "-_" else "_" for c in inst_names)
    #     default_filename = f"instrument_registry_{inst_names}_{timestamp}.json"
    #
    #     filepath = filedialog.asksaveasfilename(
    #         title="Save Instrument Registry",
    #         defaultextension=".json",
    #         filetypes=[("JSON files", "*.json"), ("All files", "*.*")],
    #         initialfile=default_filename
    #     )
    #
    #     if not filepath:
    #         return
    #
    #     try:
    #         save_instrument_profiles(self.instrument_profiles, filepath)
    #         messagebox.showinfo("Success",
    #             f"Saved {len(self.instrument_profiles)} instruments to:\n{filepath}")
    #     except Exception as e:
    #         messagebox.showerror("Error",
    #             f"Failed to save registry:\n{str(e)}")

    # REMOVED - Instrument Lab functionality deprecated
    # def _load_instrument_registry(self):
    #     """Load instrument registry from JSON file."""
    #     filepath = filedialog.askopenfilename(
    #         title="Load Instrument Registry",
    #         filetypes=[("JSON files", "*.json"), ("All files", "*.*")]
    #     )
    #
    #     if not filepath:
    #         return
    #
    #     try:
    #         profiles = load_instrument_profiles(filepath)
    #         self.instrument_profiles = profiles
    #         self._update_registry_table()
    #         # Update calibration transfer registry view
    #         self._refresh_ct_registry()
    #         messagebox.showinfo("Registry Loaded",
    #             f"Loaded {len(profiles)} instruments from:\n{filepath}\n\n"
    #             f"Note: Registry contains metadata only.\n\n"
    #             f"To use instruments for Calibration Transfer:\n"
    #             f"• Re-characterize each instrument with its spectral data file\n"
    #             f"• This loads the actual spectra needed for transfer models")
    #     except Exception as e:
    #         messagebox.showerror("Error",
    #             f"Failed to load registry:\n{str(e)}")

    # REMOVED - Instrument Lab functionality deprecated
    # def _delete_selected_instrument(self):
    #     """Delete selected instrument from registry."""
    #     selection = self.inst_registry_tree.selection()
    #     if not selection:
    #         messagebox.showwarning("Warning", "Please select an instrument to delete")
    #         return
    #
    #     # Get instrument ID from selection
    #     item = selection[0]
    #     values = self.inst_registry_tree.item(item, 'values')
    #     inst_id = values[0]
    #
    #     # Confirm deletion
    #     if messagebox.askyesno("Confirm Deletion",
    #                           f"Delete instrument '{inst_id}' from registry?"):
    #         del self.instrument_profiles[inst_id]
    #         self._update_registry_table()

    # ======================================================================
    # TAB 9 HELPER METHODS: Calibration Transfer
    # ======================================================================

    def _browse_ct_master_model(self):
        """Browse for master model .pkl file."""
        filepath = filedialog.askopenfilename(
            title="Select Master Model",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if filepath:
            self.ct_master_model_path_var.set(filepath)

    def _load_ct_master_model(self):
        """Load master model from selected .pkl file."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        filepath = self.ct_master_model_path_var.get()
        if not filepath:
            messagebox.showwarning("Warning", "Please browse and select a master model file")
            return

        try:
            import pickle
            with open(filepath, 'rb') as f:
                self.ct_master_model_dict = pickle.load(f)

            # Display model info
            model_type = self.ct_master_model_dict.get('model_type', 'Unknown')
            n_components = self.ct_master_model_dict.get('n_components', 'N/A')
            wl_model = self.ct_master_model_dict.get('wavelengths', np.array([]))

            info_text = (f"Model Type: {model_type}\n"
                        f"Components: {n_components}\n"
                        f"Wavelength Range: {wl_model.min():.1f} - {wl_model.max():.1f} nm\n"
                        f"Number of Wavelengths: {len(wl_model)}")

            self.ct_model_info_text.config(state='normal')
            self.ct_model_info_text.delete('1.0', tk.END)
            self.ct_model_info_text.insert('1.0', info_text)
            self.ct_model_info_text.config(state='disabled')

            messagebox.showinfo("Success", "Master model loaded successfully")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to load model:\n{str(e)}")

    # REMOVED - Instrument Lab functionality deprecated
    # def _refresh_ct_instrument_combos(self):
    #     """Refresh instrument comboboxes from registry."""
    #     if not self.instrument_profiles:
    #         messagebox.showinfo("Info", "No instruments in registry. Please use Tab 8 to add instruments.")
    #         return
    #
    #     inst_ids = list(self.instrument_profiles.keys())
    #     self.ct_master_instrument_combo['values'] = inst_ids
    #     self.ct_slave_instrument_combo['values'] = inst_ids
    #
    #     messagebox.showinfo("Success", f"Loaded {len(inst_ids)} instruments from registry")

    def _refresh_ct_registry(self):
        """Refresh the transfer model registry view and instruments list."""
        # Update instruments listbox
        self.ct_instrument_listbox.delete(0, tk.END)
        for inst_id in sorted(self.instrument_spectral_data.keys()):
            self.ct_instrument_listbox.insert(tk.END, inst_id)

        # Update transfer models treeview
        for item in self.ct_registry_tree.get_children():
            self.ct_registry_tree.delete(item)

        for model_key, model_data in self.transfer_model_registry.items():
            master_id = model_data['master_id']
            slave_id = model_data['slave_id']
            method = model_data['method']
            date_built = model_data['date_built']
            n_samples = model_data['n_samples']

            self.ct_registry_tree.insert('', 'end', iid=model_key,
                                        values=(master_id, slave_id, method, date_built, n_samples))

        # Update registry combos in sections C and D
        registry_keys = list(self.transfer_model_registry.keys())
        if hasattr(self, 'ct_eq_registry_combo'):
            self.ct_eq_registry_combo['values'] = registry_keys
        if hasattr(self, 'ct_pred_registry_combo'):
            self.ct_pred_registry_combo['values'] = registry_keys

    def _load_model_from_registry(self):
        """Load selected transfer model from registry to current model."""
        selected = self.ct_registry_tree.selection()
        if not selected:
            messagebox.showwarning("Warning", "Please select a transfer model from the registry")
            return

        model_key = selected[0]
        model_data = self.transfer_model_registry[model_key]

        self.ct_transfer_model = model_data['model']

        messagebox.showinfo("Success",
            f"Loaded transfer model from registry:\n"
            f"Master: {model_data['master_id']}\n"
            f"Slave: {model_data['slave_id']}\n"
            f"Method: {model_data['method'].upper()}")

    def _delete_from_registry(self):
        """Delete selected transfer model from registry."""
        selected = self.ct_registry_tree.selection()
        if not selected:
            messagebox.showwarning("Warning", "Please select a transfer model to delete")
            return

        model_key = selected[0]
        model_data = self.transfer_model_registry[model_key]

        response = messagebox.askyesno("Confirm Delete",
            f"Delete transfer model?\n\n"
            f"Master: {model_data['master_id']}\n"
            f"Slave: {model_data['slave_id']}\n"
            f"Method: {model_data['method'].upper()}\n\n"
            f"This cannot be undone.")

        if response:
            del self.transfer_model_registry[model_key]
            self._refresh_ct_registry()
            messagebox.showinfo("Success", "Transfer model deleted from registry")

    def _import_model_to_registry(self):
        """Import a transfer model from file into the registry."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        from spectral_predict.calibration_transfer import load_transfer_model

        file_path = filedialog.askopenfilename(
            title="Select Transfer Model JSON File",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")]
        )

        if not file_path:
            return

        try:
            # Load the model
            transfer_model = load_transfer_model(file_path)

            # Try to extract master/slave IDs and method from the model
            # These might be stored as metadata in the model
            master_id = getattr(transfer_model, 'master_id', 'Unknown_Master')
            slave_id = getattr(transfer_model, 'slave_id', 'Unknown_Slave')
            method = transfer_model.method

            # Create registry key
            import datetime
            model_key = f"{master_id}_{slave_id}_{method}"

            # Check if already exists
            if model_key in self.transfer_model_registry:
                response = messagebox.askyesno("Model Exists",
                    f"A model with key '{model_key}' already exists in the registry.\n\n"
                    f"Do you want to replace it?")
                if not response:
                    return

            # Store in registry
            self.transfer_model_registry[model_key] = {
                'model': transfer_model,
                'master_id': master_id,
                'slave_id': slave_id,
                'method': method,
                'date_built': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'n_samples': 0,  # Unknown from loaded file
                'n_features': 0  # Unknown from loaded file
            }

            self._refresh_ct_registry()

            messagebox.showinfo("Success",
                f"Transfer model imported to registry:\n"
                f"Key: {model_key}")

        except Exception as e:
            messagebox.showerror("Error", f"Failed to import transfer model:\n{str(e)}")

    def _load_ct_eq_from_registry(self):
        """Load transfer model from registry for file equalization (Section C)."""
        model_key = self.ct_eq_registry_combo_var.get()
        if not model_key or model_key not in self.transfer_model_registry:
            messagebox.showwarning("Warning", "Please select a valid transfer model from the registry")
            return

        model_data = self.transfer_model_registry[model_key]
        self.ct_transfer_model = model_data['model']

        messagebox.showinfo("Success",
            f"Loaded transfer model from registry for file equalization:\n"
            f"Master: {model_data['master_id']}\n"
            f"Slave: {model_data['slave_id']}\n"
            f"Method: {model_data['method'].upper()}")

    def _load_ct_pred_from_registry(self):
        """Load transfer model from registry for prediction (Section D)."""
        model_key = self.ct_pred_registry_combo_var.get()
        if not model_key or model_key not in self.transfer_model_registry:
            messagebox.showwarning("Warning", "Please select a valid transfer model from the registry")
            return

        model_data = self.transfer_model_registry[model_key]
        self.ct_transfer_model = model_data['model']

        messagebox.showinfo("Success",
            f"Loaded transfer model from registry for prediction:\n"
            f"Master: {model_data['master_id']}\n"
            f"Slave: {model_data['slave_id']}\n"
            f"Method: {model_data['method'].upper()}")

    def _on_pred_tm_source_changed(self):
        """Handle transfer model source change in Section D."""
        source = self.ct_pred_tm_source_var.get()

        # Hide/show appropriate frames
        if source == 'registry':
            self.ct_pred_registry_frame.pack(fill='x', pady=(5, 0))
            self.ct_pred_load_tm_frame.pack_forget()
        elif source == 'file':
            self.ct_pred_registry_frame.pack_forget()
            self.ct_pred_load_tm_frame.pack(fill='x', pady=(5, 0))
        else:  # current
            self.ct_pred_registry_frame.pack_forget()
            self.ct_pred_load_tm_frame.pack_forget()

    def _browse_ct_pred_master_model(self):
        """Browse for master calibration model in Section D."""
        file_path = filedialog.askopenfilename(
            title="Select Master Calibration Model",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if file_path:
            self.ct_pred_master_model_var.set(file_path)

    def _load_ct_pred_master_model(self):
        """Load master calibration model in Section D."""
        model_path = self.ct_pred_master_model_var.get()
        if not model_path:
            messagebox.showwarning("Warning", "Please browse and select a master model file")
            return

        try:
            import pickle
            with open(model_path, 'rb') as f:
                self.ct_master_model_dict = pickle.load(f)

            messagebox.showinfo("Success", f"Master model loaded successfully")

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load master model:\n{str(e)}")

    def _browse_ct_master_spectra_dir(self):
        """Browse for master instrument standardization spectra directory."""
        directory = filedialog.askdirectory(title="Select Master Instrument Spectra Directory")
        if directory:
            self.ct_master_spectra_dir_var.set(directory)

    def _browse_ct_slave_spectra_dir(self):
        """Browse for slave instrument standardization spectra directory."""
        directory = filedialog.askdirectory(title="Select Slave Instrument Spectra Directory")
        if directory:
            self.ct_slave_spectra_dir_var.set(directory)

    def _load_ct_paired_spectra(self):
        """Load paired standardization spectra for master and slave instruments from separate directories."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        master_id = self.ct_master_instrument_id.get()
        slave_id = self.ct_slave_instrument_id.get()
        master_dir = self.ct_master_spectra_dir_var.get()
        slave_dir = self.ct_slave_spectra_dir_var.get()

        if not master_id or not slave_id:
            messagebox.showwarning("Warning", "Please select both master and slave instruments")
            return

        if not master_dir or not slave_dir:
            messagebox.showwarning("Warning", "Please browse and select both master and slave spectra directories")
            return

        if master_id not in self.instrument_profiles or slave_id not in self.instrument_profiles:
            messagebox.showerror("Error", "Selected instruments not found in registry")
            return

        # VALIDATION: Check that master and slave are different instruments
        if master_id == slave_id:
            messagebox.showerror(
                "Same Instrument Selected",
                "Master and slave instruments must be different for calibration transfer.\n\n"
                f"You selected: {master_id} for both master and slave.\n\n"
                "Please select different instruments."
            )
            return

        try:
            # Load spectra from SEPARATE directories
            wavelengths_master, X_master = self._load_spectra_from_directory(master_dir)
            wavelengths_slave, X_slave = self._load_spectra_from_directory(slave_dir)

            # VALIDATION 1: Same Sample Count Check
            if X_master.shape[0] != X_slave.shape[0]:
                messagebox.showerror(
                    "Sample Count Mismatch",
                    f"Master has {X_master.shape[0]} samples, Slave has {X_slave.shape[0]} samples.\n\n"
                    "Paired spectra must have the same number of samples (same sample set measured on both instruments).\n\n"
                    "Please ensure both instruments measured the exact same samples."
                )
                return

            # VALIDATION 2: Minimum Sample Check
            if X_master.shape[0] < 20:
                response = messagebox.askokcancel(
                    "Few Samples",
                    f"Only {X_master.shape[0]} paired samples loaded.\n\n"
                    "At least 30 samples recommended for robust calibration transfer.\n"
                    "Results may be unreliable with fewer samples.\n\n"
                    "Do you want to continue anyway?"
                )
                if not response:
                    return

            # VALIDATION 3: Wavelength Overlap Check
            master_range = (wavelengths_master[0], wavelengths_master[-1])
            slave_range = (wavelengths_slave[0], wavelengths_slave[-1])

            overlap_start = max(master_range[0], slave_range[0])
            overlap_end = min(master_range[1], slave_range[1])

            if overlap_start >= overlap_end:
                messagebox.showerror(
                    "No Wavelength Overlap",
                    f"Master range: {master_range[0]:.1f}-{master_range[1]:.1f} nm\n"
                    f"Slave range: {slave_range[0]:.1f}-{slave_range[1]:.1f} nm\n\n"
                    "Instruments must have overlapping wavelength ranges for calibration transfer.\n\n"
                    "Please select instruments with compatible wavelength coverage."
                )
                return

            # Check overlap percentage
            master_span = master_range[1] - master_range[0]
            slave_span = slave_range[1] - slave_range[0]
            overlap_span = overlap_end - overlap_start

            master_overlap_pct = (overlap_span / master_span) * 100
            slave_overlap_pct = (overlap_span / slave_span) * 100
            min_overlap_pct = min(master_overlap_pct, slave_overlap_pct)

            if min_overlap_pct < 80:
                response = messagebox.askokcancel(
                    "Limited Wavelength Overlap",
                    f"Wavelength overlap is {min_overlap_pct:.1f}% of instrument range.\n\n"
                    f"Master range: {master_range[0]:.1f}-{master_range[1]:.1f} nm\n"
                    f"Slave range: {slave_range[0]:.1f}-{slave_range[1]:.1f} nm\n"
                    f"Overlap region: {overlap_start:.1f}-{overlap_end:.1f} nm\n\n"
                    "Transfer quality may be reduced with limited overlap.\n"
                    "Consider using instruments with better wavelength coverage overlap.\n\n"
                    "Do you want to continue anyway?"
                )
                if not response:
                    return

            # Get instrument profiles
            master_prof = self.instrument_profiles[master_id]
            slave_prof = self.instrument_profiles[slave_id]

            # Choose common grid
            common_wl = choose_common_grid(
                {master_id: master_prof, slave_id: slave_prof},
                [master_id, slave_id]
            )

            # Resample both to common grid
            self.ct_X_master_common = resample_to_grid(X_master, wavelengths_master, common_wl)
            self.ct_X_slave_common = resample_to_grid(X_slave, wavelengths_slave, common_wl)
            self.ct_wavelengths_common = common_wl

            # Display info
            info_text = (f"Loaded {X_master.shape[0]} paired spectra\n"
                        f"Common wavelength grid: {common_wl.shape[0]} points\n"
                        f"Range: {common_wl.min():.1f} - {common_wl.max():.1f} nm\n"
                        f"Wavelength overlap: {min_overlap_pct:.1f}%")

            self.ct_spectra_info_text.config(state='normal')
            self.ct_spectra_info_text.delete('1.0', tk.END)
            self.ct_spectra_info_text.insert('1.0', info_text)
            self.ct_spectra_info_text.config(state='disabled')

            # Show preview plots immediately after loading
            self._plot_paired_spectra_preview(master_id, slave_id)

            messagebox.showinfo("Success", "Paired spectra loaded and resampled to common grid.\n\nPreview plots displayed.")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to load spectra:\n{str(e)}")

    # REMOVED - Instrument Lab functionality deprecated
    # def _import_from_instrument_lab(self):
    #     """Import paired spectra from Instrument Lab characterization data."""
    #     if not HAS_CALIBRATION_TRANSFER:
    #         messagebox.showerror("Error", "Calibration transfer modules not available")
    #         return
    #
    #     master_id = self.ct_master_instrument_id.get()
    #     slave_id = self.ct_slave_instrument_id.get()
    #
    #     if not master_id or not slave_id:
    #         messagebox.showwarning("Warning", "Please select both master and slave instruments from the dropdowns")
    #         return
    #
    #     # Check if both instruments have characterization data
    #     # First check if instrument is in registry but spectral data not loaded
    #     if master_id not in self.instrument_spectral_data:
    #         # Check if it's in the registry (metadata exists but data doesn't)
    #         if master_id in self.instrument_profiles:
    #             messagebox.showerror(
    #                 "Master Spectral Data Not Loaded",
    #                 f"Instrument '{master_id}' is in the registry, but spectral data is not loaded.\n\n"
    #                 f"This happens when you load a registry JSON file without the actual data files.\n\n"
    #                 f"To fix this:\n"
    #                 f"1. Go to the Instrument Lab tab\n"
    #                 f"2. Enter instrument ID: {master_id}\n"
    #                 f"3. Browse and load the original spectral data file\n"
    #                 f"4. Click 'Characterize Instrument'\n\n"
    #                 f"This will load the spectral data needed for calibration transfer."
    #             )
    #         else:
    #             messagebox.showerror(
    #                 "Master Data Not Found",
    #                 f"No characterization data found for '{master_id}' in Instrument Lab.\n\n"
    #                 "Please go to the Instrument Lab tab and load & characterize this instrument first."
    #             )
    #         return
    #
    #     if slave_id not in self.instrument_spectral_data:
    #         # Check if it's in the registry (metadata exists but data doesn't)
    #         if slave_id in self.instrument_profiles:
    #             messagebox.showerror(
    #                 "Slave Spectral Data Not Loaded",
    #                 f"Instrument '{slave_id}' is in the registry, but spectral data is not loaded.\n\n"
    #                 f"This happens when you load a registry JSON file without the actual data files.\n\n"
    #                 f"To fix this:\n"
    #                 f"1. Go to the Instrument Lab tab\n"
    #                 f"2. Enter instrument ID: {slave_id}\n"
    #                 f"3. Browse and load the original spectral data file\n"
    #                 f"4. Click 'Characterize Instrument'\n\n"
    #                 f"This will load the spectral data needed for calibration transfer."
    #             )
    #         else:
    #             messagebox.showerror(
    #                 "Slave Data Not Found",
    #                 f"No characterization data found for '{slave_id}' in Instrument Lab.\n\n"
    #                 "Please go to the Instrument Lab tab and load & characterize this instrument first."
    #             )
    #         return
    #
    #     # VALIDATION: Check that master and slave are different
    #     if master_id == slave_id:
    #         messagebox.showerror(
    #             "Same Instrument Selected",
    #             "Master and slave instruments must be different for calibration transfer.\n\n"
    #             f"You selected: {master_id} for both master and slave.\n\n"
    #             "Please select different instruments."
    #         )
    #         return
    #
    #     try:
    #         # Import data from Instrument Lab
    #         wavelengths_master, X_master = self.instrument_spectral_data[master_id]
    #         wavelengths_slave, X_slave = self.instrument_spectral_data[slave_id]
    #
    #         # Make copies to avoid modifying original data
    #         wavelengths_master = wavelengths_master.copy()
    #         X_master = X_master.copy()
    #         wavelengths_slave = wavelengths_slave.copy()
    #         X_slave = X_slave.copy()
    #
    #         # VALIDATION 1: Sample Count Check
    #         if X_master.shape[0] != X_slave.shape[0]:
    #             response = messagebox.askyesno(
    #                 "Sample Count Mismatch",
    #                 f"Master has {X_master.shape[0]} samples, Slave has {X_slave.shape[0]} samples.\n\n"
    #                 "For optimal calibration transfer, both instruments should measure the same samples.\n\n"
    #                 "Do you want to continue anyway?\n"
    #                 "(Will use minimum sample count)"
    #             )
    #             if not response:
    #                 return
    #
    #             # Use minimum number of samples
    #             min_samples = min(X_master.shape[0], X_slave.shape[0])
    #             X_master = X_master[:min_samples, :]
    #             X_slave = X_slave[:min_samples, :]
    #
    #         # VALIDATION 2: Minimum Sample Check
    #         if X_master.shape[0] < 20:
    #             response = messagebox.askokcancel(
    #                 "Few Samples",
    #                 f"Only {X_master.shape[0]} samples available.\n\n"
    #                 "At least 30 samples recommended for robust calibration transfer.\n"
    #                 "Results may be unreliable with fewer samples.\n\n"
    #                 "Do you want to continue anyway?"
    #             )
    #             if not response:
    #                 return
    #
    #         # VALIDATION 3: Wavelength Overlap Check
    #         from spectral_predict.calibration_transfer import resample_to_grid
    #
    #         master_range = (wavelengths_master[0], wavelengths_master[-1])
    #         slave_range = (wavelengths_slave[0], wavelengths_slave[-1])
    #
    #         overlap_start = max(master_range[0], slave_range[0])
    #         overlap_end = min(master_range[1], slave_range[1])
    #
    #         if overlap_start >= overlap_end:
    #             messagebox.showerror(
    #                 "No Wavelength Overlap",
    #                 f"Master range: {master_range[0]:.1f}-{master_range[1]:.1f} nm\n"
    #                 f"Slave range: {slave_range[0]:.1f}-{slave_range[1]:.1f} nm\n\n"
    #                 "Instruments must have overlapping wavelength ranges for calibration transfer.\n\n"
    #                 "Please select instruments with compatible wavelength coverage."
    #             )
    #             return
    #
    #         # Use overlap region as common grid
    #         master_in_overlap = (wavelengths_master >= overlap_start) & (wavelengths_master <= overlap_end)
    #         slave_in_overlap = (wavelengths_slave >= overlap_start) & (wavelengths_slave <= overlap_end)
    #
    #         common_wl = wavelengths_master[master_in_overlap]
    #
    #         # Resample slave to master's grid in overlap region
    #         X_master_common = X_master[:, master_in_overlap]
    #         X_slave_common = resample_to_grid(X_slave, wavelengths_slave, common_wl)
    #
    #         # Calculate overlap percentage
    #         master_span = master_range[1] - master_range[0]
    #         slave_span = slave_range[1] - slave_range[0]
    #         overlap_span = overlap_end - overlap_start
    #         min_overlap_pct = 100 * overlap_span / min(master_span, slave_span)
    #
    #         if min_overlap_pct < 80:
    #             response = messagebox.askokcancel(
    #                 "Limited Wavelength Overlap",
    #                 f"Wavelength overlap: {min_overlap_pct:.1f}%\n\n"
    #                 "Less than 80% overlap may result in suboptimal transfer performance.\n\n"
    #                 "Do you want to continue?"
    #             )
    #             if not response:
    #                 return
    #
    #         # Store for transfer model building
    #         self.ct_X_master_common = X_master_common
    #         self.ct_X_slave_common = X_slave_common
    #         self.ct_wavelengths_common = common_wl
    #
    #         # Display info
    #         info_text = (f"✅ Imported from Instrument Lab\n"
    #                     f"Samples: {X_master.shape[0]} paired spectra\n"
    #                     f"Common wavelength grid: {common_wl.shape[0]} points ({common_wl.min():.1f}-{common_wl.max():.1f} nm)\n"
    #                     f"Wavelength overlap: {min_overlap_pct:.1f}%")
    #
    #         self.ct_spectra_info_text.config(state='normal')
    #         self.ct_spectra_info_text.delete('1.0', tk.END)
    #         self.ct_spectra_info_text.insert('1.0', info_text)
    #         self.ct_spectra_info_text.config(state='disabled')
    #
    #         # Show preview plots
    #         self._plot_paired_spectra_preview(master_id, slave_id)
    #
    #         messagebox.showinfo("Success",
    #             f"Imported paired spectra from Instrument Lab!\n\n"
    #             f"Master: {master_id} ({X_master.shape[0]} samples)\n"
    #             f"Slave: {slave_id} ({X_slave.shape[0]} samples)\n\n"
    #             "Preview plots displayed. You can now build a transfer model in Section C.")
    #
    #     except Exception as e:
    #         messagebox.showerror("Error", f"Failed to import from Instrument Lab:\n{str(e)}")

    def _build_ct_transfer_model(self):
        """Build calibration transfer model (DS or PDS)."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        # VALIDATION: Data Loaded Check
        if not hasattr(self, 'ct_X_master_common') or not hasattr(self, 'ct_X_slave_common'):
            messagebox.showerror(
                "No Paired Spectra Loaded",
                "Please load paired standardization spectra in Section B first."
            )
            return

        if self.ct_X_master_common is None or self.ct_X_slave_common is None:
            messagebox.showerror(
                "No Paired Spectra Loaded",
                "Please load paired standardization spectra in Section B first."
            )
            return

        method = self.ct_method_var.get()
        master_id = self.ct_master_instrument_id.get()
        slave_id = self.ct_slave_instrument_id.get()

        # VALIDATION: Different Instruments Check
        if master_id == slave_id:
            messagebox.showerror(
                "Same Instrument Selected",
                "Master and slave instruments must be different for calibration transfer."
            )
            return

        try:
            if method == 'ds':
                # Build DS transfer model
                # VALIDATION: DS Ridge Lambda parameter
                try:
                    lam = float(self.ct_ds_lambda_var.get())
                    if lam <= 0 or lam > 100:
                        messagebox.showerror(
                            "Invalid Parameter",
                            f"DS Ridge Lambda must be between 0 and 100.\nYou entered: {lam}"
                        )
                        return
                except ValueError:
                    messagebox.showerror("Invalid Parameter", "DS Ridge Lambda must be a number.")
                    return
                A = estimate_ds(self.ct_X_master_common, self.ct_X_slave_common, lam=lam)

                # Create TransferModel object
                from spectral_predict.calibration_transfer import TransferModel
                self.ct_transfer_model = TransferModel(
                    master_id=master_id,
                    slave_id=slave_id,
                    method='ds',
                    wavelengths_common=self.ct_wavelengths_common,
                    params={'A': A},
                    meta={'lambda': lam, 'note': 'DS transfer built in GUI'}
                )

                info_text = (f"Transfer Method: Direct Standardization (DS)\n"
                            f"Master: {master_id} → Slave: {slave_id}\n"
                            f"Ridge Lambda: {lam}\n"
                            f"Matrix Shape: {A.shape}")

            elif method == 'pds':
                # Build PDS transfer model
                # VALIDATION: PDS Window parameter
                try:
                    window = int(self.ct_pds_window_var.get())
                    if window < 5 or window > 101:
                        messagebox.showerror(
                            "Invalid Parameter",
                            f"PDS Window must be between 5 and 101.\nYou entered: {window}"
                        )
                        return
                    if window % 2 == 0:
                        messagebox.showerror(
                            "Invalid Parameter",
                            f"PDS Window must be an odd number.\nYou entered: {window} (even)"
                        )
                        return
                except ValueError:
                    messagebox.showerror("Invalid Parameter", "PDS Window must be an integer.")
                    return
                B = estimate_pds(self.ct_X_master_common, self.ct_X_slave_common, window=window)

                from spectral_predict.calibration_transfer import TransferModel
                self.ct_transfer_model = TransferModel(
                    master_id=master_id,
                    slave_id=slave_id,
                    method='pds',
                    wavelengths_common=self.ct_wavelengths_common,
                    params={'B': B, 'window': window},
                    meta={'note': 'PDS transfer built in GUI'}
                )

                info_text = (f"Transfer Method: Piecewise Direct Standardization (PDS)\n"
                            f"Master: {master_id} → Slave: {slave_id}\n"
                            f"Window Size: {window}\n"
                            f"Coefficient Matrix Shape: {B.shape}")

            elif method == 'tsr':
                # Build TSR (Transfer Sample Regression) model
                from spectral_predict.calibration_transfer import estimate_tsr, TransferModel
                from spectral_predict.sample_selection import kennard_stone

                # Get number of transfer samples (default 12)
                try:
                    n_transfer = int(getattr(self, 'ct_tsr_n_samples_var', tk.IntVar(value=12)).get())
                    if n_transfer < 2:
                        messagebox.showerror("Invalid Parameter", "Need at least 2 transfer samples")
                        return
                    if n_transfer > self.ct_X_master_common.shape[0]:
                        messagebox.showerror("Invalid Parameter",
                            f"Cannot select {n_transfer} samples from {self.ct_X_master_common.shape[0]} available")
                        return
                except (ValueError, AttributeError):
                    n_transfer = 12  # Default

                # Select transfer samples using Kennard-Stone
                transfer_indices = kennard_stone(self.ct_X_master_common, n_samples=n_transfer)

                # Estimate TSR model
                tsr_params = estimate_tsr(
                    self.ct_X_master_common,
                    self.ct_X_slave_common,
                    transfer_indices
                )

                self.ct_transfer_model = TransferModel(
                    master_id=master_id,
                    slave_id=slave_id,
                    method='tsr',
                    wavelengths_common=self.ct_wavelengths_common,
                    params=tsr_params,
                    meta={'note': 'TSR transfer built in GUI', 'n_transfer_samples': n_transfer}
                )

                info_text = (f"Transfer Method: Transfer Sample Regression (TSR)\n"
                            f"Master: {master_id} → Slave: {slave_id}\n"
                            f"Transfer Samples: {n_transfer} (Kennard-Stone selection)\n"
                            f"Mean R²: {tsr_params['mean_r_squared']:.4f}\n"
                            f"Slope Range: [{tsr_params['slope'].min():.3f}, {tsr_params['slope'].max():.3f}]")

            elif method == 'ctai':
                # Build CTAI (Affine Invariance) model - NO transfer samples needed!
                from spectral_predict.calibration_transfer import estimate_ctai, TransferModel

                # Estimate CTAI model
                ctai_params = estimate_ctai(
                    self.ct_X_master_common,
                    self.ct_X_slave_common
                )

                self.ct_transfer_model = TransferModel(
                    master_id=master_id,
                    slave_id=slave_id,
                    method='ctai',
                    wavelengths_common=self.ct_wavelengths_common,
                    params=ctai_params,
                    meta={'note': 'CTAI transfer built in GUI (no standards needed)'}
                )

                info_text = (f"Transfer Method: CTAI (Affine Invariance)\n"
                            f"Master: {master_id} → Slave: {slave_id}\n"
                            f"NO TRANSFER SAMPLES NEEDED ✓\n"
                            f"Components: {ctai_params['n_components']}\n"
                            f"Explained Variance: {ctai_params['explained_variance']:.4f}\n"
                            f"Reconstruction RMSE: {ctai_params['reconstruction_error']:.6f}")

            elif method == 'jypls-inv':
                # Build JYPLS-inv (Joint-Y PLS with Inversion) model
                from spectral_predict.calibration_transfer import estimate_jypls_inv, TransferModel
                from spectral_predict.sample_selection import kennard_stone

                # Get number of transfer samples
                try:
                    n_transfer = int(getattr(self, 'ct_jypls_n_samples_var', tk.IntVar(value=12)).get())
                    if n_transfer < 5:
                        messagebox.showerror("Invalid Parameter", "JYPLS-inv needs at least 5 transfer samples")
                        return
                    if n_transfer > self.ct_X_master_common.shape[0]:
                        messagebox.showerror("Invalid Parameter",
                            f"Cannot select {n_transfer} samples from {self.ct_X_master_common.shape[0]} available")
                        return
                except (ValueError, AttributeError):
                    n_transfer = 12  # Default

                # Get PLS components
                n_comp_str = self.ct_jypls_n_components_var.get()
                if n_comp_str == 'Auto':
                    n_components = None  # Auto-select via CV
                else:
                    n_components = int(n_comp_str)

                # Select transfer samples using Kennard-Stone
                transfer_indices = kennard_stone(self.ct_X_master_common, n_samples=n_transfer)

                # Need Y values for JYPLS-inv - use spectral mean as pseudo-Y
                # In real applications, user would provide reference values
                y_transfer = self.ct_X_master_common[transfer_indices].mean(axis=1)

                # Estimate JYPLS-inv model
                jypls_params = estimate_jypls_inv(
                    self.ct_X_master_common,
                    self.ct_X_slave_common,
                    y_transfer,
                    transfer_indices,
                    n_components=n_components
                )

                self.ct_transfer_model = TransferModel(
                    master_id=master_id,
                    slave_id=slave_id,
                    method='jypls-inv',
                    wavelengths_common=self.ct_wavelengths_common,
                    params=jypls_params,
                    meta={'note': 'JYPLS-inv transfer built in GUI', 'n_transfer_samples': n_transfer}
                )

                info_text = (f"Transfer Method: JYPLS-inv (Joint-Y PLS with Inversion)\n"
                            f"Master: {master_id} → Slave: {slave_id}\n"
                            f"Transfer Samples: {n_transfer} (Kennard-Stone selection)\n"
                            f"PLS Components: {jypls_params['n_components']}\n"
                            f"CV RMSE: {jypls_params['cv_rmse']:.6f}\n"
                            f"Explained Variance: {jypls_params['explained_variance_ratio']:.4f}")

            elif method == 'nspfce':
                # Build NS-PFCE (Non-supervised Parameter-Free Calibration Enhancement) model
                from spectral_predict.calibration_transfer import estimate_nspfce, TransferModel

                # Get NS-PFCE parameters
                use_wavelength_selection = self.ct_nspfce_use_wavelength_selection_var.get()
                wavelength_selector = self.ct_nspfce_selector_var.get()

                try:
                    max_iterations = int(self.ct_nspfce_max_iterations_var.get())
                    if max_iterations < 10 or max_iterations > 500:
                        messagebox.showerror(
                            "Invalid Parameter",
                            f"NS-PFCE Max Iterations must be between 10 and 500.\nYou entered: {max_iterations}"
                        )
                        return
                except ValueError:
                    messagebox.showerror("Invalid Parameter", "NS-PFCE Max Iterations must be an integer.")
                    return

                # Estimate NS-PFCE model
                nspfce_params = estimate_nspfce(
                    self.ct_X_master_common,
                    self.ct_X_slave_common,
                    self.ct_wavelengths_common,
                    use_wavelength_selection=use_wavelength_selection,
                    wavelength_selector=wavelength_selector,
                    max_iterations=max_iterations
                )

                self.ct_transfer_model = TransferModel(
                    master_id=master_id,
                    slave_id=slave_id,
                    method='nspfce',
                    wavelengths_common=self.ct_wavelengths_common,
                    params=nspfce_params,
                    meta={
                        'note': 'NS-PFCE transfer built in GUI',
                        'use_wavelength_selection': use_wavelength_selection,
                        'wavelength_selector': wavelength_selector if use_wavelength_selection else 'N/A'
                    }
                )

                # Build info text
                info_lines = [
                    f"Transfer Method: NS-PFCE (Non-supervised Parameter-Free)",
                    f"Master: {master_id} → Slave: {slave_id}",
                    f"Iterations: {nspfce_params['n_iterations']} / {max_iterations}",
                    f"Converged: {'Yes ✓' if nspfce_params['converged'] else 'No (max iter reached)'}",
                ]

                if use_wavelength_selection:
                    n_selected = len(nspfce_params.get('selected_wavelength_indices', []))
                    n_total = len(self.ct_wavelengths_common)
                    info_lines.append(f"Wavelength Selection: {wavelength_selector.upper()}")
                    info_lines.append(f"Selected Wavelengths: {n_selected} / {n_total} ({100*n_selected/n_total:.1f}%)")
                else:
                    info_lines.append(f"Wavelength Selection: Not used")

                if nspfce_params['convergence_history']:
                    final_error = nspfce_params['convergence_history'][-1]
                    info_lines.append(f"Final RMSE: {final_error:.6f}")

                info_text = "\n".join(info_lines)

            # Display transfer model info
            self.ct_transfer_info_text.config(state='normal')
            self.ct_transfer_info_text.delete('1.0', tk.END)
            self.ct_transfer_info_text.insert('1.0', info_text)
            self.ct_transfer_info_text.config(state='disabled')

            # Generate transfer quality plots
            self._plot_transfer_quality(method)

            # Save to transfer model registry
            master_id = self.ct_master_instrument_id.get()
            slave_id = self.ct_slave_instrument_id.get()
            if master_id and slave_id:
                import datetime
                model_key = f"{master_id}_{slave_id}_{method}"
                # Store model with metadata
                self.transfer_model_registry[model_key] = {
                    'model': self.ct_transfer_model,
                    'master_id': master_id,
                    'slave_id': slave_id,
                    'method': method,
                    'date_built': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'n_samples': self.ct_X_master_common.shape[0] if self.ct_X_master_common is not None else 0,
                    'n_features': self.ct_wavelengths_common.shape[0] if self.ct_wavelengths_common is not None else 0
                }

            messagebox.showinfo("Success",
                f"{method.upper()} transfer model built successfully\n\n"
                f"Model saved to registry: {model_key}" if master_id and slave_id else
                f"{method.upper()} transfer model built successfully")
        except KeyError as e:
            # Specific handling for missing dictionary keys
            messagebox.showerror(
                "Configuration Error",
                f"Failed to build transfer model - missing expected parameter:\n{str(e)}\n\n"
                f"This may indicate a version mismatch or incomplete calibration transfer implementation."
            )
        except ValueError as e:
            # Specific handling for validation errors
            messagebox.showerror(
                "Data Validation Error",
                f"Failed to build transfer model due to invalid data:\n{str(e)}\n\n"
                f"Please check your data for NaN/inf values or ensure data shapes are correct."
            )
        except np.linalg.LinAlgError as e:
            # Specific handling for numerical errors
            messagebox.showerror(
                "Numerical Error",
                f"Failed to build transfer model due to numerical instability:\n{str(e)}\n\n"
                f"This often occurs with poorly conditioned data. Try:\n"
                f"- Preprocessing your data (scaling, normalization)\n"
                f"- Using more samples\n"
                f"- Checking for duplicate or near-duplicate spectra"
            )
        except Exception as e:
            # Generic fallback for unexpected errors
            import traceback
            error_details = traceback.format_exc()
            print(f"Transfer model build error:\n{error_details}")  # Log to console
            messagebox.showerror(
                "Error",
                f"Failed to build transfer model:\n{str(e)}\n\n"
                f"Check the console for detailed traceback."
            )

    def _save_ct_transfer_model(self):
        """Save current transfer model to disk."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        if self.ct_transfer_model is None:
            messagebox.showwarning("Warning", "No transfer model to save. Please build one first.")
            return

        directory = filedialog.askdirectory(title="Select Directory to Save Transfer Model")
        if not directory:
            return

        try:
            path_prefix = save_transfer_model(
                self.ct_transfer_model,
                directory=directory,
                name=None  # Auto-generate name
            )
            messagebox.showinfo("Success",
                f"Transfer model saved to:\n{path_prefix}.json\n{path_prefix}.npz")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to save transfer model:\n{str(e)}")

    def _load_multiinstrument_dataset(self):
        """Load multi-instrument dataset for equalization."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        # Browse for base directory containing instrument subdirectories
        base_directory = filedialog.askdirectory(
            title="Select Base Directory (containing instrument subdirectories)"
        )
        if not base_directory:
            return

        try:
            import glob

            # Find subdirectories (each represents an instrument)
            subdirs = [d for d in os.listdir(base_directory)
                      if os.path.isdir(os.path.join(base_directory, d))]

            if len(subdirs) == 0:
                messagebox.showerror("Error",
                    "No subdirectories found in selected directory.\n\n"
                    "Expected structure:\n"
                    "  base_directory/\n"
                    "    instrument1/\n"
                    "      *.asd files\n"
                    "    instrument2/\n"
                    "      *.asd files\n")
                return

            # Load spectra from each subdirectory
            self.ct_multiinstrument_data = {}
            summary_lines = []

            for subdir in sorted(subdirs):
                instrument_id = subdir
                subdir_path = os.path.join(base_directory, subdir)

                try:
                    # Load spectra from this instrument's directory
                    wavelengths, X_data = self._load_spectra_from_directory(subdir_path)

                    # Store in dictionary
                    self.ct_multiinstrument_data[instrument_id] = (wavelengths, X_data)

                    # Add to summary
                    summary_lines.append(
                        f"{instrument_id}: {X_data.shape[0]} samples, "
                        f"{wavelengths.shape[0]} wavelengths "
                        f"({wavelengths.min():.1f}-{wavelengths.max():.1f} nm)"
                    )

                except Exception as e:
                    messagebox.showwarning("Warning",
                        f"Failed to load spectra from {instrument_id}:\n{str(e)}\n\n"
                        f"Skipping this instrument.")
                    continue

            if len(self.ct_multiinstrument_data) < 2:
                messagebox.showerror("Error",
                    f"Need at least 2 instruments for equalization. "
                    f"Only loaded {len(self.ct_multiinstrument_data)}.")
                self.ct_multiinstrument_data = None
                return

            # Display summary
            summary_text = (
                f"Loaded {len(self.ct_multiinstrument_data)} instruments:\n\n" +
                "\n".join(summary_lines)
            )

            self.ct_equalize_summary_text.config(state='normal')
            self.ct_equalize_summary_text.delete('1.0', tk.END)
            self.ct_equalize_summary_text.insert('1.0', summary_text)
            self.ct_equalize_summary_text.config(state='disabled')

            messagebox.showinfo("Success",
                f"Successfully loaded {len(self.ct_multiinstrument_data)} instruments.\n"
                f"Ready for equalization.")

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load multi-instrument dataset:\n{str(e)}")
            self.ct_multiinstrument_data = None

    def _equalize_and_export(self):
        """Equalize multi-instrument dataset and export."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        # Check that multi-instrument data has been loaded
        if self.ct_multiinstrument_data is None:
            messagebox.showwarning("Warning",
                "Please load multi-instrument dataset first using 'Load Multi-Instrument Dataset' button.")
            return

        if len(self.ct_multiinstrument_data) < 2:
            messagebox.showerror("Error",
                f"Need at least 2 instruments for equalization. "
                f"Currently loaded: {len(self.ct_multiinstrument_data)}")
            return

        try:
            # Update summary text to show processing status
            self.ct_equalize_summary_text.config(state='normal')
            self.ct_equalize_summary_text.insert(tk.END, "\n\nProcessing equalization...")
            self.ct_equalize_summary_text.config(state='disabled')
            self.ct_equalize_summary_text.update()

            # Check if we have instrument profiles for all instruments
            # If not, we'll use a simplified version without profiles
            instrument_ids = list(self.ct_multiinstrument_data.keys())
            has_all_profiles = all(inst_id in self.instrument_profiles
                                  for inst_id in instrument_ids)

            if has_all_profiles:
                # Use full equalization with profiles
                from spectral_predict.equalization import equalize_dataset

                wavelengths_common, X_equalized = equalize_dataset(
                    spectra_by_instrument=self.ct_multiinstrument_data,
                    profiles=self.instrument_profiles
                )
            else:
                # Use simplified equalization without profiles
                # Just find common wavelength grid and resample
                from spectral_predict.calibration_transfer import resample_to_grid

                # Find overlapping wavelength range across all instruments
                min_wl = max(wl.min() for wl, _ in self.ct_multiinstrument_data.values())
                max_wl = min(wl.max() for wl, _ in self.ct_multiinstrument_data.values())

                # Use the coarsest spacing to avoid over-sampling
                coarsest_spacing = max(
                    np.median(np.diff(wl))
                    for wl, _ in self.ct_multiinstrument_data.values()
                )

                # Generate common wavelength grid
                wavelengths_common = np.arange(min_wl, max_wl + coarsest_spacing, coarsest_spacing)

                # Resample all instruments to common grid
                equalized_spectra = []
                for inst_id, (wavelengths, X) in self.ct_multiinstrument_data.items():
                    X_resampled = resample_to_grid(X, wavelengths, wavelengths_common)
                    equalized_spectra.append(X_resampled)

                # Stack all equalized spectra
                X_equalized = np.vstack(equalized_spectra)

            # Create sample IDs with instrument prefixes
            sample_ids = []
            for inst_id, (_, X) in self.ct_multiinstrument_data.items():
                n_samples = X.shape[0]
                for i in range(n_samples):
                    sample_ids.append(f"{inst_id}_sample{i+1:03d}")

            # Store results
            self.ct_equalized_wavelengths = wavelengths_common
            self.ct_equalized_X = X_equalized
            self.ct_equalized_sample_ids = sample_ids

            # Prompt for export file
            export_path = filedialog.asksaveasfilename(
                title="Save Equalized Dataset",
                defaultextension=".csv",
                filetypes=[("CSV files", "*.csv"), ("All files", "*.*")],
                initialdir=os.getcwd()
            )

            if not export_path:
                # User cancelled, but still show summary
                summary_text = (
                    f"Equalization complete (not exported):\n\n"
                    f"Common wavelength grid: {len(wavelengths_common)} points\n"
                    f"Range: {wavelengths_common.min():.1f} - {wavelengths_common.max():.1f} nm\n"
                    f"Total samples: {len(sample_ids)}\n"
                    f"Data shape: {X_equalized.shape}"
                )

                self.ct_equalize_summary_text.config(state='normal')
                self.ct_equalize_summary_text.delete('1.0', tk.END)
                self.ct_equalize_summary_text.insert('1.0', summary_text)
                self.ct_equalize_summary_text.config(state='disabled')
                return

            # Export to CSV
            # First row: wavelengths (with empty first cell for sample ID column)
            # Subsequent rows: sample_id, spectrum values
            with open(export_path, 'w', newline='') as f:
                import csv
                writer = csv.writer(f)

                # Write header row (wavelengths)
                header = ['sample_id'] + [f"{wl:.2f}" for wl in wavelengths_common]
                writer.writerow(header)

                # Write data rows
                for sample_id, spectrum in zip(sample_ids, X_equalized):
                    row = [sample_id] + list(spectrum)
                    writer.writerow(row)

            # Display summary
            summary_text = (
                f"Equalization complete and exported!\n\n"
                f"Common wavelength grid: {len(wavelengths_common)} points\n"
                f"Range: {wavelengths_common.min():.1f} - {wavelengths_common.max():.1f} nm\n"
                f"Total samples: {len(sample_ids)}\n"
                f"Data shape: {X_equalized.shape}\n\n"
                f"Exported to:\n{export_path}"
            )

            self.ct_equalize_summary_text.config(state='normal')
            self.ct_equalize_summary_text.delete('1.0', tk.END)
            self.ct_equalize_summary_text.insert('1.0', summary_text)
            self.ct_equalize_summary_text.config(state='disabled')

            # Generate equalization quality plots
            # Prepare data for plotting: need to split X_equalized back by instrument
            equalized_by_instrument = {}
            start_idx = 0
            for inst_id, (_, X) in self.ct_multiinstrument_data.items():
                n_samples = X.shape[0]
                equalized_by_instrument[inst_id] = X_equalized[start_idx:start_idx + n_samples, :]
                start_idx += n_samples

            self._plot_equalization_quality(
                self.ct_multiinstrument_data,
                equalized_by_instrument,
                wavelengths_common
            )

            messagebox.showinfo("Success",
                f"Equalized dataset exported successfully!\n\n"
                f"Samples: {len(sample_ids)}\n"
                f"Wavelengths: {len(wavelengths_common)}\n"
                f"File: {os.path.basename(export_path)}")

        except Exception as e:
            messagebox.showerror("Error", f"Failed to equalize and export:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _browse_ct_pred_transfer_model(self):
        """Browse for transfer model to use for prediction."""
        filepath = filedialog.askopenfilename(
            title="Select Transfer Model (JSON)",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")],
            initialdir=os.getcwd()
        )
        if filepath:
            # Remove .json extension to get prefix
            if filepath.endswith('.json'):
                filepath = filepath[:-5]
            self.ct_pred_tm_path_var.set(filepath)

    def _load_ct_pred_transfer_model(self):
        """Load transfer model for prediction."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        path_prefix = self.ct_pred_tm_path_var.get()
        if not path_prefix:
            messagebox.showwarning("Warning", "Please browse and select a transfer model")
            return

        try:
            self.ct_pred_transfer_model = load_transfer_model(path_prefix)
            messagebox.showinfo("Success",
                f"Transfer model loaded:\n"
                f"Method: {self.ct_pred_transfer_model.method.upper()}\n"
                f"Master: {self.ct_pred_transfer_model.master_id}\n"
                f"Slave: {self.ct_pred_transfer_model.slave_id}")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to load transfer model:\n{str(e)}")

    def _browse_ct_new_slave_dir(self):
        """Browse for new slave spectra directory."""
        directory = filedialog.askdirectory(title="Select New Slave Spectra Directory")
        if directory:
            self.ct_new_slave_dir_var.set(directory)

    def _load_and_predict_ct(self):
        """Load new slave spectra, apply transfer, and predict."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        # VALIDATION: Transfer Model Check
        if self.ct_pred_transfer_model is None:
            messagebox.showerror(
                "Transfer Model Not Loaded",
                "Please load a transfer model first using the 'Browse Transfer Model' button above.\n\n"
                "The transfer model contains the calibration transfer mapping between instruments."
            )
            return

        # VALIDATION: Master Model Check (needed for prediction)
        if self.ct_master_model_dict is None:
            messagebox.showerror(
                "Master Model Required for Prediction",
                "To make predictions, you need to load the master model in Section A.\n\n"
                "The master model is the trained PLS/PCR model that makes the actual predictions.\n\n"
                "If you only want to transform spectra WITHOUT making predictions, "
                "use Section F 'File Equalize' instead."
            )
            return

        new_slave_dir = self.ct_new_slave_dir_var.get()
        if not new_slave_dir:
            messagebox.showwarning("Warning", "Please browse and select new slave spectra directory")
            return

        try:
            # Load new slave spectra
            wavelengths_slave, X_slave_new = self._load_spectra_from_directory(new_slave_dir)

            # VALIDATION: Wavelength Compatibility Check
            transfer_slave_range = (
                self.ct_pred_transfer_model.wavelengths_common[0],
                self.ct_pred_transfer_model.wavelengths_common[-1]
            )
            new_slave_range = (wavelengths_slave[0], wavelengths_slave[-1])

            # Check if new slave data can be resampled to transfer model wavelengths
            if new_slave_range[0] > transfer_slave_range[0] or new_slave_range[1] < transfer_slave_range[1]:
                messagebox.showwarning(
                    "Wavelength Range Mismatch",
                    f"Transfer model expects wavelengths: {transfer_slave_range[0]:.1f}-{transfer_slave_range[1]:.1f} nm\n"
                    f"New slave data has wavelengths: {new_slave_range[0]:.1f}-{new_slave_range[1]:.1f} nm\n\n"
                    "New slave data has narrower wavelength coverage than the transfer model expects.\n"
                    "Predictions may require extrapolation and could be unreliable."
                )

            # Resample to common grid
            common_wl = self.ct_pred_transfer_model.wavelengths_common
            X_slave_common = resample_to_grid(X_slave_new, wavelengths_slave, common_wl)

            # Apply transfer model
            if self.ct_pred_transfer_model.method == 'ds':
                A = self.ct_pred_transfer_model.params['A']
                X_transferred = apply_ds(X_slave_common, A)
            elif self.ct_pred_transfer_model.method == 'pds':
                B = self.ct_pred_transfer_model.params['B']
                window = self.ct_pred_transfer_model.params['window']
                X_transferred = apply_pds(X_slave_common, B, window)
            elif self.ct_pred_transfer_model.method == 'tsr':
                from spectral_predict.calibration_transfer import apply_tsr
                X_transferred = apply_tsr(X_slave_common, self.ct_pred_transfer_model.params)
            elif self.ct_pred_transfer_model.method == 'ctai':
                from spectral_predict.calibration_transfer import apply_ctai
                X_transferred = apply_ctai(X_slave_common, self.ct_pred_transfer_model.params)
            elif self.ct_pred_transfer_model.method == 'jypls-inv':
                from spectral_predict.calibration_transfer import apply_jypls_inv
                X_transferred = apply_jypls_inv(X_slave_common, self.ct_pred_transfer_model.params)
            elif self.ct_pred_transfer_model.method == 'nspfce':
                from spectral_predict.calibration_transfer import apply_nspfce
                X_transferred = apply_nspfce(X_slave_common, self.ct_pred_transfer_model.params)
            else:
                raise ValueError(f"Unknown transfer method: {self.ct_pred_transfer_model.method}")

            # Resample transferred spectra to master model's wavelength grid
            wl_model = self.ct_master_model_dict['wavelengths']
            X_for_prediction = resample_to_grid(X_transferred, common_wl, wl_model)

            # VALIDATION: Extrapolation Warning
            if 'wavelength_range' in self.ct_master_model_dict:
                model_wl_range = self.ct_master_model_dict['wavelength_range']
                if wl_model[0] < model_wl_range[0] or wl_model[-1] > model_wl_range[1]:
                    messagebox.showwarning(
                        "Extrapolation Warning",
                        f"Transferred data wavelengths ({wl_model[0]:.1f}-{wl_model[-1]:.1f} nm)\n"
                        f"exceed master model training range ({model_wl_range[0]:.1f}-{model_wl_range[1]:.1f} nm).\n\n"
                        "Predictions may be unreliable in extrapolated regions."
                    )

            # Apply preprocessing if present
            if 'preprocessing' in self.ct_master_model_dict:
                prep = self.ct_master_model_dict['preprocessing']
                X_for_prediction = prep.transform(X_for_prediction)

            # Predict
            model = self.ct_master_model_dict['model']
            y_pred = model.predict(X_for_prediction).ravel()

            # Store predictions
            self.ct_pred_y_pred = y_pred
            self.ct_pred_sample_ids = [f"Sample_{i+1}" for i in range(len(y_pred))]

            # Display results
            pred_text = f"Transferred {len(y_pred)} spectra using {self.ct_pred_transfer_model.method.upper()}\n"
            pred_text += f"Predictions (first 10):\n"
            for i in range(min(10, len(y_pred))):
                pred_text += f"  {self.ct_pred_sample_ids[i]}: {y_pred[i]:.3f}\n"

            if len(y_pred) > 10:
                pred_text += f"  ... and {len(y_pred) - 10} more\n"

            pred_text += f"\nMean: {y_pred.mean():.3f}, Std: {y_pred.std():.3f}"

            self.ct_prediction_text.config(state='normal')
            self.ct_prediction_text.delete('1.0', tk.END)
            self.ct_prediction_text.insert('1.0', pred_text)
            self.ct_prediction_text.config(state='disabled')

            # Generate prediction plots
            self._plot_ct_predictions(y_pred)

            messagebox.showinfo("Success", f"Predictions generated for {len(y_pred)} samples")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to predict:\n{str(e)}")

    def _export_ct_predictions(self):
        """Export calibration transfer predictions to CSV."""
        if self.ct_pred_y_pred is None:
            messagebox.showwarning("Warning", "No predictions to export. Please run prediction first.")
            return

        filepath = filedialog.asksaveasfilename(
            title="Save Predictions",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv"), ("All files", "*.*")]
        )

        if not filepath:
            return

        try:
            df = pd.DataFrame({
                'Sample_ID': self.ct_pred_sample_ids,
                'Prediction': self.ct_pred_y_pred
            })
            df.to_csv(filepath, index=False)
            messagebox.showinfo("Success", f"Predictions exported to:\n{filepath}")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to export predictions:\n{str(e)}")

    # ======================================================================
    # File Equalize Helper Methods (Section F)
    # ======================================================================

    def _on_eq_tm_source_changed(self):
        """Show/hide transfer model file selection based on source selection."""
        if self.ct_eq_tm_source_var.get() == 'file':
            self.ct_eq_load_tm_frame.pack(fill='x', pady=(5, 10))
        else:
            self.ct_eq_load_tm_frame.pack_forget()

    def _on_mode_selected(self):
        """Handle application mode selection.

        Updates self.application_mode and provides user feedback.
        Shows/hides Section C or Section D based on the selected mode.
        """
        mode = self.ct_mode_var.get()

        # Update application mode
        self.application_mode = mode

        # Provide user feedback
        if mode == 'predict':
            self.ct_mode_status_label.config(
                text="Mode A selected: Ready to configure prediction workflow (Section C)",
                foreground=self.colors['accent']
            )
            # Show Section C, hide Section D
            self.ct_step3a_frame.pack(fill='x', pady=(0, 15))
            self.ct_step3b_frame.pack_forget()

        elif mode == 'export':
            self.ct_mode_status_label.config(
                text="Mode B selected: Ready to configure export workflow (Section D)",
                foreground=self.colors['accent']
            )
            # Hide Section C, show Section D
            self.ct_step3a_frame.pack_forget()
            self.ct_step3b_frame.pack(fill='x', pady=(0, 15))
        else:
            self.ct_mode_status_label.config(text="", foreground='gray')
            # Hide both sections if no mode selected
            self.ct_step3a_frame.pack_forget()
            self.ct_step3b_frame.pack_forget()

    def _browse_ct_eq_transfer_model(self):
        """Browse for transfer model .json file for equalization."""
        filepath = filedialog.askopenfilename(
            title="Select Transfer Model",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")]
        )
        if filepath:
            self.ct_eq_tm_path_var.set(filepath)

    def _load_ct_eq_transfer_model(self):
        """Load transfer model from file for equalization."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        filepath = self.ct_eq_tm_path_var.get()
        if not filepath:
            messagebox.showwarning("Warning", "Please browse and select a transfer model file")
            return

        try:
            from spectral_predict.calibration_transfer import TransferModel

            tm = TransferModel.load(filepath)
            self.ct_eq_loaded_transfer_model = tm

            info_text = (f"Loaded Transfer Model:\n"
                        f"  Master: {tm.master_id}\n"
                        f"  Slave: {tm.slave_id}\n"
                        f"  Method: {tm.method.upper()}\n"
                        f"  Wavelengths: {len(tm.wavelengths_common)}")

            self.ct_eq_status_text.config(state='normal')
            self.ct_eq_status_text.delete('1.0', tk.END)
            self.ct_eq_status_text.insert('1.0', info_text)
            self.ct_eq_status_text.config(state='disabled')

            messagebox.showinfo("Success", "Transfer model loaded for equalization")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to load transfer model:\n{str(e)}")

    def _browse_ct_eq_input_dir(self):
        """Browse for input directory containing slave spectra to transform."""
        directory = filedialog.askdirectory(title="Select Slave Spectra Directory")
        if directory:
            self.ct_eq_input_dir_var.set(directory)

    def _browse_ct_eq_output_dir(self):
        """Browse for output directory for transformed spectra."""
        directory = filedialog.askdirectory(title="Select Output Directory")
        if directory:
            self.ct_eq_output_dir_var.set(directory)

    def _file_equalize_batch(self):
        """Main File Equalize function: Transform slave spectra and export to files."""
        if not HAS_CALIBRATION_TRANSFER:
            messagebox.showerror("Error", "Calibration transfer modules not available")
            return

        # Get transfer model based on source selection
        tm_source = self.ct_eq_tm_source_var.get()
        if tm_source == 'current':
            transfer_model = self.ct_transfer_model
            if transfer_model is None:
                messagebox.showerror("Error",
                    "No transfer model available. Please build a model in Section C or load from file.")
                return
        else:  # 'file'
            transfer_model = getattr(self, 'ct_eq_loaded_transfer_model', None)
            if transfer_model is None:
                messagebox.showerror("Error",
                    "No transfer model loaded. Please load a transfer model file.")
                return

        # Get input/output directories
        input_dir = self.ct_eq_input_dir_var.get()
        if not input_dir:
            messagebox.showwarning("Warning", "Please select an input directory containing slave spectra")
            return

        output_dir = self.ct_eq_output_dir_var.get()
        if not output_dir:
            output_dir = input_dir  # Use input directory if output not specified

        # Get output options
        file_org = self.ct_eq_file_org_var.get()
        output_format = self.ct_eq_format_var.get()

        try:
            from spectral_predict.calibration_transfer import apply_ds, apply_pds, resample_to_grid
            from spectral_predict.io import write_spectra
            from pathlib import Path

            # Update status
            self.ct_eq_status_text.config(state='normal')
            self.ct_eq_status_text.delete('1.0', tk.END)
            self.ct_eq_status_text.insert('1.0', "Loading slave spectra...\n")
            self.ct_eq_status_text.config(state='disabled')
            self.root.update()

            # Load slave spectra
            wavelengths_slave, X_slave = self._load_spectra_from_directory(input_dir)

            # Get sample IDs from filenames in the directory
            input_path = Path(input_dir)
            sample_files = []
            for ext in ['.asd', '.sig', '.csv', '.xlsx', '.spc', '.txt']:
                sample_files.extend(list(input_path.glob(f'*{ext}')))

            sample_ids = [f.stem for f in sample_files]

            if len(sample_ids) != X_slave.shape[0]:
                # Fallback to generic IDs if mismatch
                sample_ids = [f"spectrum_{i+1:03d}" for i in range(X_slave.shape[0])]

            # Detect input format for auto mode
            detected_format = 'csv'  # default fallback
            if sample_files:
                first_ext = sample_files[0].suffix.lower()
                format_map = {'.csv': 'csv', '.xlsx': 'excel', '.spc': 'spc',
                             '.txt': 'ascii', '.jdx': 'jcamp', '.dx': 'jcamp'}
                detected_format = format_map.get(first_ext, 'csv')

            # Determine actual output format
            if output_format == 'auto (match input)' or output_format == 'auto':
                actual_format = detected_format
            else:
                actual_format = output_format

            # Update status
            self.ct_eq_status_text.config(state='normal')
            self.ct_eq_status_text.insert(tk.END, f"Loaded {X_slave.shape[0]} spectra\n")
            self.ct_eq_status_text.insert(tk.END, "Resampling to transfer model grid...\n")
            self.ct_eq_status_text.config(state='disabled')
            self.root.update()

            # Resample to transfer model's common grid
            X_slave_resampled = resample_to_grid(X_slave, wavelengths_slave,
                                                  transfer_model.wavelengths_common)

            # Apply transfer transformation
            self.ct_eq_status_text.config(state='normal')
            self.ct_eq_status_text.insert(tk.END, f"Applying {transfer_model.method.upper()} transformation...\n")
            self.ct_eq_status_text.config(state='disabled')
            self.root.update()

            if transfer_model.method == 'ds':
                X_transformed = apply_ds(X_slave_resampled, transfer_model.params['A'])
            else:  # pds
                X_transformed = apply_pds(X_slave_resampled,
                                         transfer_model.params['B'],
                                         transfer_model.params['window'])

            # Create model name for filename prefix
            model_name = f"{transfer_model.master_id}_from_{transfer_model.slave_id}_{transfer_model.method}"

            # Create DataFrame with transformed spectra
            transformed_ids = [f"{model_name}_{sid}" for sid in sample_ids]
            df_transformed = pd.DataFrame(X_transformed,
                                         index=transformed_ids,
                                         columns=transfer_model.wavelengths_common)

            # Export based on file organization choice
            output_path = Path(output_dir)

            self.ct_eq_status_text.config(state='normal')
            self.ct_eq_status_text.insert(tk.END, f"Exporting to {actual_format.upper()} format...\n")
            self.ct_eq_status_text.config(state='disabled')
            self.root.update()

            if file_org == 'single':
                # Export all spectra to single file
                # Get proper extension for the format
                extension_map = {'csv': '.csv', 'excel': '.xlsx', 'spc': '.spc',
                               'jcamp': '.jdx', 'ascii': '.txt'}
                extension = extension_map.get(actual_format, '.csv')
                output_file = output_path / f"{model_name}_transformed_spectra{extension}"
                write_spectra(df_transformed, str(output_file), format=actual_format)

                self.ct_eq_status_text.config(state='normal')
                self.ct_eq_status_text.insert(tk.END,
                    f"\nSuccess! Exported {len(sample_ids)} transformed spectra to:\n{output_file}\n")
                self.ct_eq_status_text.config(state='disabled')

                messagebox.showinfo("Success",
                    f"Exported {len(sample_ids)} transformed spectra to single file:\n{output_file}")

            else:  # 'individual'
                # Export each spectrum to individual file
                # Get proper extension for the format
                extension_map = {'csv': '.csv', 'excel': '.xlsx', 'spc': '.spc',
                               'jcamp': '.jdx', 'ascii': '.txt'}
                extension = extension_map.get(actual_format, '.csv')
                files_created = []
                for i, (sample_id, transformed_id) in enumerate(zip(sample_ids, transformed_ids)):
                    # Create single-row DataFrame for this spectrum
                    df_single = pd.DataFrame(X_transformed[i:i+1, :],
                                           index=[transformed_id],
                                           columns=transfer_model.wavelengths_common)

                    output_file = output_path / f"{transformed_id}{extension}"
                    write_spectra(df_single, str(output_file), format=actual_format)
                    files_created.append(f"{transformed_id}{extension}")

                self.ct_eq_status_text.config(state='normal')
                self.ct_eq_status_text.insert(tk.END,
                    f"\nSuccess! Exported {len(files_created)} individual files to:\n{output_path}\n")
                self.ct_eq_status_text.insert(tk.END,
                    f"First few files:\n  " + "\n  ".join(files_created[:5]))
                if len(files_created) > 5:
                    self.ct_eq_status_text.insert(tk.END, f"\n  ... and {len(files_created)-5} more")
                self.ct_eq_status_text.insert(tk.END, "\n")
                self.ct_eq_status_text.config(state='disabled')

                messagebox.showinfo("Success",
                    f"Exported {len(files_created)} transformed spectra as individual files to:\n{output_path}")

        except Exception as e:
            self.ct_eq_status_text.config(state='normal')
            self.ct_eq_status_text.insert(tk.END, f"\nError: {str(e)}\n")
            self.ct_eq_status_text.config(state='disabled')
            messagebox.showerror("Error", f"Failed to equalize files:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _plot_paired_spectra_preview(self, master_id, slave_id):
        """
        Plot preview comparison of master and slave spectra immediately after loading.

        Shows:
        1. Master spectra overlay (mean ± std)
        2. Slave spectra overlay (mean ± std)
        3. Side-by-side wavelength range comparison
        """
        if not HAS_MATPLOTLIB:
            return

        import matplotlib.pyplot as plt

        # Get data
        wl = self.ct_wavelengths_common
        X_master = self.ct_X_master_common
        X_slave = self.ct_X_slave_common

        # Compute statistics
        master_mean = np.mean(X_master, axis=0)
        master_std = np.std(X_master, axis=0)
        slave_mean = np.mean(X_slave, axis=0)
        slave_std = np.std(X_slave, axis=0)

        # Create figure with 2 subplots
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        # Plot 1: Master spectra
        ax1.plot(wl, master_mean, 'b-', linewidth=2, label='Mean')
        ax1.fill_between(wl, master_mean - master_std, master_mean + master_std,
                        alpha=0.3, color='b', label='±1 Std')
        ax1.set_xlabel('Wavelength (nm)', fontsize=11)
        ax1.set_ylabel(self._get_spectral_ylabel(), fontsize=11)
        ax1.set_title(f'Master: {master_id}\n({X_master.shape[0]} spectra)', fontsize=12, fontweight='bold')
        ax1.legend(loc='best')
        ax1.grid(True, alpha=0.3)

        # Plot 2: Slave spectra
        ax2.plot(wl, slave_mean, 'r-', linewidth=2, label='Mean')
        ax2.fill_between(wl, slave_mean - slave_std, slave_mean + slave_std,
                        alpha=0.3, color='r', label='±1 Std')
        ax2.set_xlabel('Wavelength (nm)', fontsize=11)
        ax2.set_ylabel(self._get_spectral_ylabel(), fontsize=11)
        ax2.set_title(f'Slave: {slave_id}\n({X_slave.shape[0]} spectra)', fontsize=12, fontweight='bold')
        ax2.legend(loc='best')
        ax2.grid(True, alpha=0.3)

        fig.suptitle('Paired Spectra Preview - Before Calibration Transfer', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

        # Create a second figure for wavelength range comparison
        fig2, ax = plt.subplots(figsize=(10, 4))

        # Show overlaid comparison
        for i in range(min(10, X_master.shape[0])):  # Plot first 10 samples
            ax.plot(wl, X_master[i, :], 'b-', alpha=0.3, linewidth=0.5)
            ax.plot(wl, X_slave[i, :], 'r-', alpha=0.3, linewidth=0.5)

        # Add means on top
        ax.plot(wl, master_mean, 'b-', linewidth=2, label=f'Master ({master_id})')
        ax.plot(wl, slave_mean, 'r-', linewidth=2, label=f'Slave ({slave_id})')

        ax.set_xlabel('Wavelength (nm)', fontsize=11)
        ax.set_ylabel(self._get_spectral_ylabel(), fontsize=11)
        ax.set_title('Master vs. Slave Spectra Overlay (First 10 Samples + Means)', fontsize=12, fontweight='bold')
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def _plot_transfer_quality(self, method):
        """Plot transfer quality diagnostics for Section C.

        Shows:
        1. Transfer Quality Plot (3 subplots): Master, Slave before, Slave after
        2. Transfer Scatter Plot: Master vs Transferred with R²
        """
        if not HAS_MATPLOTLIB:
            return

        try:
            # Clear previous plots
            for widget in self.ct_transfer_plot_frame.winfo_children():
                widget.destroy()

            # Apply transfer to get transferred spectra
            if method == 'ds':
                A = self.ct_transfer_model.params['A']
                X_transferred = apply_ds(self.ct_X_slave_common, A)
            elif method == 'pds':
                B = self.ct_transfer_model.params['B']
                window = self.ct_transfer_model.params['window']
                X_transferred = apply_pds(self.ct_X_slave_common, B, window)
            elif method == 'tsr':
                from spectral_predict.calibration_transfer import apply_tsr
                X_transferred = apply_tsr(self.ct_X_slave_common, self.ct_transfer_model.params)
            elif method == 'ctai':
                from spectral_predict.calibration_transfer import apply_ctai
                X_transferred = apply_ctai(self.ct_X_slave_common, self.ct_transfer_model.params)
            elif method == 'jypls-inv':
                from spectral_predict.calibration_transfer import apply_jypls_inv
                X_transferred = apply_jypls_inv(self.ct_X_slave_common, self.ct_transfer_model.params)
            elif method == 'nspfce':
                from spectral_predict.calibration_transfer import apply_nspfce
                X_transferred = apply_nspfce(self.ct_X_slave_common, self.ct_transfer_model.params)
            else:
                # Unsupported method for plotting
                return

            # === Create Tabbed Interface for Raw / 1st Deriv / 2nd Deriv ===
            from tkinter import ttk

            # Create notebook for tabs
            derivative_notebook = ttk.Notebook(self.ct_transfer_plot_frame)
            derivative_notebook.pack(fill=tk.BOTH, expand=True, pady=(0, 10))

            # Helper function to compute derivatives
            def compute_derivative(X, wavelengths, deriv_order):
                """Compute derivative using Savitzky-Golay filter."""
                from scipy.signal import savgol_filter
                if deriv_order == 0:
                    return X
                # Use window length of 11 and polynomial order 2 (common for NIR)
                window_length = min(11, len(wavelengths) - 1)
                if window_length % 2 == 0:
                    window_length -= 1  # Must be odd
                if window_length < 5:
                    window_length = 5
                X_deriv = np.apply_along_axis(
                    lambda y: savgol_filter(y, window_length, polyorder=2, deriv=deriv_order),
                    axis=1, arr=X
                )
                return X_deriv

            # Helper function to create 3-subplot comparison figure
            def create_comparison_figure(master_data, slave_data, transferred_data,
                                        ylabel, title_suffix):
                """Create figure with Master, Slave Before, Slave After subplots."""
                fig = Figure(figsize=(12, 4))

                # Subplot 1: Master
                ax1 = fig.add_subplot(131)
                master_mean = np.mean(master_data, axis=0)
                master_std = np.std(master_data, axis=0)
                ax1.plot(self.ct_wavelengths_common, master_mean, 'b-', linewidth=2, label='Mean')
                ax1.fill_between(self.ct_wavelengths_common,
                               master_mean - master_std,
                               master_mean + master_std,
                               alpha=0.3, color='b', label='±1 Std')
                ax1.set_xlabel('Wavelength (nm)', fontsize=10)
                ax1.set_ylabel(ylabel, fontsize=10)
                ax1.set_title(f'Master {title_suffix}', fontsize=11, fontweight='bold')
                ax1.legend(fontsize=8)
                ax1.grid(True, alpha=0.3)

                # Subplot 2: Slave before transfer
                ax2 = fig.add_subplot(132)
                slave_mean = np.mean(slave_data, axis=0)
                slave_std = np.std(slave_data, axis=0)
                ax2.plot(self.ct_wavelengths_common, slave_mean, 'r-', linewidth=2, label='Mean')
                ax2.fill_between(self.ct_wavelengths_common,
                               slave_mean - slave_std,
                               slave_mean + slave_std,
                               alpha=0.3, color='r', label='±1 Std')
                ax2.set_xlabel('Wavelength (nm)', fontsize=10)
                ax2.set_ylabel(ylabel, fontsize=10)
                ax2.set_title(f'Slave Before {title_suffix}', fontsize=11, fontweight='bold')
                ax2.legend(fontsize=8)
                ax2.grid(True, alpha=0.3)

                # Subplot 3: Slave after transfer
                ax3 = fig.add_subplot(133)
                trans_mean = np.mean(transferred_data, axis=0)
                trans_std = np.std(transferred_data, axis=0)
                ax3.plot(self.ct_wavelengths_common, trans_mean, 'g-', linewidth=2, label='Mean')
                ax3.fill_between(self.ct_wavelengths_common,
                               trans_mean - trans_std,
                               trans_mean + trans_std,
                               alpha=0.3, color='g', label='±1 Std')
                ax3.set_xlabel('Wavelength (nm)', fontsize=10)
                ax3.set_ylabel(ylabel, fontsize=10)
                ax3.set_title(f'Slave After {title_suffix}', fontsize=11, fontweight='bold')
                ax3.legend(fontsize=8)
                ax3.grid(True, alpha=0.3)

                fig.tight_layout()
                return fig

            # Compute all derivatives
            master_d1 = compute_derivative(self.ct_X_master_common, self.ct_wavelengths_common, 1)
            slave_d1 = compute_derivative(self.ct_X_slave_common, self.ct_wavelengths_common, 1)
            transferred_d1 = compute_derivative(X_transferred, self.ct_wavelengths_common, 1)

            master_d2 = compute_derivative(self.ct_X_master_common, self.ct_wavelengths_common, 2)
            slave_d2 = compute_derivative(self.ct_X_slave_common, self.ct_wavelengths_common, 2)
            transferred_d2 = compute_derivative(X_transferred, self.ct_wavelengths_common, 2)

            # Tab 1: Raw spectra
            tab_raw = ttk.Frame(derivative_notebook)
            derivative_notebook.add(tab_raw, text='Raw Spectra')

            fig_raw = create_comparison_figure(
                self.ct_X_master_common, self.ct_X_slave_common, X_transferred,
                self._get_spectral_ylabel(), 'Spectra'
            )

            canvas_raw = FigureCanvasTkAgg(fig_raw, tab_raw)
            canvas_raw.draw()
            canvas_raw.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            self._add_plot_export_button(tab_raw, fig_raw, "transfer_quality_raw")

            # Tab 2: 1st derivative
            tab_d1 = ttk.Frame(derivative_notebook)
            derivative_notebook.add(tab_d1, text='1st Derivative')

            fig_d1 = create_comparison_figure(
                master_d1, slave_d1, transferred_d1,
                '1st Derivative', '(1st Derivative)'
            )

            canvas_d1 = FigureCanvasTkAgg(fig_d1, tab_d1)
            canvas_d1.draw()
            canvas_d1.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            self._add_plot_export_button(tab_d1, fig_d1, "transfer_quality_1st_deriv")

            # Tab 3: 2nd derivative
            tab_d2 = ttk.Frame(derivative_notebook)
            derivative_notebook.add(tab_d2, text='2nd Derivative')

            fig_d2 = create_comparison_figure(
                master_d2, slave_d2, transferred_d2,
                '2nd Derivative', '(2nd Derivative)'
            )

            canvas_d2 = FigureCanvasTkAgg(fig_d2, tab_d2)
            canvas_d2.draw()
            canvas_d2.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            self._add_plot_export_button(tab_d2, fig_d2, "transfer_quality_2nd_deriv")

            # === Plot 2: Transfer Scatter Plot ===
            fig2 = Figure(figsize=(7, 6))
            ax = fig2.add_subplot(111)

            # Flatten arrays for scatter plot
            master_flat = self.ct_X_master_common.ravel()
            transferred_flat = X_transferred.ravel()

            # Calculate R²
            from sklearn.metrics import r2_score
            r2 = r2_score(master_flat, transferred_flat)

            # Scatter plot with alpha for density
            ax.scatter(master_flat, transferred_flat, alpha=0.3, s=10, edgecolors='none')

            # Add 1:1 line
            min_val = min(master_flat.min(), transferred_flat.min())
            max_val = max(master_flat.max(), transferred_flat.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='1:1 Line')

            ax.set_xlabel('Master Spectra Values', fontsize=11)
            ax.set_ylabel('Transferred Slave Values', fontsize=11)
            ax.set_title(f'Transfer Quality Scatter Plot (R² = {r2:.4f})',
                        fontsize=12, fontweight='bold')
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)

            fig2.tight_layout()

            # Embed plot 2
            canvas2 = FigureCanvasTkAgg(fig2, self.ct_transfer_plot_frame)
            canvas2.draw()
            canvas2.get_tk_widget().pack(fill=tk.BOTH, expand=True, pady=(10, 0))

            # Add export button for plot 2
            self._add_plot_export_button(self.ct_transfer_plot_frame, fig2, "transfer_scatter")

        except Exception as e:
            print(f"Error creating transfer quality plots: {str(e)}")

    def _plot_equalization_quality(self, instruments_data, equalized_data, common_grid):
        """Plot equalization quality diagnostics for Section D.

        Shows:
        1. Multi-Instrument Overlay Plot (2 subplots): Before and After
        2. Wavelength Grid Comparison

        Parameters
        ----------
        instruments_data : dict
            Dictionary of {instrument_id: (wavelengths, X)} before equalization
        equalized_data : dict
            Dictionary of {instrument_id: X_equalized} after equalization
        common_grid : array
            Common wavelength grid
        """
        if not HAS_MATPLOTLIB:
            return

        try:
            # Clear previous plots
            for widget in self.ct_equalize_plot_frame.winfo_children():
                widget.destroy()

            # === Plot 1: Multi-Instrument Overlay (2 subplots) ===
            fig1 = Figure(figsize=(12, 5))

            # Subplot 1: Before equalization
            ax1 = fig1.add_subplot(121)
            colors = plt.cm.tab10(np.linspace(0, 1, len(instruments_data)))
            for idx, (inst_id, (wl, X)) in enumerate(instruments_data.items()):
                # Plot mean spectrum for each instrument
                mean_spectrum = np.mean(X, axis=0)
                ax1.plot(wl, mean_spectrum, linewidth=2, label=inst_id, color=colors[idx])

            ax1.set_xlabel('Wavelength (nm)', fontsize=10)
            ax1.set_ylabel(self._get_spectral_ylabel(), fontsize=10)
            ax1.set_title('Before Equalization\n(Different Wavelength Grids)',
                         fontsize=11, fontweight='bold')
            ax1.legend(fontsize=8, loc='best')
            ax1.grid(True, alpha=0.3)

            # Subplot 2: After equalization
            ax2 = fig1.add_subplot(122)
            for idx, (inst_id, X_eq) in enumerate(equalized_data.items()):
                mean_spectrum = np.mean(X_eq, axis=0)
                ax2.plot(common_grid, mean_spectrum, linewidth=2, label=inst_id, color=colors[idx])

            ax2.set_xlabel('Wavelength (nm)', fontsize=10)
            ax2.set_ylabel(self._get_spectral_ylabel(), fontsize=10)
            ax2.set_title('After Equalization\n(Common Wavelength Grid)',
                         fontsize=11, fontweight='bold')
            ax2.legend(fontsize=8, loc='best')
            ax2.grid(True, alpha=0.3)

            fig1.tight_layout()

            # Embed plot 1
            canvas1 = FigureCanvasTkAgg(fig1, self.ct_equalize_plot_frame)
            canvas1.draw()
            canvas1.get_tk_widget().pack(fill=tk.BOTH, expand=True, pady=(0, 10))

            # Add export button
            self._add_plot_export_button(self.ct_equalize_plot_frame, fig1, "equalization_overlay")

            # === Plot 2: Wavelength Grid Comparison ===
            fig2 = Figure(figsize=(10, 4))
            ax = fig2.add_subplot(111)

            # Prepare data for bar chart
            labels = []
            min_wls = []
            max_wls = []

            for inst_id, (wl, _) in instruments_data.items():
                labels.append(inst_id)
                min_wls.append(wl.min())
                max_wls.append(wl.max())

            # Add common grid
            labels.append('Common Grid')
            min_wls.append(common_grid.min())
            max_wls.append(common_grid.max())

            # Create bar chart
            y_pos = np.arange(len(labels))
            widths = np.array(max_wls) - np.array(min_wls)

            bars = ax.barh(y_pos, widths, left=min_wls, height=0.6)

            # Highlight common grid
            bars[-1].set_color('red')
            bars[-1].set_alpha(0.7)

            ax.set_yticks(y_pos)
            ax.set_yticklabels(labels, fontsize=10)
            ax.set_xlabel('Wavelength (nm)', fontsize=11)
            ax.set_title('Wavelength Range Comparison', fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3, axis='x')

            # Add wavelength range annotations
            for i, (label, min_wl, max_wl) in enumerate(zip(labels, min_wls, max_wls)):
                ax.text(min_wl + (max_wl - min_wl)/2, i,
                       f'{min_wl:.0f}-{max_wl:.0f} nm',
                       ha='center', va='center', fontsize=8, fontweight='bold',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))

            fig2.tight_layout()

            # Embed plot 2
            canvas2 = FigureCanvasTkAgg(fig2, self.ct_equalize_plot_frame)
            canvas2.draw()
            canvas2.get_tk_widget().pack(fill=tk.BOTH, expand=True, pady=(10, 0))

            # Add export button
            self._add_plot_export_button(self.ct_equalize_plot_frame, fig2, "wavelength_grid_comparison")

        except Exception as e:
            print(f"Error creating equalization plots: {str(e)}")

    def _plot_ct_predictions(self, y_pred):
        """Plot prediction results for Section E.

        Shows:
        1. Prediction Distribution Histogram
        2. Prediction Results Plot (scatter with line)

        Parameters
        ----------
        y_pred : array
            Predicted values
        """
        if not HAS_MATPLOTLIB:
            return

        try:
            # Clear previous plots
            for widget in self.ct_prediction_plot_frame.winfo_children():
                widget.destroy()

            # Calculate statistics
            mean_pred = np.mean(y_pred)
            std_pred = np.std(y_pred)

            # === Plot 1: Prediction Distribution Histogram ===
            fig1 = Figure(figsize=(8, 5))
            ax = fig1.add_subplot(111)

            # Histogram
            n, bins, patches = ax.hist(y_pred, bins=20, alpha=0.7, color='steelblue',
                                      edgecolor='black', linewidth=1.2)

            # Add mean line
            ax.axvline(mean_pred, color='red', linestyle='--', linewidth=2,
                      label=f'Mean = {mean_pred:.3f}')

            # Add ±1 std lines
            ax.axvline(mean_pred - std_pred, color='orange', linestyle=':', linewidth=2,
                      label=f'Mean ± Std')
            ax.axvline(mean_pred + std_pred, color='orange', linestyle=':', linewidth=2)

            ax.set_xlabel('Predicted Value', fontsize=11)
            ax.set_ylabel('Frequency', fontsize=11)
            ax.set_title(f'Prediction Distribution\n(Mean={mean_pred:.3f}, Std={std_pred:.3f})',
                        fontsize=12, fontweight='bold')
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3, axis='y')

            fig1.tight_layout()

            # Embed plot 1
            canvas1 = FigureCanvasTkAgg(fig1, self.ct_prediction_plot_frame)
            canvas1.draw()
            canvas1.get_tk_widget().pack(fill=tk.BOTH, expand=True, pady=(0, 10))

            # Add export button
            self._add_plot_export_button(self.ct_prediction_plot_frame, fig1, "prediction_distribution")

            # === Plot 2: Prediction Results Plot ===
            fig2 = Figure(figsize=(10, 5))
            ax = fig2.add_subplot(111)

            # Sample indices
            sample_indices = np.arange(len(y_pred))

            # Scatter plot
            ax.scatter(sample_indices, y_pred, alpha=0.6, s=50,
                      edgecolors='black', linewidths=0.5, c='steelblue', label='Predictions')

            # Connecting line
            ax.plot(sample_indices, y_pred, 'b-', alpha=0.3, linewidth=1)

            # Mean line
            ax.axhline(mean_pred, color='red', linestyle='--', linewidth=2,
                      label=f'Mean = {mean_pred:.3f}')

            ax.set_xlabel('Sample Index', fontsize=11)
            ax.set_ylabel('Predicted Value', fontsize=11)
            ax.set_title('Calibration Transfer Predictions', fontsize=12, fontweight='bold')
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)

            fig2.tight_layout()

            # Embed plot 2
            canvas2 = FigureCanvasTkAgg(fig2, self.ct_prediction_plot_frame)
            canvas2.draw()
            canvas2.get_tk_widget().pack(fill=tk.BOTH, expand=True, pady=(10, 0))

            # Add export button
            self._add_plot_export_button(self.ct_prediction_plot_frame, fig2, "prediction_results")

        except Exception as e:
            print(f"Error creating prediction plots: {str(e)}")

    def _load_spectra_from_directory(self, directory):
        """Helper method to load spectra from a directory. Returns (wavelengths, X)."""
        import glob

        # Try to detect file type
        asd_files = sorted(glob.glob(os.path.join(directory, "*.asd")))
        csv_files = sorted(glob.glob(os.path.join(directory, "*.csv")))
        spc_files = sorted(glob.glob(os.path.join(directory, "*.spc")))

        if asd_files:
            # Load ASD files
            from spectral_predict.io import read_asd_dir
            df, metadata = read_asd_dir(directory)
            wavelengths = df.columns.astype(float).values
            X = df.values
            return wavelengths, X
        elif csv_files:
            # Load CSV files (assume first row is wavelengths, each subsequent row is a spectrum)
            all_spectra = []
            wavelengths = None
            for csv_file in sorted(csv_files):
                df = pd.read_csv(csv_file, header=None)
                if wavelengths is None:
                    wavelengths = df.iloc[0, :].values.astype(float)
                spectrum = df.iloc[1, :].values.astype(float)
                all_spectra.append(spectrum)
            X = np.array(all_spectra)
            return wavelengths, X
        elif spc_files:
            # Load SPC files
            from spectral_predict.io import read_spc_dir
            df, metadata = read_spc_dir(directory)
            wavelengths = df.columns.astype(float).values
            X = df.values
            return wavelengths, X
        else:
            raise ValueError("No supported spectral files found in directory (ASD, CSV, or SPC)")

    # ========================================================================
    # STEP 1 HELPER METHODS FOR NEW CALIBRATION TRANSFER WIZARD
    # ========================================================================

    def _on_step1_mode_changed(self):
        """Handle radio button change between Load Existing and Build New."""
        mode = self.ct_step1_mode_var.get()

        if mode == 'load':
            # Show load frame, hide build frame
            self.ct_load_existing_frame.pack(fill='x', pady=(0, 15), before=self.ct_build_new_frame)
            self.ct_build_new_frame.pack_forget()
        else:
            # Show build frame, hide load frame
            self.ct_load_existing_frame.pack_forget()
            self.ct_build_new_frame.pack(fill='x', pady=(0, 15))

    def _browse_existing_transfer_model(self):
        """Browse for existing .pkl transfer model file."""
        from tkinter import filedialog
        filepath = filedialog.askopenfilename(
            title="Select Transfer Model File",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if filepath:
            self.ct_load_model_path_var.set(filepath)

    def _load_existing_transfer_model(self):
        """Load existing transfer model from .pkl file."""
        import pickle
        from tkinter import messagebox

        filepath = self.ct_load_model_path_var.get()
        if not filepath:
            messagebox.showwarning("No File", "Please browse and select a transfer model file first.")
            return

        try:
            with open(filepath, 'rb') as f:
                transfer_model_data = pickle.load(f)

            # Extract components
            self.current_transfer_model = transfer_model_data.get('model')
            method = transfer_model_data.get('method', 'Unknown')
            master_id = transfer_model_data.get('master_id', 'N/A')
            slave_id = transfer_model_data.get('slave_id', 'N/A')
            date_created = transfer_model_data.get('date_created', 'N/A')
            wavelengths = transfer_model_data.get('wavelengths_common', None)
            n_samples = transfer_model_data.get('n_samples', 'N/A')

            # Display model info
            self._display_transfer_model_info(
                method, master_id, slave_id, date_created, n_samples, wavelengths
            )

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load transfer model:\n{str(e)}")

    def _display_transfer_model_info(self, method, master_id, slave_id, date_created, n_samples, wavelengths):
        """Display transfer model information in text widget."""
        info_text = f"Method: {method}\n"
        info_text += f"Master ID: {master_id}\n"
        info_text += f"Slave ID: {slave_id}\n"
        info_text += f"Date Created: {date_created}\n"
        info_text += f"Training Samples: {n_samples}\n"

        if wavelengths is not None:
            info_text += f"Wavelength Range: {wavelengths[0]:.1f} - {wavelengths[-1]:.1f} nm ({len(wavelengths)} points)\n"

        # Update the text widget
        self.ct_loaded_model_info_text.config(state='normal')
        self.ct_loaded_model_info_text.delete('1.0', tk.END)
        self.ct_loaded_model_info_text.insert('1.0', info_text)
        self.ct_loaded_model_info_text.config(state='disabled')

    def _browse_master_spectra(self):
        """Browse for master spectra folder or file."""
        from tkinter import filedialog
        path = filedialog.askdirectory(title="Select Master Spectra Directory")
        if not path:
            # Try file selection
            path = filedialog.askopenfilename(
                title="Select Master Spectra File",
                filetypes=[("CSV files", "*.csv"), ("NumPy files", "*.npy"), ("All files", "*.*")]
            )
        if path:
            self.ct_master_data_path_var.set(path)

    def _browse_slave_spectra(self):
        """Browse for slave spectra folder or file."""
        from tkinter import filedialog
        path = filedialog.askdirectory(title="Select Slave Spectra Directory")
        if not path:
            # Try file selection
            path = filedialog.askopenfilename(
                title="Select Slave Spectra File",
                filetypes=[("CSV files", "*.csv"), ("NumPy files", "*.npy"), ("All files", "*.*")]
            )
        if path:
            self.ct_slave_data_path_var.set(path)

    def _detect_data_format(self, filepath):
        """Detect data format for export matching.

        Returns: 'csv', 'npy', 'folder', 'asd_folder', 'spc_folder'
        """
        if os.path.isdir(filepath):
            # Check folder contents
            import glob
            asd_files = glob.glob(os.path.join(filepath, "*.asd"))
            spc_files = glob.glob(os.path.join(filepath, "*.spc"))
            csv_files = glob.glob(os.path.join(filepath, "*.csv"))

            if asd_files:
                return 'asd_folder'
            elif spc_files:
                return 'spc_folder'
            elif csv_files:
                return 'csv_folder'
            else:
                return 'folder'
        elif filepath.endswith('.csv'):
            return 'csv'
        elif filepath.endswith('.npy'):
            return 'npy'
        else:
            return 'unknown'

    def _load_master_spectra(self):
        """Load master spectra from file or folder."""
        from tkinter import messagebox

        path = self.ct_master_data_path_var.get()
        if not path:
            messagebox.showwarning("No Path", "Please browse and select master spectra first.")
            return

        try:
            # Detect format
            self.master_data_format = self._detect_data_format(path)

            # Load data
            if os.path.isdir(path):
                wavelengths, X = self._load_spectra_from_directory(path)
            elif path.endswith('.npy'):
                # Load numpy array (assume shape is (n_samples, n_wavelengths))
                X = np.load(path)
                wavelengths = np.arange(X.shape[1])  # Placeholder wavelengths
                messagebox.showinfo("Note", "NPY file loaded. Using placeholder wavelengths (0, 1, 2, ...). "
                                          "Ensure slave data uses same wavelength grid.")
            elif path.endswith('.csv'):
                # Load CSV (assume first row is wavelengths, rest are spectra)
                df = pd.read_csv(path, header=None)
                wavelengths = df.iloc[0, :].values.astype(float)
                X = df.iloc[1:, :].values.astype(float)
            else:
                raise ValueError(f"Unsupported file format: {path}")

            # Store data
            self.current_master_data = (wavelengths, X)

            # Update info display
            self._update_data_info()

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load master spectra:\n{str(e)}")

    def _load_slave_spectra(self):
        """Load slave spectra from file or folder."""
        from tkinter import messagebox

        path = self.ct_slave_data_path_var.get()
        if not path:
            messagebox.showwarning("No Path", "Please browse and select slave spectra first.")
            return

        try:
            # Detect format
            self.slave_data_format = self._detect_data_format(path)

            # Load data
            if os.path.isdir(path):
                wavelengths, X = self._load_spectra_from_directory(path)
            elif path.endswith('.npy'):
                X = np.load(path)
                wavelengths = np.arange(X.shape[1])
                messagebox.showinfo("Note", "NPY file loaded. Using placeholder wavelengths (0, 1, 2, ...). "
                                          "Ensure master data uses same wavelength grid.")
            elif path.endswith('.csv'):
                df = pd.read_csv(path, header=None)
                wavelengths = df.iloc[0, :].values.astype(float)
                X = df.iloc[1:, :].values.astype(float)
            else:
                raise ValueError(f"Unsupported file format: {path}")

            # Store data
            self.current_slave_data = (wavelengths, X)

            # Update info display
            self._update_data_info()

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load slave spectra:\n{str(e)}")

    def _update_data_info(self):
        """Update data information display."""
        info_text = ""

        # Check enhanced loading first (with Y values)
        if self.ct_master_X is not None:
            info_text += f"Master (with Y values):\n"
            info_text += f"  Samples: {len(self.ct_master_X)}\n"
            info_text += f"  Wavelengths: {len(self.ct_master_wavelengths)} "
            info_text += f"({self.ct_master_wavelengths[0]:.1f} - {self.ct_master_wavelengths[-1]:.1f} nm)\n"
            if hasattr(self, 'ct_master_target_col_var') and self.ct_master_target_col_var.get():
                info_text += f"  Target: {self.ct_master_target_col_var.get()}\n"
            info_text += "\n"
        elif self.current_master_data is not None:
            # Fall back to simple loading
            wl_m, X_m = self.current_master_data
            info_text += f"Master: {X_m.shape[0]} samples, {len(wl_m)} wavelengths "
            info_text += f"({wl_m[0]:.1f} - {wl_m[-1]:.1f} nm)\n"
            info_text += f"Format: {self.master_data_format}\n"
            info_text += "⚠ No Y values loaded (simple loading)\n\n"

        if self.ct_slave_X is not None:
            info_text += f"Slave (with Y values):\n"
            info_text += f"  Samples: {len(self.ct_slave_X)}\n"
            info_text += f"  Wavelengths: {len(self.ct_slave_wavelengths)} "
            info_text += f"({self.ct_slave_wavelengths[0]:.1f} - {self.ct_slave_wavelengths[-1]:.1f} nm)\n"
            if hasattr(self, 'ct_slave_target_col_var') and self.ct_slave_target_col_var.get():
                info_text += f"  Target: {self.ct_slave_target_col_var.get()}\n"
            info_text += "\n"
        elif self.current_slave_data is not None:
            wl_s, X_s = self.current_slave_data
            info_text += f"Slave: {X_s.shape[0]} samples, {len(wl_s)} wavelengths "
            info_text += f"({wl_s[0]:.1f} - {wl_s[-1]:.1f} nm)\n"
            info_text += f"Format: {self.slave_data_format}\n"
            info_text += "⚠ No Y values loaded (simple loading)\n\n"

        # Check for sample matching (if both enhanced loading used)
        if self.ct_master_X is not None and self.ct_slave_X is not None:
            master_ids = set(self.ct_master_X.index)
            slave_ids = set(self.ct_slave_X.index)
            common_ids = master_ids & slave_ids

            info_text += f"Sample Matching:\n"
            info_text += f"  Matched samples: {len(common_ids)}\n"
            if len(common_ids) < len(master_ids):
                info_text += f"  Only in master: {len(master_ids) - len(common_ids)}\n"
            if len(common_ids) < len(slave_ids):
                info_text += f"  Only in slave: {len(slave_ids) - len(common_ids)}\n"

            if len(common_ids) == len(master_ids) == len(slave_ids):
                info_text += f"  ✓ Perfect match!\n"

        # Calculate wavelength overlap (works for both loading types)
        wl_m = None
        wl_s = None
        if self.ct_master_X is not None:
            wl_m = self.ct_master_wavelengths
        elif self.current_master_data is not None:
            wl_m, _ = self.current_master_data

        if self.ct_slave_X is not None:
            wl_s = self.ct_slave_wavelengths
        elif self.current_slave_data is not None:
            wl_s, _ = self.current_slave_data

        if wl_m is not None and wl_s is not None:
            overlap_start = max(wl_m[0], wl_s[0])
            overlap_end = min(wl_m[-1], wl_s[-1])
            overlap_pct = 100.0 * (overlap_end - overlap_start) / (max(wl_m[-1], wl_s[-1]) - min(wl_m[0], wl_s[0]))
            info_text += f"\nWavelength Overlap: {overlap_start:.1f} - {overlap_end:.1f} nm ({overlap_pct:.1f}%)"

        self.ct_data_info_text.config(state='normal')
        self.ct_data_info_text.delete('1.0', tk.END)
        self.ct_data_info_text.insert('1.0', info_text)
        self.ct_data_info_text.config(state='disabled')

    def _preview_spectra(self):
        """Preview master vs slave spectra."""
        from tkinter import messagebox

        if self.current_master_data is None or self.current_slave_data is None:
            messagebox.showwarning("No Data", "Please load both master and slave spectra first.")
            return

        if not HAS_MATPLOTLIB:
            messagebox.showwarning("No Matplotlib", "Matplotlib not available for plotting.")
            return

        try:
            import matplotlib.pyplot as plt

            wl_m, X_m = self.current_master_data
            wl_s, X_s = self.current_slave_data

            # Create figure with 2 subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

            # Plot master
            master_mean = np.mean(X_m, axis=0)
            master_std = np.std(X_m, axis=0)
            ax1.plot(wl_m, master_mean, 'b-', linewidth=2, label='Mean')
            ax1.fill_between(wl_m, master_mean - master_std, master_mean + master_std,
                           alpha=0.3, color='b', label='±1 Std')
            ax1.set_xlabel('Wavelength (nm)', fontsize=11)
            ax1.set_ylabel('Intensity', fontsize=11)
            ax1.set_title(f'Master Spectra\n({X_m.shape[0]} samples)', fontsize=12, fontweight='bold')
            ax1.legend(loc='best')
            ax1.grid(True, alpha=0.3)

            # Plot slave
            slave_mean = np.mean(X_s, axis=0)
            slave_std = np.std(X_s, axis=0)
            ax2.plot(wl_s, slave_mean, 'r-', linewidth=2, label='Mean')
            ax2.fill_between(wl_s, slave_mean - slave_std, slave_mean + slave_std,
                           alpha=0.3, color='r', label='±1 Std')
            ax2.set_xlabel('Wavelength (nm)', fontsize=11)
            ax2.set_ylabel('Intensity', fontsize=11)
            ax2.set_title(f'Slave Spectra\n({X_s.shape[0]} samples)', fontsize=12, fontweight='bold')
            ax2.legend(loc='best')
            ax2.grid(True, alpha=0.3)

            fig.suptitle('Master vs Slave Spectra Preview', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.show()

        except Exception as e:
            messagebox.showerror("Error", f"Failed to create preview plot:\n{str(e)}")

    def _build_transfer_model_new(self):
        """Build new transfer model from loaded master/slave data."""
        from tkinter import messagebox

        try:
            from spectral_predict.calibration_transfer import (
                estimate_ds, estimate_pds, estimate_tsr, estimate_ctai, estimate_nspfce, estimate_jypls_inv,
                TransferModel
            )
        except ImportError as e:
            messagebox.showerror("Import Error", f"Failed to import calibration transfer functions:\n{str(e)}")
            return

        # Validate data loaded
        if self.current_master_data is None or self.current_slave_data is None:
            messagebox.showwarning("No Data", "Please load both master and slave spectra first.")
            return

        try:
            wl_master, X_master = self.current_master_data
            wl_slave, X_slave = self.current_slave_data

            # Find common wavelength grid (interpolate if needed)
            if np.allclose(wl_master, wl_slave):
                wl_common = wl_master
                X_master_common = X_master
                X_slave_common = X_slave
            else:
                # Interpolate to common grid (use master wavelengths as reference)
                from scipy.interpolate import interp1d
                wl_common = wl_master

                # Interpolate slave to master grid
                X_slave_common = np.zeros_like(X_master)
                for i in range(X_slave.shape[0]):
                    f = interp1d(wl_slave, X_slave[i, :], kind='linear', fill_value='extrapolate')
                    X_slave_common[i, :] = f(wl_common)

                X_master_common = X_master
                messagebox.showinfo("Info", "Slave spectra interpolated to master wavelength grid.")

            # Store common grid data
            self.ct_wavelengths_common = wl_common
            self.ct_X_master_common = X_master_common
            self.ct_X_slave_common = X_slave_common

            # Get method and parameters
            method = self.ct_method_var.get()

            # Build transfer model based on method
            if method == 'ds':
                lambda_val = float(self.ct_ds_lambda_var.get())
                A = estimate_ds(X_master_common, X_slave_common, lam=lambda_val)
                params = {'A': A}

            elif method == 'pds':
                window = int(self.ct_pds_window_var.get())
                B = estimate_pds(X_master_common, X_slave_common, window=window)
                params = {'B': B, 'window': window}

            elif method == 'tsr':
                n_samples = int(self.ct_tsr_n_samples_var.get())
                if n_samples > X_master_common.shape[0]:
                    raise ValueError(f"TSR requires {n_samples} samples, but only {X_master_common.shape[0]} available.")
                # TSR requires transfer_indices: assume all loaded samples are paired
                transfer_indices = np.arange(n_samples)
                params = estimate_tsr(X_master_common[:n_samples], X_slave_common[:n_samples],
                                     transfer_indices)

            elif method == 'ctai':
                params = estimate_ctai(X_master_common, X_slave_common)

            elif method == 'nspfce':
                max_iter = int(self.ct_nspfce_max_iterations_var.get())
                use_wavelength_selection = self.ct_nspfce_use_wavelength_selection_var.get()
                selector_name = self.ct_nspfce_selector_var.get() if use_wavelength_selection else None

                params = estimate_nspfce(X_master_common, X_slave_common, wl_common,
                                        use_wavelength_selection=use_wavelength_selection,
                                        wavelength_selector=selector_name if selector_name else 'vcpa-iriv',
                                        max_iterations=max_iter)

            elif method == 'jypls-inv':
                n_samples = int(self.ct_jypls_n_samples_var.get())
                n_comp_str = self.ct_jypls_n_components_var.get()
                n_components = None if n_comp_str == 'Auto' else int(n_comp_str)

                # Check if enhanced loading was used (with Y values)
                if self.ct_master_X is not None and self.ct_slave_X is not None and \
                   self.ct_master_y is not None and self.ct_slave_y is not None:

                    # Enhanced loading: Use real Y values
                    # First, validate master/slave pairing
                    if not self._validate_master_slave_pairing():
                        return  # Validation failed, user notified

                    # Get common sample IDs
                    master_ids = set(self.ct_master_X.index)
                    slave_ids = set(self.ct_slave_X.index)
                    common_ids = sorted(list(master_ids & slave_ids))

                    if len(common_ids) < n_samples:
                        raise ValueError(f"JYPLS-inv requires {n_samples} transfer samples, "
                                       f"but only {len(common_ids)} matched samples available.")

                    # Filter to common samples only
                    X_master_paired = self.ct_master_X.loc[common_ids].values
                    X_slave_paired = self.ct_slave_X.loc[common_ids].values
                    y_paired = self.ct_master_y.loc[common_ids].values

                    # Select transfer samples using Kennard-Stone
                    from spectral_predict.sample_selection import kennard_stone
                    transfer_indices = kennard_stone(X_master_paired, n_samples=n_samples)

                    # Extract Y values for selected transfer samples (REAL VALUES!)
                    y_transfer = y_paired[transfer_indices]

                    # Use selected samples for building
                    X_master_transfer = X_master_paired[transfer_indices]
                    X_slave_transfer = X_slave_paired[transfer_indices]

                    # Show info about Y values used
                    y_min, y_max = y_transfer.min(), y_transfer.max()
                    y_mean, y_std = y_transfer.mean(), y_transfer.std()
                    messagebox.showinfo("JYPLS-inv Y Values",
                        f"Using real reference Y values:\n\n"
                        f"Transfer samples: {n_samples}\n"
                        f"Y range: {y_min:.3f} - {y_max:.3f}\n"
                        f"Y mean: {y_mean:.3f} ± {y_std:.3f}\n\n"
                        f"Selected using Kennard-Stone algorithm.")

                    params = estimate_jypls_inv(X_master_transfer, X_slave_transfer,
                                               y_transfer, transfer_indices,
                                               n_components=n_components)

                else:
                    # Simple loading: Fall back to placeholder (not recommended)
                    if n_samples > X_master_common.shape[0]:
                        raise ValueError(f"JYPLS-inv requires {n_samples} samples, but only {X_master_common.shape[0]} available.")

                    messagebox.showwarning("JYPLS-inv Limitation",
                        "JYPLS-inv requires reference property values (Y) for transfer samples.\n\n"
                        "Building with placeholder zeros because enhanced loading was not used.\n\n"
                        "For accurate results, use the 'Load Master/Slave Data with Y values' section above.\n"
                        "Otherwise, consider using CTAI or NS-PFCE instead, which don't require reference values.")

                    y_transfer = np.zeros(n_samples)  # Placeholder
                    transfer_indices = np.arange(n_samples)

                    params = estimate_jypls_inv(X_master_common[:n_samples], X_slave_common[:n_samples],
                                               y_transfer, transfer_indices,
                                               n_components=n_components)

            else:
                raise ValueError(f"Unknown method: {method}")

            # Create TransferModel with all required fields
            self.ct_transfer_model = TransferModel(
                master_id='master',
                slave_id='slave',
                method=method,
                wavelengths_common=wl_common,
                params=params,
                meta={}
            )

            # Store in current_transfer_model for consistency
            self.current_transfer_model = self.ct_transfer_model

            # Update info display
            info_text = f"Transfer Model Built Successfully!\n"
            info_text += f"Method: {method.upper()}\n"
            info_text += f"Training Samples: {X_master_common.shape[0]}\n"
            info_text += f"Wavelength Range: {wl_common[0]:.1f} - {wl_common[-1]:.1f} nm ({len(wl_common)} points)\n"

            self.ct_transfer_info_text.config(state='normal')
            self.ct_transfer_info_text.delete('1.0', tk.END)
            self.ct_transfer_info_text.insert('1.0', info_text)
            self.ct_transfer_info_text.config(state='disabled')

            # Enable save button
            self.ct_save_tm_button.config(state='normal')

            # Plot transfer quality (reuse existing method from lines 14939+)
            self._plot_transfer_quality(method)

            # Play success sound (removed popup - info shown in text widget and plot)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to build transfer model:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _save_transfer_model(self):
        """Save transfer model to .pkl file with metadata."""
        import pickle
        from datetime import datetime
        from tkinter import filedialog, messagebox

        if self.ct_transfer_model is None:
            messagebox.showwarning("No Model", "Please build a transfer model first.")
            return

        # Ask for save location
        filepath = filedialog.asksaveasfilename(
            title="Save Transfer Model",
            defaultextension=".pkl",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )

        if not filepath:
            return

        try:
            # Create metadata dictionary
            transfer_model_data = {
                'model': self.ct_transfer_model,
                'method': self.ct_transfer_model.method,
                'master_id': 'master',  # Could be customized with a dialog
                'slave_id': 'slave',    # Could be customized with a dialog
                'date_created': datetime.now().isoformat(),
                'wavelengths_common': self.ct_wavelengths_common,
                'n_samples': self.ct_X_master_common.shape[0],
                'metadata': {
                    'master_shape': self.ct_X_master_common.shape,
                    'slave_shape': self.ct_X_slave_common.shape,
                    'master_format': self.master_data_format,
                    'slave_format': self.slave_data_format
                }
            }

            # Save to file
            with open(filepath, 'wb') as f:
                pickle.dump(transfer_model_data, f)

            # Play success sound (removed popup - file path shown in status)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to save transfer model:\n{str(e)}")

    # ========================================================================
    # END OF STEP 1 HELPER METHODS
    # ========================================================================

    # ========================================================================
    # STEP 1 ENHANCED HELPER METHODS: Load Master/Slave Data with Y Values (for JYPLS-inv)
    # ========================================================================

    def _browse_master_spectra_simple(self):
        """Browse master spectra directory (simple version without Y auto-detection)."""
        from tkinter import filedialog
        from pathlib import Path

        directory = filedialog.askdirectory(title="Select Master Spectra Directory")
        if not directory:
            return

        self.ct_master_spectra_path_var.set(directory)

        # Auto-detect file type (priority order)
        path = Path(directory)
        asd_files = sorted(list(path.glob("*.asd")) + list(path.glob("*.sig")))
        csv_files = sorted(list(path.glob("*.csv")))
        spc_files = sorted(list(path.glob("*.spc")))

        if asd_files:
            self.ct_master_detected_type = "asd"
            self.ct_master_detection_status.config(
                text=f"✓ Detected {len(asd_files)} ASD files",
                foreground=self.colors['success']
            )
        elif spc_files:
            self.ct_master_detected_type = "spc"
            self.ct_master_detection_status.config(
                text=f"✓ Detected {len(spc_files)} SPC files",
                foreground=self.colors['success']
            )
        elif csv_files:
            self.ct_master_detected_type = "csv"
            self.ct_master_detection_status.config(
                text=f"✓ Detected {len(csv_files)} CSV files",
                foreground=self.colors['success']
            )
        else:
            self.ct_master_detected_type = None
            self.ct_master_detection_status.config(
                text="⚠ No spectral files detected",
                foreground=self.colors['warning']
            )

    def _load_master_data_simple(self):
        """Load master spectra without Y values (for non-JYPLS methods)."""
        from tkinter import messagebox
        import pandas as pd
        import numpy as np
        from spectral_predict.io import read_asd_dir, read_csv_spectra, read_spc_dir

        # Validate inputs
        if not self.ct_master_spectra_path_var.get():
            messagebox.showwarning("No Path", "Please browse and select master spectra directory.")
            return

        try:
            # Load spectra based on detected type
            # These functions return (df, metadata) where df is a DataFrame with samples as rows
            if self.ct_master_detected_type == 'asd':
                df, metadata = read_asd_dir(self.ct_master_spectra_path_var.get())
            elif self.ct_master_detected_type == 'spc':
                df, metadata = read_spc_dir(self.ct_master_spectra_path_var.get())
            elif self.ct_master_detected_type == 'csv':
                df, metadata = read_csv_spectra(self.ct_master_spectra_path_var.get())
            else:
                raise ValueError(f"Unsupported file type: {self.ct_master_detected_type}")

            # Store data (without Y values)
            # df is a DataFrame with index=sample names, columns=wavelengths
            self.ct_master_X = df  # Store as DataFrame
            self.ct_master_wavelengths = df.columns.values  # Get wavelengths from column names
            self.master_data_format = self.ct_master_detected_type

            # Update info display
            info_text = f"Master: {self.ct_master_X.shape[0]} samples, {len(self.ct_master_wavelengths)} wavelengths\n"
            self.ct_data_info_text.config(state='normal')
            self.ct_data_info_text.delete('1.0', tk.END)
            self.ct_data_info_text.insert('1.0', info_text)
            self.ct_data_info_text.config(state='disabled')

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load master spectra:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _load_slave_data_simple(self):
        """Load slave spectra without Y values (for non-JYPLS methods)."""
        from tkinter import messagebox
        import pandas as pd
        import numpy as np
        from spectral_predict.io import read_asd_dir, read_csv_spectra, read_spc_dir

        # Validate inputs
        if not self.ct_slave_spectra_path_var.get():
            messagebox.showwarning("No Path", "Please browse and select slave spectra directory.")
            return

        try:
            # Load spectra based on detected type
            # These functions return (df, metadata) where df is a DataFrame with samples as rows
            if self.ct_slave_detected_type == 'asd':
                df, metadata = read_asd_dir(self.ct_slave_spectra_path_var.get())
            elif self.ct_slave_detected_type == 'spc':
                df, metadata = read_spc_dir(self.ct_slave_spectra_path_var.get())
            elif self.ct_slave_detected_type == 'csv':
                df, metadata = read_csv_spectra(self.ct_slave_spectra_path_var.get())
            else:
                raise ValueError(f"Unsupported file type: {self.ct_slave_detected_type}")

            # Store data (without Y values)
            # df is a DataFrame with index=sample names, columns=wavelengths
            self.ct_slave_X = df  # Store as DataFrame
            self.ct_slave_wavelengths = df.columns.values  # Get wavelengths from column names
            self.slave_data_format = self.ct_slave_detected_type

            # Update info display
            master_info = ""
            if hasattr(self, 'ct_master_X') and self.ct_master_X is not None:
                master_info = f"Master: {self.ct_master_X.shape[0]} samples, {len(self.ct_master_wavelengths)} wavelengths\n"

            slave_info = f"Slave: {self.ct_slave_X.shape[0]} samples, {len(self.ct_slave_wavelengths)} wavelengths\n"

            self.ct_data_info_text.config(state='normal')
            self.ct_data_info_text.delete('1.0', tk.END)
            self.ct_data_info_text.insert('1.0', master_info + slave_info)
            self.ct_data_info_text.config(state='disabled')

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load slave spectra:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _browse_master_spectra_with_y(self):
        """Browse master spectra directory with auto-detection (Import tab pattern)."""
        from tkinter import filedialog, messagebox
        from pathlib import Path

        directory = filedialog.askdirectory(title="Select Master Spectra Directory")
        if not directory:
            return

        self.ct_master_spectra_path_var.set(directory)

        # Auto-detect file type (priority order)
        path = Path(directory)
        asd_files = sorted(list(path.glob("*.asd")) + list(path.glob("*.sig")))
        csv_files = sorted(list(path.glob("*.csv")))
        spc_files = sorted(list(path.glob("*.spc")))

        if asd_files:
            self.ct_master_detected_type = "asd"
            self.ct_master_detection_status.config(
                text=f"✓ Detected {len(asd_files)} ASD files",
                foreground=self.colors['success']
            )
        elif spc_files:
            self.ct_master_detected_type = "spc"
            self.ct_master_detection_status.config(
                text=f"✓ Detected {len(spc_files)} SPC files",
                foreground=self.colors['success']
            )
        elif csv_files:
            self.ct_master_detected_type = "csv"
            self.ct_master_detection_status.config(
                text=f"✓ Detected {len(csv_files)} CSV files",
                foreground=self.colors['success']
            )
        else:
            self.ct_master_detected_type = None
            self.ct_master_detection_status.config(
                text="⚠ No spectral files detected",
                foreground=self.colors['warning']
            )
            return

        # Auto-detect reference CSV/Excel in same folder
        ref_files = sorted(list(path.glob("*.csv")) + list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
        # Filter out files that are spectra
        ref_files = [f for f in ref_files if f not in csv_files]

        if len(ref_files) == 1:
            self.ct_master_reference_path_var.set(str(ref_files[0]))
            self._auto_detect_master_columns()
            messagebox.showinfo("Auto-Detected",
                f"Auto-detected reference file:\n{ref_files[0].name}")

    def _browse_master_reference(self):
        """Browse for master reference CSV/Excel file."""
        from tkinter import filedialog

        filepath = filedialog.askopenfilename(
            title="Select Master Reference CSV/Excel",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx;*.xls"), ("All files", "*.*")]
        )
        if filepath:
            self.ct_master_reference_path_var.set(filepath)
            self._auto_detect_master_columns()

    def _auto_detect_master_columns(self):
        """Auto-detect column mapping from master reference file."""
        from tkinter import messagebox
        import pandas as pd

        if not self.ct_master_reference_path_var.get():
            return

        try:
            ref_path = self.ct_master_reference_path_var.get()
            if ref_path.lower().endswith(('.xlsx', '.xls')):
                df = pd.read_excel(ref_path, nrows=5)
            else:
                df = pd.read_csv(ref_path, nrows=5)

            columns = list(df.columns)

            # Update comboboxes
            self.ct_master_spectral_file_combo['values'] = columns
            self.ct_master_id_combo['values'] = columns
            self.ct_master_target_combo['values'] = columns

            # Auto-select if 3+ columns
            if len(columns) >= 3:
                self.ct_master_spectral_file_col_var.set(columns[0])
                self.ct_master_id_col_var.set(columns[1])
                self.ct_master_target_col_var.set(columns[2])
            elif len(columns) >= 2:
                self.ct_master_spectral_file_col_var.set(columns[0])
                self.ct_master_id_col_var.set(columns[0])
                self.ct_master_target_col_var.set(columns[1])

        except Exception as e:
            messagebox.showerror("Error", f"Could not read reference file:\n{str(e)}")

    def _load_master_data_with_y(self):
        """Load master spectra + reference Y values and align."""
        from tkinter import messagebox
        import pandas as pd
        import numpy as np
        from spectral_predict.io import read_asd_dir, read_csv_spectra, read_spc_dir, read_reference_csv, align_xy

        # Validate inputs
        if not self.ct_master_spectra_path_var.get():
            messagebox.showwarning("No Path", "Please browse and select master spectra directory.")
            return

        if not self.ct_master_reference_path_var.get():
            messagebox.showwarning("No Reference", "Please browse and select master reference CSV/Excel.")
            return

        if not self.ct_master_spectral_file_col_var.get():
            messagebox.showwarning("No Column", "Please select spectral file column.")
            return

        if not self.ct_master_target_col_var.get():
            messagebox.showwarning("No Target", "Please select target variable column.")
            return

        try:
            # Load spectra based on detected type
            if self.ct_master_detected_type == 'asd':
                X, metadata = read_asd_dir(self.ct_master_spectra_path_var.get())
            elif self.ct_master_detected_type == 'spc':
                X, metadata = read_spc_dir(self.ct_master_spectra_path_var.get())
            elif self.ct_master_detected_type == 'csv':
                X, metadata = read_csv_spectra(self.ct_master_spectra_path_var.get())
            else:
                raise ValueError(f"Unsupported file type: {self.ct_master_detected_type}")

            # Load reference
            ref = read_reference_csv(
                self.ct_master_reference_path_var.get(),
                self.ct_master_spectral_file_col_var.get()
            )

            # Align by sample ID
            X_aligned, y_aligned, alignment_info = align_xy(
                X, ref,
                self.ct_master_spectral_file_col_var.get(),
                self.ct_master_target_col_var.get(),
                return_alignment_info=True
            )

            # Show alignment report if there are issues
            if alignment_info['unmatched_spectra'] or alignment_info['n_nan_dropped'] > 0:
                msg = f"Alignment Report:\n\n"
                msg += f"Matched samples: {len(alignment_info['matched_ids'])}\n"
                if alignment_info['unmatched_spectra']:
                    msg += f"Unmatched spectra (no ref): {len(alignment_info['unmatched_spectra'])}\n"
                if alignment_info['unmatched_reference']:
                    msg += f"Unmatched references (no spectra): {len(alignment_info['unmatched_reference'])}\n"
                if alignment_info['n_nan_dropped'] > 0:
                    msg += f"Dropped (NaN targets): {alignment_info['n_nan_dropped']}\n"
                if alignment_info['used_fuzzy_matching']:
                    msg += f"\n⚠ Used fuzzy filename matching"
                messagebox.showinfo("Alignment Report", msg)

            # Store as DataFrames (keep index with sample IDs)
            self.ct_master_X = X_aligned
            self.ct_master_y = y_aligned
            self.ct_master_wavelengths = X_aligned.columns.astype(float).values

            # Update data info display
            self._update_data_info()

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load master data:\n{str(e)}")
            import traceback
            traceback.print_exc()

    # Slave data methods (parallel to master)

    def _browse_slave_spectra_simple(self):
        """Browse slave spectra directory (simple version without Y auto-detection)."""
        from tkinter import filedialog
        from pathlib import Path

        directory = filedialog.askdirectory(title="Select Slave Spectra Directory")
        if not directory:
            return

        self.ct_slave_spectra_path_var.set(directory)

        # Auto-detect file type (priority order)
        path = Path(directory)
        asd_files = sorted(list(path.glob("*.asd")) + list(path.glob("*.sig")))
        csv_files = sorted(list(path.glob("*.csv")))
        spc_files = sorted(list(path.glob("*.spc")))

        if asd_files:
            self.ct_slave_detected_type = "asd"
            self.ct_slave_detection_status.config(
                text=f"✓ Detected {len(asd_files)} ASD files",
                foreground=self.colors['success']
            )
        elif spc_files:
            self.ct_slave_detected_type = "spc"
            self.ct_slave_detection_status.config(
                text=f"✓ Detected {len(spc_files)} SPC files",
                foreground=self.colors['success']
            )
        elif csv_files:
            self.ct_slave_detected_type = "csv"
            self.ct_slave_detection_status.config(
                text=f"✓ Detected {len(csv_files)} CSV files",
                foreground=self.colors['success']
            )
        else:
            self.ct_slave_detected_type = None
            self.ct_slave_detection_status.config(
                text="⚠ No spectral files detected",
                foreground=self.colors['warning']
            )

    def _browse_slave_spectra_with_y(self):
        """Browse slave spectra directory with auto-detection (Import tab pattern)."""
        from tkinter import filedialog, messagebox
        from pathlib import Path

        directory = filedialog.askdirectory(title="Select Slave Spectra Directory")
        if not directory:
            return

        self.ct_slave_spectra_path_var.set(directory)

        # Auto-detect file type
        path = Path(directory)
        asd_files = sorted(list(path.glob("*.asd")) + list(path.glob("*.sig")))
        csv_files = sorted(list(path.glob("*.csv")))
        spc_files = sorted(list(path.glob("*.spc")))

        if asd_files:
            self.ct_slave_detected_type = "asd"
            self.ct_slave_detection_status.config(
                text=f"✓ Detected {len(asd_files)} ASD files",
                foreground=self.colors['success']
            )
        elif spc_files:
            self.ct_slave_detected_type = "spc"
            self.ct_slave_detection_status.config(
                text=f"✓ Detected {len(spc_files)} SPC files",
                foreground=self.colors['success']
            )
        elif csv_files:
            self.ct_slave_detected_type = "csv"
            self.ct_slave_detection_status.config(
                text=f"✓ Detected {len(csv_files)} CSV files",
                foreground=self.colors['success']
            )
        else:
            self.ct_slave_detected_type = None
            self.ct_slave_detection_status.config(
                text="⚠ No spectral files detected",
                foreground=self.colors['warning']
            )
            return

        # Auto-detect reference CSV/Excel
        ref_files = sorted(list(path.glob("*.csv")) + list(path.glob("*.xlsx")) + list(path.glob("*.xls")))
        ref_files = [f for f in ref_files if f not in csv_files]

        if len(ref_files) == 1:
            self.ct_slave_reference_path_var.set(str(ref_files[0]))
            self._auto_detect_slave_columns()
            messagebox.showinfo("Auto-Detected",
                f"Auto-detected reference file:\n{ref_files[0].name}")

    def _browse_slave_reference(self):
        """Browse for slave reference CSV/Excel file."""
        from tkinter import filedialog

        filepath = filedialog.askopenfilename(
            title="Select Slave Reference CSV/Excel",
            filetypes=[("CSV files", "*.csv"), ("Excel files", "*.xlsx;*.xls"), ("All files", "*.*")]
        )
        if filepath:
            self.ct_slave_reference_path_var.set(filepath)
            self._auto_detect_slave_columns()

    def _auto_detect_slave_columns(self):
        """Auto-detect column mapping from slave reference file."""
        from tkinter import messagebox
        import pandas as pd

        if not self.ct_slave_reference_path_var.get():
            return

        try:
            ref_path = self.ct_slave_reference_path_var.get()
            if ref_path.lower().endswith(('.xlsx', '.xls')):
                df = pd.read_excel(ref_path, nrows=5)
            else:
                df = pd.read_csv(ref_path, nrows=5)

            columns = list(df.columns)

            # Update comboboxes
            self.ct_slave_spectral_file_combo['values'] = columns
            self.ct_slave_id_combo['values'] = columns
            self.ct_slave_target_combo['values'] = columns

            # Auto-select
            if len(columns) >= 3:
                self.ct_slave_spectral_file_col_var.set(columns[0])
                self.ct_slave_id_col_var.set(columns[1])
                self.ct_slave_target_col_var.set(columns[2])
            elif len(columns) >= 2:
                self.ct_slave_spectral_file_col_var.set(columns[0])
                self.ct_slave_id_col_var.set(columns[0])
                self.ct_slave_target_col_var.set(columns[1])

        except Exception as e:
            messagebox.showerror("Error", f"Could not read reference file:\n{str(e)}")

    def _load_slave_data_with_y(self):
        """Load slave spectra + reference Y values and align."""
        from tkinter import messagebox
        import pandas as pd
        import numpy as np
        from spectral_predict.io import read_asd_dir, read_csv_spectra, read_spc_dir, read_reference_csv, align_xy

        # Validate inputs
        if not self.ct_slave_spectra_path_var.get():
            messagebox.showwarning("No Path", "Please browse and select slave spectra directory.")
            return

        if not self.ct_slave_reference_path_var.get():
            messagebox.showwarning("No Reference", "Please browse and select slave reference CSV/Excel.")
            return

        if not self.ct_slave_spectral_file_col_var.get():
            messagebox.showwarning("No Column", "Please select spectral file column.")
            return

        if not self.ct_slave_target_col_var.get():
            messagebox.showwarning("No Target", "Please select target variable column.")
            return

        try:
            # Load spectra based on detected type
            if self.ct_slave_detected_type == 'asd':
                X, metadata = read_asd_dir(self.ct_slave_spectra_path_var.get())
            elif self.ct_slave_detected_type == 'spc':
                X, metadata = read_spc_dir(self.ct_slave_spectra_path_var.get())
            elif self.ct_slave_detected_type == 'csv':
                X, metadata = read_csv_spectra(self.ct_slave_spectra_path_var.get())
            else:
                raise ValueError(f"Unsupported file type: {self.ct_slave_detected_type}")

            # Load reference
            ref = read_reference_csv(
                self.ct_slave_reference_path_var.get(),
                self.ct_slave_spectral_file_col_var.get()
            )

            # Align by sample ID
            X_aligned, y_aligned, alignment_info = align_xy(
                X, ref,
                self.ct_slave_spectral_file_col_var.get(),
                self.ct_slave_target_col_var.get(),
                return_alignment_info=True
            )

            # Show alignment report
            if alignment_info['unmatched_spectra'] or alignment_info['n_nan_dropped'] > 0:
                msg = f"Alignment Report:\n\n"
                msg += f"Matched samples: {len(alignment_info['matched_ids'])}\n"
                if alignment_info['unmatched_spectra']:
                    msg += f"Unmatched spectra (no ref): {len(alignment_info['unmatched_spectra'])}\n"
                if alignment_info['unmatched_reference']:
                    msg += f"Unmatched references (no spectra): {len(alignment_info['unmatched_reference'])}\n"
                if alignment_info['n_nan_dropped'] > 0:
                    msg += f"Dropped (NaN targets): {alignment_info['n_nan_dropped']}\n"
                if alignment_info['used_fuzzy_matching']:
                    msg += f"\n⚠ Used fuzzy filename matching"
                messagebox.showinfo("Alignment Report", msg)

            # Store as DataFrames
            self.ct_slave_X = X_aligned
            self.ct_slave_y = y_aligned
            self.ct_slave_wavelengths = X_aligned.columns.astype(float).values

            # Update data info display
            self._update_data_info()

            # Validate pairing with master if both loaded
            if self.ct_master_X is not None:
                self._validate_master_slave_pairing()

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load slave data:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _validate_master_slave_pairing(self):
        """Validate that master and slave have matching sample IDs."""
        from tkinter import messagebox

        if self.ct_master_X is None or self.ct_slave_X is None:
            return False

        master_ids = set(self.ct_master_X.index)
        slave_ids = set(self.ct_slave_X.index)

        # Find intersection
        common_ids = master_ids & slave_ids

        if len(common_ids) == 0:
            messagebox.showerror("No Matching Samples",
                "Master and slave datasets have NO common sample IDs!\n\n"
                "JYPLS-inv requires the same samples measured on both instruments.\n\n"
                "Please check that:\n"
                "1. Sample IDs match between master and slave reference files\n"
                "2. Filename matching is correct")
            return False

        # Warn if not all samples match
        if len(common_ids) < len(master_ids) or len(common_ids) < len(slave_ids):
            only_master = master_ids - slave_ids
            only_slave = slave_ids - master_ids

            msg = f"Partial sample overlap detected:\n\n"
            msg += f"✓ Matched samples: {len(common_ids)}\n"
            if only_master:
                msg += f"⚠ Only in master: {len(only_master)}\n"
                msg += f"   Examples: {', '.join(list(only_master)[:3])}\n"
            if only_slave:
                msg += f"⚠ Only in slave: {len(only_slave)}\n"
                msg += f"   Examples: {', '.join(list(only_slave)[:3])}\n"
            msg += f"\nOnly matched samples will be used for JYPLS-inv."

            messagebox.showwarning("Partial Match", msg)
        else:
            # Perfect match!
            messagebox.showinfo("Perfect Match",
                f"✓ All {len(common_ids)} samples matched between master and slave!")

        return True

    # ========================================================================
    # END OF ENHANCED STEP 1 HELPER METHODS
    # ========================================================================

    # ========================================================================
    # HELPER METHOD: Apply Transfer Model
    # ========================================================================

    def _apply_transfer_model(self, X_slave, transfer_model):
        """Apply a transfer model to transform slave spectra to master domain.

        Parameters
        ----------
        X_slave : np.ndarray
            Slave spectra, shape (n_samples, n_wavelengths)
        transfer_model : TransferModel
            Transfer model object with method and params

        Returns
        -------
        np.ndarray
            Transformed spectra in master domain
        """
        from spectral_predict.calibration_transfer import (
            apply_ds, apply_pds, apply_tsr, apply_ctai, apply_nspfce, apply_jypls_inv
        )

        method = transfer_model.method
        params = transfer_model.params

        if method == 'ds':
            return apply_ds(X_slave, params['A'])
        elif method == 'pds':
            return apply_pds(X_slave, params['B'], params['window'])
        elif method == 'tsr':
            return apply_tsr(X_slave, params)
        elif method == 'ctai':
            return apply_ctai(X_slave, params)
        elif method == 'nspfce':
            return apply_nspfce(X_slave, params)
        elif method == 'jypls-inv':
            return apply_jypls_inv(X_slave, params)
        else:
            raise ValueError(f"Unknown transfer method: {method}")

    # ========================================================================
    # STEP 3A HELPER METHODS: Prediction Workflow
    # ========================================================================

    def _browse_prediction_model(self):
        """Browse for prediction model .pkl file."""
        from tkinter import filedialog
        filepath = filedialog.askopenfilename(
            title="Select Prediction Model File",
            filetypes=[("Pickle files", "*.pkl"), ("DASP Model files", "*.dasp"), ("All files", "*.*")]
        )
        if filepath:
            self.ct_pred_model_path_var.set(filepath)

    def _load_prediction_model(self):
        """Load sklearn prediction model from .pkl or .dasp file."""
        import pickle
        from tkinter import messagebox

        filepath = self.ct_pred_model_path_var.get()
        if not filepath:
            messagebox.showwarning("No File", "Please browse and select a prediction model file first.")
            return

        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)

            # Handle different model formats
            if isinstance(model_data, dict):
                # DASP model format
                self.current_prediction_model = model_data.get('model')
                model_type = model_data.get('model_type', 'Unknown')
                target = model_data.get('target_variable', 'Unknown')

                # Extract metrics
                metrics = model_data.get('performance', {})
                if 'R2' in metrics:
                    metric_str = f"R² = {metrics['R2']:.4f}"
                elif 'Accuracy' in metrics:
                    metric_str = f"Accuracy = {metrics['Accuracy']:.4f}"
                else:
                    metric_str = "No metrics available"

            else:
                # Raw sklearn model
                self.current_prediction_model = model_data
                model_type = type(model_data).__name__
                target = "Unknown"
                metric_str = "No metrics available (raw model)"

            # Display model info
            info_text = f"Model Type: {model_type}\n"
            info_text += f"Target Variable: {target}\n"
            info_text += f"Performance: {metric_str}\n"
            info_text += f"File: {filepath}"

            self.ct_pred_model_info_text.config(state='normal')
            self.ct_pred_model_info_text.delete('1.0', 'end')
            self.ct_pred_model_info_text.insert('1.0', info_text)
            self.ct_pred_model_info_text.config(state='disabled')

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load prediction model:\n{str(e)}")

    def _browse_new_slave_data_predict(self):
        """Browse for new slave spectra for prediction."""
        from tkinter import filedialog
        filepath = filedialog.askopenfilename(
            title="Select New Slave Spectra",
            filetypes=[("CSV files", "*.csv"), ("NumPy files", "*.npy"), ("All files", "*.*")]
        )
        if filepath:
            self.ct_pred_slave_path_var.set(filepath)

    def _load_new_slave_data_predict(self):
        """Load new slave data for prediction workflow."""
        import numpy as np
        import pandas as pd
        from tkinter import messagebox

        filepath = self.ct_pred_slave_path_var.get()
        if not filepath:
            messagebox.showwarning("No File", "Please browse and select slave spectra file first.")
            return

        try:
            # Load data based on file type
            if filepath.endswith('.csv'):
                df = pd.read_csv(filepath)
                # Assume first column is wavelengths or sample IDs
                if 'wavelength' in df.columns[0].lower():
                    wavelengths = df.iloc[:, 0].values
                    X = df.iloc[:, 1:].values.T  # Transpose to (samples, wavelengths)
                else:
                    # First column is sample IDs
                    wavelengths = np.array([float(col) for col in df.columns[1:]])
                    X = df.iloc[:, 1:].values

            elif filepath.endswith('.npy'):
                data = np.load(filepath)
                if data.ndim == 2:
                    # Assume shape is (samples, wavelengths)
                    X = data
                    wavelengths = np.arange(X.shape[1])  # Generic wavelengths
                else:
                    raise ValueError("NumPy array must be 2D (samples, wavelengths)")
            else:
                raise ValueError("Unsupported file format. Use .csv or .npy")

            # Validate against transfer model
            if self.current_transfer_model is None:
                messagebox.showwarning("Warning",
                    "No transfer model loaded. Cannot validate wavelength grid.\n"
                    "Data loaded, but may not be compatible.")
            else:
                # Check if wavelengths match transfer model
                if hasattr(self.current_transfer_model, 'wavelengths_common'):
                    expected_wl = self.current_transfer_model.wavelengths_common
                elif self.ct_wavelengths_common is not None:
                    expected_wl = self.ct_wavelengths_common
                else:
                    expected_wl = None

                if expected_wl is not None:
                    if len(wavelengths) != len(expected_wl):
                        messagebox.showerror("Error",
                            f"Wavelength mismatch!\n"
                            f"Transfer model expects {len(expected_wl)} wavelengths\n"
                            f"Loaded data has {len(wavelengths)} wavelengths")
                        return

            # Store data
            self.new_slave_data_predict = (wavelengths, X)

            # Display info
            info_text = f"Samples: {X.shape[0]}\n"
            info_text += f"Wavelengths: {len(wavelengths)} ({wavelengths[0]:.1f} - {wavelengths[-1]:.1f} nm)\n"
            info_text += f"File: {filepath}"

            self.ct_pred_slave_info_text.config(state='normal')
            self.ct_pred_slave_info_text.delete('1.0', 'end')
            self.ct_pred_slave_info_text.insert('1.0', info_text)
            self.ct_pred_slave_info_text.config(state='disabled')

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load slave data:\n{str(e)}")

    def _run_prediction_workflow(self):
        """Run the complete prediction workflow: transform -> predict -> display."""
        from tkinter import messagebox
        import numpy as np

        # Validate prerequisites
        if self.current_transfer_model is None:
            messagebox.showerror("Error", "Please load or build a transfer model first (Step 1).")
            return

        if self.current_prediction_model is None:
            messagebox.showerror("Error", "Please load a prediction model first (C1).")
            return

        if self.new_slave_data_predict is None:
            messagebox.showerror("Error", "Please load new slave spectra first (C2).")
            return

        try:
            # Step 1: Apply transfer model to new slave data
            wavelengths, X_slave = self.new_slave_data_predict

            # Transform slave data to master domain
            X_transferred = self._apply_transfer_model(X_slave, self.current_transfer_model)

            # Step 2: Use prediction model to predict properties
            y_pred = self.current_prediction_model.predict(X_transferred)

            # Step 3: Display results
            # Clear existing results
            for item in self.ct_predictions_tree.get_children():
                self.ct_predictions_tree.delete(item)

            # Add predictions to treeview
            for i, pred in enumerate(y_pred):
                sample_id = f"Sample_{i+1}"
                self.ct_predictions_tree.insert('', 'end', values=(sample_id, f"{pred:.4f}"))

            # Store predictions for export
            self.ct_pred_y_pred = y_pred
            self.ct_pred_sample_ids = [f"Sample_{i+1}" for i in range(len(y_pred))]

            # Enable export button
            self.ct_export_predictions_button.config(state='normal')

            # Create plots
            self._plot_ct_prediction_results(y_pred, X_slave, X_transferred)

            # Play success sound (removed popup - results shown in tree and plots)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Prediction workflow failed:\n{str(e)}")

    def _plot_ct_prediction_results(self, y_pred, X_slave_original, X_transferred):
        """Create plots showing prediction results and spectral transformation."""
        if not HAS_MATPLOTLIB:
            return

        # Clear existing plots
        for widget in self.ct_predictions_plot_frame.winfo_children():
            widget.destroy()

        # Create figure with 2 subplots
        fig = Figure(figsize=(12, 5), facecolor=self.colors['bg'])

        # Subplot 1: Distribution of predicted values
        ax1 = fig.add_subplot(121)
        ax1.hist(y_pred, bins=20, color=self.colors['accent'], alpha=0.7, edgecolor='black')
        ax1.set_xlabel('Predicted Value', color=self.colors['text'])
        ax1.set_ylabel('Frequency', color=self.colors['text'])
        ax1.set_title('Distribution of Predictions', color=self.colors['text'])
        ax1.set_facecolor(self.colors['panel'])
        ax1.tick_params(colors=self.colors['text'])
        for spine in ax1.spines.values():
            spine.set_edgecolor(self.colors['text'])

        # Subplot 2: Before/After spectra comparison (mean spectra)
        ax2 = fig.add_subplot(122)
        mean_slave = X_slave_original.mean(axis=0)
        mean_transferred = X_transferred.mean(axis=0)

        wavelengths = self.new_slave_data_predict[0]
        ax2.plot(wavelengths, mean_slave, label='Slave (Before)', color='red', alpha=0.7)
        ax2.plot(wavelengths, mean_transferred, label='Transferred (After)', color='blue', alpha=0.7)
        ax2.set_xlabel('Wavelength (nm)', color=self.colors['text'])
        ax2.set_ylabel('Intensity', color=self.colors['text'])
        ax2.set_title('Mean Spectra: Before vs After Transfer', color=self.colors['text'])
        ax2.legend(facecolor=self.colors['panel'], edgecolor=self.colors['text'], labelcolor=self.colors['text'])
        ax2.set_facecolor(self.colors['panel'])
        ax2.tick_params(colors=self.colors['text'])
        for spine in ax2.spines.values():
            spine.set_edgecolor(self.colors['text'])

        fig.tight_layout()

        # Embed in tkinter
        canvas = FigureCanvasTkAgg(fig, master=self.ct_predictions_plot_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill='both', expand=True)

        # Add toolbar
        toolbar = NavigationToolbar2Tk(canvas, self.ct_predictions_plot_frame)
        toolbar.update()

    def _export_predictions(self):
        """Export predictions to CSV file."""
        import pandas as pd
        from tkinter import filedialog, messagebox

        if self.ct_pred_y_pred is None:
            messagebox.showwarning("No Predictions", "Please run the prediction workflow first.")
            return

        # Ask for save location
        filepath = filedialog.asksaveasfilename(
            title="Export Predictions",
            defaultextension=".csv",
            filetypes=[("CSV files", "*.csv"), ("All files", "*.*")]
        )

        if not filepath:
            return

        try:
            # Create dataframe
            df = pd.DataFrame({
                'Sample_ID': self.ct_pred_sample_ids,
                'Predicted_Value': self.ct_pred_y_pred
            })

            # Save to CSV
            df.to_csv(filepath, index=False)

            # Play success sound (removed popup - file saved)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to export predictions:\n{str(e)}")

    # ========================================================================
    # END OF SECTION C HELPER METHODS
    # ========================================================================

    # ========================================================================
    # SECTION D HELPER METHODS FOR EXPORT WORKFLOW
    # ========================================================================

    def _browse_export_slave_spectra(self):
        """Browse for new slave spectra to transform and export."""
        from tkinter import filedialog
        path = filedialog.askdirectory(title="Select Slave Spectra Directory for Export")
        if not path:
            # Try file selection
            path = filedialog.askopenfilename(
                title="Select Slave Spectra File for Export",
                filetypes=[("CSV files", "*.csv"), ("NumPy files", "*.npy"), ("All files", "*.*")]
            )
        if path:
            self.ct_export_slave_path_var.set(path)

    def _load_new_slave_data_export(self):
        """Load new slave data for transformation and export.

        Loads data, detects format, validates compatibility with transfer model,
        and displays information to the user.
        """
        from tkinter import messagebox

        path = self.ct_export_slave_path_var.get()
        if not path:
            messagebox.showwarning("No Path", "Please browse and select slave spectra first.")
            return

        # Check if transfer model exists
        if self.current_transfer_model is None and self.ct_transfer_model is None:
            messagebox.showerror("No Transfer Model",
                               "Please load or build a transfer model in Step 1 first.")
            return

        try:
            # Detect format
            detected_format = self._detect_data_format(path)
            self.slave_data_format = detected_format  # Store for export

            # Load data
            if os.path.isdir(path):
                wavelengths, X = self._load_spectra_from_directory(path)
            elif path.endswith('.npy'):
                X = np.load(path)
                wavelengths = np.arange(X.shape[1])  # Placeholder wavelengths
                messagebox.showinfo("Note", "NPY file loaded. Using placeholder wavelengths.")
            elif path.endswith('.csv'):
                df = pd.read_csv(path, header=None)
                wavelengths = df.iloc[0, :].values.astype(float)
                X = df.iloc[1:, :].values.astype(float)
            else:
                raise ValueError(f"Unsupported file format: {path}")

            # Store data
            self.new_slave_data_export = (wavelengths, X)

            # Validate compatibility with transfer model
            transfer_model = self.current_transfer_model or self.ct_transfer_model
            if transfer_model is not None:
                # Check wavelength compatibility
                model_wavelengths = getattr(transfer_model, 'wavelengths_common', None)
                if model_wavelengths is not None:
                    if len(wavelengths) != len(model_wavelengths):
                        messagebox.showwarning("Wavelength Mismatch",
                            f"Warning: Loaded data has {len(wavelengths)} wavelengths, "
                            f"but transfer model expects {len(model_wavelengths)}. "
                            f"Resampling may be required.")

            # Display data info
            info_text = f"Samples: {X.shape[0]}\n"
            info_text += f"Wavelengths: {len(wavelengths)} points ({wavelengths[0]:.1f} - {wavelengths[-1]:.1f} nm)\n"
            info_text += f"Detected Format: {detected_format}"

            self.ct_export_data_info_text.config(state='normal')
            self.ct_export_data_info_text.delete('1.0', tk.END)
            self.ct_export_data_info_text.insert('1.0', info_text)
            self.ct_export_data_info_text.config(state='disabled')

            # Play success sound (removed popup - info shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load slave spectra:\n{str(e)}")

    def _transform_spectra(self):
        """Apply transfer model to new slave data and show preview.

        Transforms spectra, displays statistics, and shows before/after plots
        with tabbed derivatives (reusing _plot_transfer_quality pattern).
        """
        from tkinter import messagebox

        # Validate inputs
        if self.new_slave_data_export is None:
            messagebox.showwarning("No Data", "Please load slave spectra first (Section D1).")
            return

        transfer_model = self.current_transfer_model or self.ct_transfer_model
        if transfer_model is None:
            messagebox.showerror("No Transfer Model",
                               "Please load or build a transfer model in Step 1 first.")
            return

        try:
            wavelengths_slave, X_slave = self.new_slave_data_export

            # Get transfer model wavelengths
            model_wavelengths = getattr(transfer_model, 'wavelengths_common', None)
            if model_wavelengths is None:
                # Use legacy wavelengths if available
                model_wavelengths = self.ct_wavelengths_common

            # Resample slave data to match transfer model wavelengths if needed
            if model_wavelengths is not None:
                if not np.array_equal(wavelengths_slave, model_wavelengths):
                    from spectral_predict.calibration_transfer import resample_to_grid
                    X_slave_resampled = resample_to_grid(wavelengths_slave, X_slave, model_wavelengths)
                else:
                    X_slave_resampled = X_slave
            else:
                X_slave_resampled = X_slave
                model_wavelengths = wavelengths_slave

            # Apply transfer based on method
            method = transfer_model.method.lower()

            if method == 'ds':
                from spectral_predict.calibration_transfer import apply_ds
                A = transfer_model.params['A']
                X_transferred = apply_ds(X_slave_resampled, A)
            elif method == 'pds':
                from spectral_predict.calibration_transfer import apply_pds
                B = transfer_model.params['B']
                window = transfer_model.params['window']
                X_transferred = apply_pds(X_slave_resampled, B, window)
            elif method == 'tsr':
                from spectral_predict.calibration_transfer import apply_tsr
                X_transferred = apply_tsr(X_slave_resampled, transfer_model.params)
            elif method == 'ctai':
                from spectral_predict.calibration_transfer import apply_ctai
                X_transferred = apply_ctai(X_slave_resampled, transfer_model.params)
            elif method == 'jypls-inv':
                from spectral_predict.calibration_transfer import apply_jypls_inv
                X_transferred = apply_jypls_inv(X_slave_resampled, transfer_model.params)
            elif method == 'nspfce':
                from spectral_predict.calibration_transfer import apply_nspfce
                X_transferred = apply_nspfce(X_slave_resampled, transfer_model.params)
            else:
                raise ValueError(f"Unsupported transfer method: {method}")

            # Store transformed spectra
            self.transformed_spectra = (model_wavelengths, X_transferred)

            # Calculate statistics
            rmse = np.sqrt(np.mean((X_transferred - X_slave_resampled) ** 2))
            coverage = 100.0  # All spectra transformed

            # Display statistics
            stats_text = f"Transformation Complete!\n"
            stats_text += f"RMSE: {rmse:.6f}\n"
            stats_text += f"Coverage: {coverage:.1f}% ({X_transferred.shape[0]} samples)"

            self.ct_transform_stats_text.config(state='normal')
            self.ct_transform_stats_text.delete('1.0', tk.END)
            self.ct_transform_stats_text.insert('1.0', stats_text)
            self.ct_transform_stats_text.config(state='disabled')

            # Create before/after plot with tabbed derivatives
            self._plot_transform_preview(X_slave_resampled, X_transferred, model_wavelengths)

            # Play success sound (removed popup - stats and plot shown)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to transform spectra:\n{str(e)}\n\nDetails: {repr(e)}")
            import traceback
            traceback.print_exc()

    def _plot_transform_preview(self, X_before, X_after, wavelengths):
        """Plot before/after transformation preview with tabbed derivatives.

        Similar to _plot_transfer_quality but for export workflow.
        Shows 2 subplots: Before and After transformation.
        """
        if not HAS_MATPLOTLIB:
            return

        try:
            # Clear previous plots
            for widget in self.ct_transform_preview_frame.winfo_children():
                widget.destroy()

            # Create notebook for tabs
            from tkinter import ttk
            derivative_notebook = ttk.Notebook(self.ct_transform_preview_frame)
            derivative_notebook.pack(fill=tk.BOTH, expand=True, pady=(0, 10))

            # Helper function to compute derivatives
            def compute_derivative(X, wavelengths_arr, deriv_order):
                """Compute derivative using Savitzky-Golay filter."""
                from scipy.signal import savgol_filter
                if deriv_order == 0:
                    return X
                window_length = min(11, len(wavelengths_arr) - 1)
                if window_length % 2 == 0:
                    window_length -= 1
                if window_length < 5:
                    window_length = 5
                X_deriv = np.apply_along_axis(
                    lambda y: savgol_filter(y, window_length, polyorder=2, deriv=deriv_order),
                    axis=1, arr=X
                )
                return X_deriv

            # Helper function to create 2-subplot comparison figure
            def create_comparison_figure(before_data, after_data, ylabel, title_suffix):
                """Create figure with Before and After subplots."""
                fig = Figure(figsize=(10, 4))

                # Subplot 1: Before transformation
                ax1 = fig.add_subplot(121)
                before_mean = np.mean(before_data, axis=0)
                before_std = np.std(before_data, axis=0)
                ax1.plot(wavelengths, before_mean, 'r-', linewidth=2, label='Mean')
                ax1.fill_between(wavelengths,
                               before_mean - before_std,
                               before_mean + before_std,
                               alpha=0.3, color='r', label='±1 Std')
                ax1.set_xlabel('Wavelength (nm)', fontsize=10)
                ax1.set_ylabel(ylabel, fontsize=10)
                ax1.set_title(f'Before Transfer {title_suffix}', fontsize=11, fontweight='bold')
                ax1.legend(fontsize=8)
                ax1.grid(True, alpha=0.3)

                # Subplot 2: After transformation
                ax2 = fig.add_subplot(122)
                after_mean = np.mean(after_data, axis=0)
                after_std = np.std(after_data, axis=0)
                ax2.plot(wavelengths, after_mean, 'g-', linewidth=2, label='Mean')
                ax2.fill_between(wavelengths,
                               after_mean - after_std,
                               after_mean + after_std,
                               alpha=0.3, color='g', label='±1 Std')
                ax2.set_xlabel('Wavelength (nm)', fontsize=10)
                ax2.set_ylabel(ylabel, fontsize=10)
                ax2.set_title(f'After Transfer {title_suffix}', fontsize=11, fontweight='bold')
                ax2.legend(fontsize=8)
                ax2.grid(True, alpha=0.3)

                fig.tight_layout()
                return fig

            # Compute all derivatives
            before_d1 = compute_derivative(X_before, wavelengths, 1)
            after_d1 = compute_derivative(X_after, wavelengths, 1)

            before_d2 = compute_derivative(X_before, wavelengths, 2)
            after_d2 = compute_derivative(X_after, wavelengths, 2)

            # Tab 1: Raw spectra
            tab_raw = ttk.Frame(derivative_notebook)
            derivative_notebook.add(tab_raw, text='Raw Spectra')

            fig_raw = create_comparison_figure(
                X_before, X_after,
                self._get_spectral_ylabel(), 'Spectra'
            )

            canvas_raw = FigureCanvasTkAgg(fig_raw, tab_raw)
            canvas_raw.draw()
            canvas_raw.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            self._add_plot_export_button(tab_raw, fig_raw, "transform_preview_raw")

            # Tab 2: 1st derivative
            tab_d1 = ttk.Frame(derivative_notebook)
            derivative_notebook.add(tab_d1, text='1st Derivative')

            fig_d1 = create_comparison_figure(
                before_d1, after_d1,
                '1st Derivative', '(1st Derivative)'
            )

            canvas_d1 = FigureCanvasTkAgg(fig_d1, tab_d1)
            canvas_d1.draw()
            canvas_d1.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            self._add_plot_export_button(tab_d1, fig_d1, "transform_preview_d1")

            # Tab 3: 2nd derivative
            tab_d2 = ttk.Frame(derivative_notebook)
            derivative_notebook.add(tab_d2, text='2nd Derivative')

            fig_d2 = create_comparison_figure(
                before_d2, after_d2,
                '2nd Derivative', '(2nd Derivative)'
            )

            canvas_d2 = FigureCanvasTkAgg(fig_d2, tab_d2)
            canvas_d2.draw()
            canvas_d2.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            self._add_plot_export_button(tab_d2, fig_d2, "transform_preview_d2")

        except Exception as e:
            print(f"Error creating transform preview plot: {e}")
            import traceback
            traceback.print_exc()

    def _browse_export_output_dir(self):
        """Browse for output directory to save transformed spectra."""
        from tkinter import filedialog
        directory = filedialog.askdirectory(title="Select Output Directory for Transformed Spectra")
        if directory:
            self.ct_export_output_dir_var.set(directory)

    def _export_transformed_spectra(self):
        """Export transformed spectra with automatic format matching.

        Preserves file naming and structure from input based on detected format.
        Supports: csv, npy, asd_folder, spc_folder, csv_folder formats.
        """
        from tkinter import messagebox
        from pathlib import Path

        # Validate inputs
        if self.transformed_spectra is None:
            messagebox.showwarning("No Data", "Please transform spectra first (Section D2).")
            return

        output_dir = self.ct_export_output_dir_var.get()
        if not output_dir:
            messagebox.showwarning("No Output Directory", "Please select an output directory.")
            return

        try:
            wavelengths, X_transformed = self.transformed_spectra
            input_path = self.ct_export_slave_path_var.get()
            data_format = self.slave_data_format

            # Update status
            self.ct_export_status_text.config(state='normal')
            self.ct_export_status_text.delete('1.0', tk.END)
            self.ct_export_status_text.insert('1.0', f"Exporting {X_transformed.shape[0]} spectra...\n")
            self.ct_export_status_text.config(state='disabled')
            self.root.update()

            # Export based on detected format
            if data_format == 'csv':
                # Export as single CSV file with wavelengths as header
                output_path = os.path.join(output_dir, "transformed_spectra.csv")
                df = pd.DataFrame(X_transformed, columns=wavelengths)
                df.to_csv(output_path, index=False)
                status_msg = f"Exported to CSV: {output_path}"

            elif data_format == 'npy':
                # Export as .npy file
                output_path = os.path.join(output_dir, "transformed_spectra.npy")
                np.save(output_path, X_transformed)
                status_msg = f"Exported to NPY: {output_path}"

            elif data_format in ['asd_folder', 'spc_folder', 'csv_folder']:
                # Recreate folder structure with transformed data
                input_path_obj = Path(input_path)

                # Get list of original files to preserve naming
                import glob
                if data_format == 'asd_folder':
                    original_files = sorted(glob.glob(os.path.join(input_path, "*.asd")))
                    ext = '.asd'
                elif data_format == 'spc_folder':
                    original_files = sorted(glob.glob(os.path.join(input_path, "*.spc")))
                    ext = '.spc'
                else:  # csv_folder
                    original_files = sorted(glob.glob(os.path.join(input_path, "*.csv")))
                    ext = '.csv'

                # Create output directory if it doesn't exist
                os.makedirs(output_dir, exist_ok=True)

                # Export each spectrum with preserved naming
                for i, orig_file in enumerate(original_files[:X_transformed.shape[0]]):
                    if i >= X_transformed.shape[0]:
                        break

                    orig_name = Path(orig_file).stem

                    if ext == '.csv':
                        # Export as CSV with wavelengths in first row
                        output_file = os.path.join(output_dir, f"{orig_name}.csv")
                        df = pd.DataFrame([wavelengths, X_transformed[i, :]])
                        df.to_csv(output_file, index=False, header=False)
                    elif ext == '.npy':
                        # Export as NPY
                        output_file = os.path.join(output_dir, f"{orig_name}.npy")
                        np.save(output_file, X_transformed[i, :])
                    else:
                        # For ASD/SPC, export as CSV (since writing binary formats is complex)
                        output_file = os.path.join(output_dir, f"{orig_name}_transformed.csv")
                        df = pd.DataFrame([wavelengths, X_transformed[i, :]])
                        df.to_csv(output_file, index=False, header=False)

                status_msg = f"Exported {X_transformed.shape[0]} files to: {output_dir}"

            else:
                # Fallback: export as CSV
                output_path = os.path.join(output_dir, "transformed_spectra.csv")
                df = pd.DataFrame(X_transformed, columns=wavelengths)
                df.to_csv(output_path, index=False)
                status_msg = f"Exported to CSV (fallback): {output_path}"

            # Update status
            self.ct_export_status_text.config(state='normal')
            self.ct_export_status_text.delete('1.0', tk.END)
            self.ct_export_status_text.insert('1.0', status_msg)
            self.ct_export_status_text.config(state='disabled')

            # Play success sound (removed popup - status shown in text widget)
            self.play_sound('success')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to export spectra:\n{str(e)}")
            import traceback
            traceback.print_exc()

    # ========================================================================
    # END OF SECTION D HELPER METHODS
    # ========================================================================

    # ========================================================================
    # TAB 9: MULTI-MODEL COMPARISON
    # ========================================================================

    def _create_tab9_multi_model_comparison(self):
        """Tab 9: Multi-Model Comparison - Side-by-side analysis of multiple models with conditional flagging."""
        self.tab9 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab9, text='  🔬 Multi-Model  ')

        # Initialize comparison-specific storage
        self.comparison_primary_model = None
        self.comparison_auxiliary_models = []
        self.comparison_data = None
        self.comparison_results = None
        self.comparison_rules = []

        # Transfer model storage
        self.transfer_models = []  # List of transfer model dicts with 'path', 'model', 'description'
        self.apply_transfer = tk.BooleanVar(value=False)

        # Live monitoring variables
        self.live_monitoring_active = False
        self.live_monitoring_timer_id = None
        self.live_last_file_count = 0

        # Create scrollable canvas
        canvas = tk.Canvas(self.tab9, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(self.tab9, orient="vertical", command=canvas.yview)
        main_frame = ttk.Frame(canvas, style='TFrame', padding="30")

        main_frame.bind("<Configure>", lambda e: self._debounced_configure_scrollregion("tab9", canvas))
        canvas.create_window((0, 0), window=main_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        # === STEP 1: Load Models ===
        step1_frame = ttk.LabelFrame(main_frame, text="Step 1: Load Models", padding="20")
        step1_frame.pack(fill='x', pady=(0, 15))

        # Primary model section
        primary_section = ttk.Frame(step1_frame)
        primary_section.pack(fill='x', pady=(0, 10))

        ttk.Label(primary_section, text="🟢 Primary Model:", style='Subheading.TLabel').pack(anchor='w', pady=(0, 5))
        ttk.Label(primary_section, text="The main model whose predictions are your primary focus",
                  style='Caption.TLabel').pack(anchor='w', pady=(0, 5))

        primary_btn_frame = ttk.Frame(primary_section)
        primary_btn_frame.pack(anchor='w', pady=5)
        ttk.Button(primary_btn_frame, text="📂 Load Primary Model",
                   command=self._load_comparison_primary_model, style='Modern.TButton').pack(side='left', padx=(0, 5))
        ttk.Button(primary_btn_frame, text="🗑️ Clear",
                   command=self._clear_comparison_primary_model, style='Modern.TButton').pack(side='left')

        # Primary model display
        self.comparison_primary_text = tk.Text(primary_section, height=4, width=90,
                                               font=('Consolas', 9),
                                               bg=self.colors['panel'], fg=self.colors['text'],
                                               wrap=tk.WORD, state='disabled',
                                               relief='flat', borderwidth=0,
                                               selectbackground=self.colors['accent'],
                                               selectforeground=self.colors['text_inverse'])
        self.comparison_primary_text.pack(fill='x', pady=5)
        self._update_comparison_primary_display()

        # Separator
        ttk.Separator(step1_frame, orient='horizontal').pack(fill='x', pady=10)

        # Auxiliary models section
        aux_section = ttk.Frame(step1_frame)
        aux_section.pack(fill='x')

        ttk.Label(aux_section, text="🔵 Auxiliary Models:", style='Subheading.TLabel').pack(anchor='w', pady=(0, 5))
        ttk.Label(aux_section, text="Additional models for context (e.g., consolidant detection, moisture content)",
                  style='Caption.TLabel').pack(anchor='w', pady=(0, 5))

        aux_btn_frame = ttk.Frame(aux_section)
        aux_btn_frame.pack(anchor='w', pady=5)
        ttk.Button(aux_btn_frame, text="➕ Add Auxiliary Model(s)",
                   command=self._load_comparison_auxiliary_models, style='Modern.TButton').pack(side='left', padx=(0, 5))
        ttk.Button(aux_btn_frame, text="🗑️ Clear All",
                   command=self._clear_comparison_auxiliary_models, style='Modern.TButton').pack(side='left')

        # Auxiliary models display with scrollbar
        aux_text_frame = ttk.Frame(aux_section)
        aux_text_frame.pack(fill='x', pady=5)

        self.comparison_auxiliary_text = tk.Text(aux_text_frame, height=6, width=90,
                                                 font=('Consolas', 9),
                                                 bg=self.colors['panel'], fg=self.colors['text'],
                                                 wrap=tk.WORD, state='disabled',
                                                 relief='flat', borderwidth=0,
                                                 selectbackground=self.colors['accent'],
                                                 selectforeground=self.colors['text_inverse'])
        self.comparison_auxiliary_text.pack(side='left', fill='both', expand=True)

        aux_scrollbar = ttk.Scrollbar(aux_text_frame, orient='vertical',
                                     command=self.comparison_auxiliary_text.yview)
        aux_scrollbar.pack(side='right', fill='y')
        self.comparison_auxiliary_text.config(yscrollcommand=aux_scrollbar.set)
        self._update_comparison_auxiliary_display()

        # === STEP 1.5: Transfer Models (Optional) ===
        transfer_frame = ttk.LabelFrame(main_frame, text="Step 1.5: Load Transfer Models (Optional)", padding="20")
        transfer_frame.pack(fill='x', pady=(0, 15))

        # Enable checkbox and description
        self.apply_transfer_checkbox = ttk.Checkbutton(transfer_frame,
                                                       text="Apply Transfer Models Before Prediction",
                                                       variable=self.apply_transfer,
                                                       command=self._on_transfer_toggle)
        self.apply_transfer_checkbox.pack(anchor='w', pady=(0, 10))

        ttk.Label(transfer_frame, text="Apply calibration transfer models to transform spectra between instrument domains",
                  style='Caption.TLabel').pack(anchor='w', pady=(0, 10))

        # Button frame
        transfer_btn_frame = ttk.Frame(transfer_frame)
        transfer_btn_frame.pack(anchor='w', pady=5)

        ttk.Button(transfer_btn_frame, text="📂 Browse Transfer Models...",
                   command=self._load_transfer_models, style='Modern.TButton').pack(side='left', padx=(0, 5))
        ttk.Button(transfer_btn_frame, text="🗑️ Clear All",
                   command=self._clear_transfer_models, style='Modern.TButton').pack(side='left')

        # Transfer models listbox with controls
        list_frame = ttk.Frame(transfer_frame)
        list_frame.pack(fill='both', expand=True, pady=(10, 0))

        ttk.Label(list_frame, text="Loaded Transfer Models (applied in order):",
                  style='Subheading.TLabel').pack(anchor='w', pady=(0, 5))

        # Listbox with scrollbar
        listbox_frame = ttk.Frame(list_frame)
        listbox_frame.pack(fill='both', expand=True)

        self.transfer_listbox = tk.Listbox(listbox_frame, height=5, width=80,
                                          font=('Consolas', 9),
                                          bg=self.colors['panel'], fg=self.colors['text'],
                                          selectbackground=self.colors['accent'],
                                          selectforeground=self.colors['text_inverse'],
                                          relief='flat', borderwidth=0)
        self.transfer_listbox.pack(side='left', fill='both', expand=True)

        transfer_scrollbar = ttk.Scrollbar(listbox_frame, orient='vertical',
                                          command=self.transfer_listbox.yview)
        transfer_scrollbar.pack(side='right', fill='y')
        self.transfer_listbox.config(yscrollcommand=transfer_scrollbar.set)

        # Listbox controls
        control_frame = ttk.Frame(list_frame)
        control_frame.pack(fill='x', pady=(5, 0))

        ttk.Button(control_frame, text="↑", command=self._move_transfer_up,
                   style='Modern.TButton', width=3).pack(side='left', padx=(0, 5))
        ttk.Button(control_frame, text="↓", command=self._move_transfer_down,
                   style='Modern.TButton', width=3).pack(side='left', padx=(0, 5))
        ttk.Button(control_frame, text="Remove Selected", command=self._remove_transfer_model,
                   style='Modern.TButton').pack(side='left', padx=(10, 0))

        # === STEP 2: Load Data ===
        step2_frame = ttk.LabelFrame(main_frame, text="Step 2: Load Spectra", padding="20")
        step2_frame.pack(fill='x', pady=(0, 15))

        ttk.Label(step2_frame, text="Data Source:", style='Subheading.TLabel').grid(
            row=0, column=0, sticky=tk.W, pady=5)

        self.comparison_data_source = tk.StringVar(value='directory')
        source_frame = ttk.Frame(step2_frame)
        source_frame.grid(row=1, column=0, columnspan=2, sticky=tk.W, pady=5)

        ttk.Radiobutton(source_frame, text="Directory (ASD/SPC)",
                       variable=self.comparison_data_source, value='directory',
                       command=self._on_comparison_source_change).pack(side='left', padx=5)
        ttk.Radiobutton(source_frame, text="CSV/Excel File",
                       variable=self.comparison_data_source, value='csv',
                       command=self._on_comparison_source_change).pack(side='left', padx=5)
        ttk.Radiobutton(source_frame, text="Use Validation Set 🔬",
                       variable=self.comparison_data_source, value='validation',
                       command=self._on_comparison_source_change).pack(side='left', padx=5)
        ttk.Radiobutton(source_frame, text="Live Folder Monitoring 🔴",
                       variable=self.comparison_data_source, value='live',
                       command=self._on_comparison_source_change).pack(side='left', padx=5)

        # Path entry
        self.comparison_path_label = ttk.Label(step2_frame, text="Path:", style='Caption.TLabel')
        self.comparison_path_label.grid(row=2, column=0, sticky=tk.W, pady=(10, 5))

        path_entry_frame = ttk.Frame(step2_frame)
        path_entry_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)

        self.comparison_data_path = tk.StringVar()
        ttk.Entry(path_entry_frame, textvariable=self.comparison_data_path,
                 width=60).pack(side='left', fill='x', expand=True, padx=(0, 10))

        self.comparison_browse_btn = ttk.Button(path_entry_frame, text="Browse",
                                                command=self._browse_comparison_data, style='Modern.TButton')
        self.comparison_browse_btn.pack(side='left', padx=(0, 10))

        self.comparison_load_data_btn = ttk.Button(path_entry_frame, text="Load Data",
                                                   command=self._load_comparison_data, style='Modern.TButton')
        self.comparison_load_data_btn.pack(side='left')

        # Live monitoring controls frame (shown only when live mode selected)
        self.comparison_live_frame = ttk.Frame(step2_frame)
        self.comparison_live_frame.grid(row=5, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(15, 0))

        # Live folder path
        live_path_frame = ttk.Frame(self.comparison_live_frame)
        live_path_frame.pack(fill='x', pady=(0, 10))
        ttk.Label(live_path_frame, text="Folder:", style='Caption.TLabel').pack(side='left', padx=(0, 5))
        self.comparison_live_path = tk.StringVar()
        ttk.Entry(live_path_frame, textvariable=self.comparison_live_path, width=50).pack(
            side='left', fill='x', expand=True, padx=(0, 10))
        ttk.Button(live_path_frame, text="Browse", command=self._browse_live_folder,
                  style='Modern.TButton').pack(side='left')

        # Scan interval control
        interval_frame = ttk.Frame(self.comparison_live_frame)
        interval_frame.pack(fill='x', pady=(0, 10))
        ttk.Label(interval_frame, text="Scan every:", style='Caption.TLabel').pack(side='left', padx=(0, 5))
        self.comparison_live_interval = tk.IntVar(value=10)
        ttk.Spinbox(interval_frame, from_=5, to=60, textvariable=self.comparison_live_interval,
                   width=10).pack(side='left', padx=(0, 5))
        ttk.Label(interval_frame, text="seconds", style='Caption.TLabel').pack(side='left')

        # Start/Stop buttons
        btn_frame = ttk.Frame(self.comparison_live_frame)
        btn_frame.pack(fill='x', pady=(0, 10))
        self.comparison_live_start_btn = ttk.Button(btn_frame, text="▶ Start Monitoring",
                                                    command=self._start_live_monitoring,
                                                    style='Modern.TButton')
        self.comparison_live_start_btn.pack(side='left', padx=(0, 5))
        self.comparison_live_stop_btn = ttk.Button(btn_frame, text="⏹ Stop Monitoring",
                                                   command=self._stop_live_monitoring,
                                                   style='Modern.TButton', state='disabled')
        self.comparison_live_stop_btn.pack(side='left')

        # Status label
        self.comparison_live_status = ttk.Label(self.comparison_live_frame,
                                               text="○ Monitoring stopped",
                                               style='Caption.TLabel', foreground='gray')
        self.comparison_live_status.pack(pady=(0, 5))

        # Data status
        self.comparison_data_status = ttk.Label(step2_frame, text="No data loaded",
                                               style='Caption.TLabel', foreground='gray')
        self.comparison_data_status.grid(row=4, column=0, columnspan=2, sticky=tk.W, pady=(5, 0))

        self._on_comparison_source_change()  # Initialize UI state

        # === STEP 3: Conditional Flagging Rules (Optional) ===
        step3_frame = ttk.LabelFrame(main_frame, text="Step 3: Conditional Flagging Rules (Optional)", padding="20")
        step3_frame.pack(fill='x', pady=(0, 15))

        ttk.Label(step3_frame, text="Create rules to flag primary predictions based on auxiliary model outputs",
                  style='Caption.TLabel').pack(anchor='w', pady=(0, 10))

        ttk.Button(step3_frame, text="➕ Add New Rule",
                   command=self._add_comparison_rule, style='Modern.TButton').pack(anchor='w', pady=5)

        # Rules display
        rules_frame = ttk.Frame(step3_frame)
        rules_frame.pack(fill='x', pady=(10, 0))

        self.comparison_rules_text = tk.Text(rules_frame, height=4, width=90,
                                            font=('Consolas', 9),
                                            bg=self.colors['panel'], fg=self.colors['text'],
                                            wrap=tk.WORD, state='disabled',
                                            relief='flat', borderwidth=0,
                                            selectbackground=self.colors['accent'],
                                            selectforeground=self.colors['text_inverse'])
        self.comparison_rules_text.pack(fill='x')
        self._update_comparison_rules_display()

        # === STEP 4: Run Comparison ===
        step4_frame = ttk.LabelFrame(main_frame, text="Step 4: Run Comparison", padding="20")
        step4_frame.pack(fill='x', pady=(0, 15))

        run_btn_frame = ttk.Frame(step4_frame)
        run_btn_frame.pack(pady=10)

        self._create_accent_button(run_btn_frame, "🚀 Run Multi-Model Comparison",
                                   self._run_comparison).pack(side='left', padx=5)

        self.comparison_status = ttk.Label(step4_frame, text="Ready to run comparison",
                                          style='Caption.TLabel', foreground='gray')
        self.comparison_status.pack(pady=(10, 0))

        # === STEP 5: Results ===
        step5_frame = ttk.LabelFrame(main_frame, text="Step 5: Comparison Results", padding="20")
        step5_frame.pack(fill='both', expand=True, pady=(0, 15))

        # Results table
        table_frame = ttk.Frame(step5_frame)
        table_frame.pack(fill='both', expand=True, pady=(0, 10))

        # Create treeview for results
        self.comparison_results_tree = ttk.Treeview(table_frame, show='headings', height=12)
        self.comparison_results_tree.pack(side='left', fill='both', expand=True)

        results_scrollbar_y = ttk.Scrollbar(table_frame, orient='vertical',
                                           command=self.comparison_results_tree.yview)
        results_scrollbar_y.pack(side='right', fill='y')
        self.comparison_results_tree.configure(yscrollcommand=results_scrollbar_y.set)

        results_scrollbar_x = ttk.Scrollbar(step5_frame, orient='horizontal',
                                           command=self.comparison_results_tree.xview)
        results_scrollbar_x.pack(fill='x')
        self.comparison_results_tree.configure(xscrollcommand=results_scrollbar_x.set)

        # Export button
        export_btn_frame = ttk.Frame(step5_frame)
        export_btn_frame.pack(pady=10)

        ttk.Button(export_btn_frame, text="📊 Export Results to Excel",
                   command=self._export_comparison_results, style='Modern.TButton').pack(side='left', padx=5)

        ttk.Button(export_btn_frame, text="📈 View Comparison Plots",
                   command=self._show_comparison_plots, style='Modern.TButton').pack(side='left', padx=5)

    # ========================================================================
    # TAB 9 HELPER METHODS: Multi-Model Comparison
    # ========================================================================

    def _update_comparison_primary_display(self):
        """Update the primary model display text widget."""
        self.comparison_primary_text.config(state='normal')
        self.comparison_primary_text.delete('1.0', tk.END)

        if self.comparison_primary_model is None:
            self.comparison_primary_text.insert('1.0', "No primary model loaded")
        else:
            model_dict = self.comparison_primary_model
            metadata = model_dict.get('metadata', {})
            filename = model_dict.get('filename', 'Unknown')

            # Extract key information
            model_name = metadata.get('model_name', 'Unknown')
            target_var = metadata.get('target_variable', 'Unknown')
            task_type = metadata.get('task_type', 'Unknown')
            preprocessing = metadata.get('preprocessing', 'Unknown')

            # Get performance metrics
            perf = metadata.get('performance', {})
            if task_type == 'regression':
                r2 = perf.get('R2', 'N/A')
                rmse = perf.get('RMSE', 'N/A')
                perf_str = f"R² = {r2:.4f}, RMSE = {rmse:.4f}" if isinstance(r2, (int, float)) else "N/A"
            else:
                acc = perf.get('accuracy', 'N/A')
                perf_str = f"Accuracy = {acc:.4f}" if isinstance(acc, (int, float)) else "N/A"

            # Add emoji indicator for task type
            task_emoji = "🏷️" if task_type == 'classification' else "📊"
            text = f"🟢 PRIMARY: {filename}\n"
            text += f"   {task_emoji} Task: {task_type.upper()}  |  Target: {target_var}  |  Model: {model_name}\n"
            text += f"   Preprocessing: {preprocessing}  |  Performance: {perf_str}"

            self.comparison_primary_text.insert('1.0', text)

        self.comparison_primary_text.config(state='disabled')

    def _update_comparison_auxiliary_display(self):
        """Update the auxiliary models display text widget."""
        self.comparison_auxiliary_text.config(state='normal')
        self.comparison_auxiliary_text.delete('1.0', tk.END)

        if not self.comparison_auxiliary_models:
            self.comparison_auxiliary_text.insert('1.0', "No auxiliary models loaded")
        else:
            for i, model_dict in enumerate(self.comparison_auxiliary_models):
                metadata = model_dict.get('metadata', {})
                filename = model_dict.get('filename', 'Unknown')

                model_name = metadata.get('model_name', 'Unknown')
                target_var = metadata.get('target_variable', 'Unknown')
                task_type = metadata.get('task_type', 'Unknown')
                preprocessing = metadata.get('preprocessing', 'Unknown')

                perf = metadata.get('performance', {})
                if task_type == 'regression':
                    r2 = perf.get('R2', 'N/A')
                    rmse = perf.get('RMSE', 'N/A')
                    perf_str = f"R² = {r2:.4f}, RMSE = {rmse:.4f}" if isinstance(r2, (int, float)) else "N/A"
                else:
                    acc = perf.get('accuracy', 'N/A')
                    perf_str = f"Acc = {acc:.4f}" if isinstance(acc, (int, float)) else "N/A"

                # Add emoji indicator for task type
                task_emoji = "🏷️" if task_type == 'classification' else "📊"
                text = f"🔵 [{i+1}] {filename}\n"
                text += f"    {task_emoji} Task: {task_type.upper()}  |  Target: {target_var}  |  Model: {model_name}\n"
                text += f"    Preprocessing: {preprocessing}  |  Performance: {perf_str}\n"

                if i < len(self.comparison_auxiliary_models) - 1:
                    text += "\n"

                self.comparison_auxiliary_text.insert(tk.END, text)

        self.comparison_auxiliary_text.config(state='disabled')

    def _update_comparison_rules_display(self):
        """Update the conditional flagging rules display."""
        self.comparison_rules_text.config(state='normal')
        self.comparison_rules_text.delete('1.0', tk.END)

        if not self.comparison_rules:
            self.comparison_rules_text.insert('1.0', "No conditional flagging rules defined")
        else:
            for i, rule in enumerate(self.comparison_rules):
                aux_model = rule['auxiliary_model']
                condition = rule['condition']
                value = rule['value']
                flag_text = rule['flag_text']

                text = f"Rule {i+1}: IF {aux_model} {condition} {value} THEN flag as \"{flag_text}\"\n"
                self.comparison_rules_text.insert(tk.END, text)

        self.comparison_rules_text.config(state='disabled')

    def _load_comparison_primary_model(self):
        """Load a single primary model for comparison."""
        from tkinter import filedialog

        filepath = filedialog.askopenfilename(
            title="Load Primary Model",
            filetypes=[("DASP Model Files", "*.dasp"), ("All Files", "*.*")],
            initialdir=self.last_directory if hasattr(self, 'last_directory') else None
        )

        if not filepath:
            return

        try:
            # Reuse existing model loading logic from Tab 8
            from pathlib import Path
            from src.spectral_predict import model_io
            import zipfile

            # Check if this is an ensemble file
            is_ensemble = False
            try:
                with zipfile.ZipFile(filepath, 'r') as zf:
                    if 'ensemble_config.json' in zf.namelist():
                        is_ensemble = True
            except:
                pass

            if is_ensemble:
                # Load as ensemble
                ensemble_dict = model_io.load_ensemble(filepath)

                # Create model_dict format compatible with existing code
                model_dict = {
                    'model': ensemble_dict['ensemble'],
                    'metadata': ensemble_dict['metadata'],
                    'preprocessor': None,  # Ensembles have preprocessing in base models
                    'filepath': filepath,
                    'filename': Path(filepath).name,
                    'is_ensemble': True,
                    'ensemble_type': ensemble_dict['config']['ensemble_type'],
                    'ensemble_name': ensemble_dict['config']['ensemble_name'],
                    'model_names': ensemble_dict['model_names'],
                    'base_model_dicts': ensemble_dict.get('base_model_dicts', [])
                }
            else:
                # Load as individual model
                model_dict = model_io.load_model(filepath)
                model_dict['filepath'] = filepath
                model_dict['filename'] = Path(filepath).name

            self.comparison_primary_model = model_dict
            self._update_comparison_primary_display()

            self.comparison_status.config(text=f"✓ Primary model loaded: {model_dict['filename']}",
                                         foreground='green')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load primary model:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _clear_comparison_primary_model(self):
        """Clear the primary model."""
        self.comparison_primary_model = None
        self._update_comparison_primary_display()
        self.comparison_status.config(text="Primary model cleared", foreground='gray')

    def _load_comparison_auxiliary_models(self):
        """Load one or more auxiliary models for comparison."""
        from tkinter import filedialog

        filepaths = filedialog.askopenfilenames(
            title="Load Auxiliary Model(s)",
            filetypes=[("DASP Model Files", "*.dasp"), ("All Files", "*.*")],
            initialdir=self.last_directory if hasattr(self, 'last_directory') else None
        )

        if not filepaths:
            return

        try:
            from pathlib import Path
            from src.spectral_predict import model_io
            import zipfile

            for filepath in filepaths:
                # Check if this is an ensemble file
                is_ensemble = False
                try:
                    with zipfile.ZipFile(filepath, 'r') as zf:
                        if 'ensemble_config.json' in zf.namelist():
                            is_ensemble = True
                except:
                    pass

                if is_ensemble:
                    # Load as ensemble
                    ensemble_dict = model_io.load_ensemble(filepath)

                    # Create model_dict format compatible with existing code
                    model_dict = {
                        'model': ensemble_dict['ensemble'],
                        'metadata': ensemble_dict['metadata'],
                        'preprocessor': None,  # Ensembles have preprocessing in base models
                        'filepath': filepath,
                        'filename': Path(filepath).name,
                        'is_ensemble': True,
                        'ensemble_type': ensemble_dict['config']['ensemble_type'],
                        'ensemble_name': ensemble_dict['config']['ensemble_name'],
                        'model_names': ensemble_dict['model_names'],
                        'base_model_dicts': ensemble_dict.get('base_model_dicts', [])
                    }
                else:
                    # Load as individual model
                    model_dict = model_io.load_model(filepath)
                    model_dict['filepath'] = filepath
                    model_dict['filename'] = Path(filepath).name

                self.comparison_auxiliary_models.append(model_dict)

            self._update_comparison_auxiliary_display()

            self.comparison_status.config(
                text=f"✓ Loaded {len(filepaths)} auxiliary model(s). Total: {len(self.comparison_auxiliary_models)}",
                foreground='green')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load auxiliary models:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _clear_comparison_auxiliary_models(self):
        """Clear all auxiliary models."""
        self.comparison_auxiliary_models = []
        self._update_comparison_auxiliary_display()
        self.comparison_status.config(text="Auxiliary models cleared", foreground='gray')

    def _on_comparison_source_change(self):
        """Handle data source selection changes."""
        source = self.comparison_data_source.get()

        if source == 'live':
            # Stop any active monitoring when switching away from live mode
            if self.live_monitoring_active:
                self._stop_live_monitoring()

            # Hide normal path controls, show live monitoring frame
            self.comparison_path_label.grid_remove()
            self.comparison_data_path.set("")
            self.comparison_browse_btn.config(state='disabled')
            self.comparison_load_data_btn.config(state='disabled')

            # Show live monitoring frame
            self.comparison_live_frame.grid()

        elif source == 'validation':
            # Stop monitoring if switching away from live
            if self.live_monitoring_active:
                self._stop_live_monitoring()

            # Disable path controls
            self.comparison_path_label.config(state='disabled')
            self.comparison_data_path.set("Using pre-selected validation set from Analysis Configuration")
            self.comparison_browse_btn.config(state='disabled')
            self.comparison_load_data_btn.config(state='normal')

            # Hide live monitoring frame
            self.comparison_live_frame.grid_remove()

        else:
            # Stop monitoring if switching away from live
            if self.live_monitoring_active:
                self._stop_live_monitoring()

            # Enable path controls for directory/csv
            self.comparison_path_label.config(state='normal')
            self.comparison_data_path.set("")
            self.comparison_browse_btn.config(state='normal')
            self.comparison_load_data_btn.config(state='normal')

            # Hide live monitoring frame
            self.comparison_live_frame.grid_remove()

    def _browse_comparison_data(self):
        """Browse for comparison data (directory, CSV, or Excel)."""
        from tkinter import filedialog

        source = self.comparison_data_source.get()

        if source == 'directory':
            path = filedialog.askdirectory(title="Select Directory with Spectral Files",
                                          initialdir=self.last_directory if hasattr(self, 'last_directory') else None)
        else:  # CSV or Excel
            path = filedialog.askopenfilename(
                title="Select CSV or Excel File",
                filetypes=[
                    ("Supported files", "*.csv;*.xlsx;*.xls"),
                    ("CSV files", "*.csv"),
                    ("Excel files", "*.xlsx;*.xls"),
                    ("All files", "*.*")
                ],
                initialdir=self.last_directory if hasattr(self, 'last_directory') else None
            )

        if path:
            self.comparison_data_path.set(path)

    def _load_comparison_data(self):
        """Load data for comparison analysis."""
        source = self.comparison_data_source.get()

        try:
            if source == 'validation':
                # Use validation set from Tab 4
                if self.validation_X is None or self.validation_y is None:
                    messagebox.showerror("Error",
                        "No validation set has been created yet.\n\n"
                        "Please go to the Analysis Configuration tab and create a validation set first.")
                    return

                self.comparison_data = self.validation_X.copy()
                self.comparison_data_status.config(
                    text=f"✓ Loaded {len(self.comparison_data)} samples from validation set",
                    foreground='green')

            elif source == 'directory':
                # Load spectral files from directory
                import pandas as pd

                directory = self.comparison_data_path.get()
                if not directory:
                    messagebox.showerror("Error", "Please select a directory")
                    return

                # Use existing _load_spectra_from_directory method
                wavelengths, X = self._load_spectra_from_directory(directory)

                if X is None or len(X) == 0:
                    messagebox.showerror("Error", "No spectral files found in directory")
                    return

                # Convert to DataFrame format (wavelengths as columns, spectra as rows)
                self.comparison_data = pd.DataFrame(X, columns=wavelengths)
                self.comparison_data_status.config(
                    text=f"✓ Loaded {len(self.comparison_data)} spectra from directory",
                    foreground='green')

            else:  # CSV or Excel
                import pandas as pd

                file_path = self.comparison_data_path.get()
                if not file_path:
                    messagebox.showerror("Error", "Please select a CSV or Excel file")
                    return

                # Detect file type and read accordingly (same as Import tab)
                if file_path.lower().endswith(('.xlsx', '.xls')):
                    self.comparison_data = pd.read_excel(file_path)
                    file_type = "Excel"
                else:
                    self.comparison_data = pd.read_csv(file_path)
                    file_type = "CSV"

                self.comparison_data_status.config(
                    text=f"✓ Loaded {len(self.comparison_data)} samples from {file_type}",
                    foreground='green')

            self.comparison_status.config(text="Data loaded. Ready to run comparison.", foreground='green')

        except Exception as e:
            messagebox.showerror("Error", f"Failed to load comparison data:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _browse_live_folder(self):
        """Browse for folder to monitor in live mode."""
        from tkinter import filedialog

        path = filedialog.askdirectory(
            title="Select Folder to Monitor",
            initialdir=self.last_directory if hasattr(self, 'last_directory') else None
        )

        if path:
            self.comparison_live_path.set(path)

    def _count_spectral_files(self, folder_path):
        """Count spectral files in a folder.

        Args:
            folder_path: Path to folder

        Returns:
            Total count of ASD, CSV, and SPC files
        """
        import glob
        import os

        if not os.path.isdir(folder_path):
            return 0

        asd_files = glob.glob(os.path.join(folder_path, "*.asd"))
        csv_files = glob.glob(os.path.join(folder_path, "*.csv"))
        spc_files = glob.glob(os.path.join(folder_path, "*.spc"))

        return len(asd_files) + len(csv_files) + len(spc_files)

    def _update_live_status(self, message, color='gray'):
        """Update live monitoring status label (thread-safe).

        Args:
            message: Status message to display
            color: Text color ('green', 'gray', 'red')
        """
        # Use root.after to ensure GUI update happens in main thread
        self.root.after(0, lambda: self.comparison_live_status.config(
            text=message, foreground=color))

    def _start_live_monitoring(self):
        """Start live folder monitoring."""
        import os
        from tkinter import messagebox
        from datetime import datetime

        # Validate folder path
        folder_path = self.comparison_live_path.get()
        if not folder_path or not os.path.isdir(folder_path):
            messagebox.showerror("Error", "Please select a valid folder to monitor")
            return

        # Validate models are loaded
        if not self.comparison_primary_model:
            messagebox.showerror("Error", "Please load a primary model first (Step 1)")
            return

        # Check for large folders (warn at 500+ files)
        file_count = self._count_spectral_files(folder_path)
        if file_count >= 500:
            response = messagebox.askyesno(
                "Large Folder Detected",
                f"This folder contains {file_count} spectral files.\n\n"
                "Monitoring large folders may impact performance.\n\n"
                "Do you want to proceed?",
                icon='warning'
            )
            if not response:
                return

        # Initialize monitoring state
        self.live_monitoring_active = True
        self.live_last_file_count = file_count

        # Update UI
        self.comparison_live_start_btn.config(state='disabled')
        self.comparison_live_stop_btn.config(state='normal')
        self._update_live_status("● Starting monitoring...", 'green')

        # Perform initial scan and schedule next scan
        self._perform_live_scan()

    def _perform_live_scan(self):
        """Scan folder and update if files changed."""
        import os
        from datetime import datetime

        if not self.live_monitoring_active:
            return

        try:
            folder_path = self.comparison_live_path.get()

            # Verify folder still exists
            if not os.path.isdir(folder_path):
                self._update_live_status("⚠ Error: Folder not found", 'red')
                self._stop_live_monitoring()
                return

            # Count files in folder
            current_count = self._count_spectral_files(folder_path)
            timestamp = datetime.now().strftime("%H:%M:%S")

            # Check if file count changed
            if current_count != self.live_last_file_count:
                # Files changed - reload and re-run
                self._update_live_status(f"● Files changed ({current_count}), updating...", 'green')

                # Load data from folder
                import pandas as pd
                wavelengths, X = self._load_spectra_from_directory(folder_path)

                if X is None or len(X) == 0:
                    self._update_live_status(f"⚠ No valid files found (Last scan: {timestamp})", 'orange')
                else:
                    # Convert to DataFrame
                    self.comparison_data = pd.DataFrame(X, columns=wavelengths)

                    # Update data status
                    self.comparison_data_status.config(
                        text=f"✓ Loaded {len(self.comparison_data)} spectra (Live mode)",
                        foreground='green'
                    )

                    # Auto-run comparison
                    self._run_comparison()

                    # Update status with file count and timestamp
                    self._update_live_status(
                        f"● Monitoring active - {current_count} files (Last update: {timestamp})",
                        'green'
                    )

                # Update last known count
                self.live_last_file_count = current_count

            else:
                # No change - just update timestamp
                self._update_live_status(
                    f"● Monitoring active - {current_count} files (Last scan: {timestamp})",
                    'green'
                )

        except Exception as e:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self._update_live_status(f"⚠ Scan error: {str(e)} ({timestamp})", 'red')
            import traceback
            traceback.print_exc()

        # Schedule next scan (convert seconds to milliseconds)
        if self.live_monitoring_active:
            interval_ms = self.comparison_live_interval.get() * 1000
            self.live_monitoring_timer_id = self.root.after(interval_ms, self._perform_live_scan)

    def _stop_live_monitoring(self):
        """Stop live folder monitoring."""
        self.live_monitoring_active = False

        # Cancel scheduled timer
        if self.live_monitoring_timer_id:
            self.root.after_cancel(self.live_monitoring_timer_id)
            self.live_monitoring_timer_id = None

        # Update UI
        self.comparison_live_start_btn.config(state='normal')
        self.comparison_live_stop_btn.config(state='disabled')
        self._update_live_status("○ Monitoring stopped", 'gray')

    def _add_comparison_rule(self):
        """Open dialog to add a conditional flagging rule."""
        if not self.comparison_auxiliary_models:
            messagebox.showinfo("Info", "Please load at least one auxiliary model first")
            return

        # Create rule dialog
        dialog = tk.Toplevel(self.root)
        dialog.title("Add Conditional Flagging Rule")
        dialog.geometry("550x450")
        dialog.configure(bg=self.colors['bg'])

        # Make modal
        dialog.transient(self.root)
        dialog.grab_set()

        main_frame = ttk.Frame(dialog, padding="20")
        main_frame.pack(fill='both', expand=True)

        ttk.Label(main_frame, text="Create a rule to flag primary predictions",
                  style='Subheading.TLabel').pack(anchor='w', pady=(0, 20))

        # Auxiliary model selection
        ttk.Label(main_frame, text="If auxiliary model:", style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        aux_model_var = tk.StringVar()
        aux_models = [m['metadata'].get('target_variable', m['filename']) for m in self.comparison_auxiliary_models]
        aux_combo = ttk.Combobox(main_frame, textvariable=aux_model_var, values=aux_models,
                                 state='readonly', width=40)
        aux_combo.pack(anchor='w', pady=(0, 15))
        if aux_models:
            aux_combo.current(0)

        # Condition selection
        ttk.Label(main_frame, text="Condition:", style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        condition_var = tk.StringVar(value='==')
        condition_frame = ttk.Frame(main_frame)
        condition_frame.pack(anchor='w', pady=(0, 15))

        conditions = ['==', '!=', '>', '<', '>=', '<=', 'contains']
        for cond in conditions:
            ttk.Radiobutton(condition_frame, text=cond, variable=condition_var,
                          value=cond).pack(side='left', padx=5)

        # Value entry
        ttk.Label(main_frame, text="Value:", style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        value_var = tk.StringVar()
        ttk.Entry(main_frame, textvariable=value_var, width=40).pack(anchor='w', pady=(0, 15))

        # Flag text
        ttk.Label(main_frame, text="Flag message:", style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        flag_var = tk.StringVar(value="⚠️ Unreliable")
        ttk.Entry(main_frame, textvariable=flag_var, width=40).pack(anchor='w', pady=(0, 20))

        # Buttons
        btn_frame = ttk.Frame(main_frame)
        btn_frame.pack(anchor='w')

        def save_rule():
            if not value_var.get():
                messagebox.showerror("Error", "Please enter a value for the condition.\n\nExample: For '==' condition, enter 'Clean' or '15.5'")
                return

            if not flag_var.get():
                messagebox.showerror("Error", "Please enter a flag message.\n\nExample: '⚠️ Unreliable' or 'Contaminated'")
                return

            rule = {
                'auxiliary_model': aux_model_var.get(),
                'condition': condition_var.get(),
                'value': value_var.get(),
                'flag_text': flag_var.get()
            }

            self.comparison_rules.append(rule)
            self._update_comparison_rules_display()
            self.comparison_status.config(text=f"✓ Rule added ({len(self.comparison_rules)} total)",
                                         foreground='green')
            dialog.destroy()

        ttk.Button(btn_frame, text="Add Rule", command=save_rule,
                   style='Modern.TButton').pack(side='left', padx=(0, 5))
        ttk.Button(btn_frame, text="Cancel", command=dialog.destroy,
                   style='Modern.TButton').pack(side='left')

    # === Transfer Model Functions ===
    def _load_transfer_models(self):
        """Load multiple transfer models maintaining order."""
        from tkinter import filedialog
        import os

        filepaths = filedialog.askopenfilenames(
            title="Select Transfer Models (in order of application)",
            filetypes=[("Transfer Models", "*.json"), ("All Files", "*.*")]
        )

        if not filepaths:
            return

        for filepath in filepaths:
            try:
                # Import calibration_transfer module
                from spectral_predict import calibration_transfer

                # Load the transfer model (json + npz pair)
                prefix = filepath.replace('.json', '')
                transfer_model = calibration_transfer.load_transfer_model(prefix)

                # Create display description
                description = f"{transfer_model.method.upper()}: {transfer_model.slave_id} → {transfer_model.master_id}"

                # Add to list maintaining order
                self.transfer_models.append({
                    'path': filepath,
                    'model': transfer_model,
                    'description': description
                })

                # Update listbox display
                self._update_transfer_model_display()

                print(f"Loaded transfer model: {description}")

            except Exception as e:
                messagebox.showerror("Error", f"Failed to load {os.path.basename(filepath)}: {str(e)}")

    def _clear_transfer_models(self):
        """Clear all loaded transfer models."""
        self.transfer_models.clear()
        self.transfer_listbox.delete(0, tk.END)
        print("Cleared all transfer models")

    def _update_transfer_model_display(self):
        """Update the transfer model listbox display."""
        self.transfer_listbox.delete(0, tk.END)
        for i, transfer_dict in enumerate(self.transfer_models, 1):
            display_text = f"{i}. {transfer_dict['description']}"
            self.transfer_listbox.insert(tk.END, display_text)

    def _move_transfer_up(self):
        """Move selected transfer model up in the chain."""
        selection = self.transfer_listbox.curselection()
        if not selection:
            return

        idx = selection[0]
        if idx > 0:
            # Swap with previous item
            self.transfer_models[idx-1], self.transfer_models[idx] = \
                self.transfer_models[idx], self.transfer_models[idx-1]
            self._update_transfer_model_display()
            self.transfer_listbox.selection_set(idx-1)

    def _move_transfer_down(self):
        """Move selected transfer model down in the chain."""
        selection = self.transfer_listbox.curselection()
        if not selection:
            return

        idx = selection[0]
        if idx < len(self.transfer_models) - 1:
            # Swap with next item
            self.transfer_models[idx], self.transfer_models[idx+1] = \
                self.transfer_models[idx+1], self.transfer_models[idx]
            self._update_transfer_model_display()
            self.transfer_listbox.selection_set(idx+1)

    def _remove_transfer_model(self):
        """Remove selected transfer model from chain."""
        selection = self.transfer_listbox.curselection()
        if not selection:
            return

        idx = selection[0]
        removed = self.transfer_models.pop(idx)
        self._update_transfer_model_display()
        print(f"Removed transfer model: {removed['description']}")

    def _on_transfer_toggle(self):
        """Handle transfer model checkbox toggle."""
        if self.apply_transfer.get():
            print("Transfer models enabled")
        else:
            print("Transfer models disabled")

    def _apply_transfer_chain(self, X_data):
        """
        Apply sequential transfer models to spectra.

        Parameters
        ----------
        X_data : pd.DataFrame
            Input spectra with wavelengths as columns

        Returns
        -------
        pd.DataFrame
            Transformed spectra with updated wavelengths
        """
        if not self.apply_transfer.get() or not self.transfer_models:
            return X_data

        from spectral_predict import calibration_transfer
        import pandas as pd
        import numpy as np

        X_current = X_data.copy()

        for i, transfer_dict in enumerate(self.transfer_models):
            transfer_model = transfer_dict['model']

            print(f"Applying transfer {i+1}/{len(self.transfer_models)}: {transfer_dict['description']}")

            # Convert to numpy if needed
            if isinstance(X_current, pd.DataFrame):
                X_values = X_current.values
                sample_names = X_current.index
            else:
                X_values = X_current
                sample_names = None

            # Apply transfer using the dispatcher
            try:
                X_transferred = calibration_transfer.apply_transfer_dispatch(X_values, transfer_model)
            except Exception as e:
                messagebox.showerror("Transfer Error",
                                   f"Failed to apply transfer model {transfer_dict['description']}: {str(e)}")
                return X_data  # Return original data on error

            # Reconstruct DataFrame with new wavelengths
            if sample_names is not None:
                X_current = pd.DataFrame(
                    X_transferred,
                    index=sample_names,
                    columns=transfer_model.wavelengths_common
                )
            else:
                X_current = X_transferred

        # Log the transformation
        transfer_chain_desc = " → ".join([t['description'] for t in self.transfer_models])
        print(f"Successfully applied transfer chain: {transfer_chain_desc}")

        return X_current

    def _run_comparison(self):
        """Run multi-model comparison with all loaded models."""
        # Validate inputs
        if self.comparison_primary_model is None:
            messagebox.showerror("Error", "Please load a primary model first")
            return

        if not self.comparison_auxiliary_models:
            response = messagebox.askyesno("No Auxiliary Models",
                                          "No auxiliary models loaded. Run with primary model only?")
            if not response:
                return

        if self.comparison_data is None:
            messagebox.showerror("Error", "Please load comparison data first")
            return

        try:
            self.comparison_status.config(text="Running comparison...", foreground='orange')
            self.root.update()

            import pandas as pd
            from src.spectral_predict import model_io

            # Apply transfer chain if enabled
            if self.apply_transfer.get() and self.transfer_models:
                self.comparison_status.config(text="Applying transfer models...", foreground='orange')
                self.root.update()
                comparison_data_transformed = self._apply_transfer_chain(self.comparison_data)
            else:
                comparison_data_transformed = self.comparison_data

            # Initialize results DataFrame
            results = pd.DataFrame()
            results['Sample'] = comparison_data_transformed.index if hasattr(comparison_data_transformed.index, 'tolist') else range(len(comparison_data_transformed))

            # Run primary model prediction
            primary_metadata = self.comparison_primary_model['metadata']
            primary_filename = self.comparison_primary_model.get('filename', 'Primary_Model')
            primary_task = primary_metadata.get('task_type', 'unknown')

            # Add task type indicator to column name
            task_indicator = "(Class)" if primary_task == 'classification' else "(Reg)"
            # Use filename instead of target_model_preprocessing
            primary_col_name = f"{primary_filename} {task_indicator}"

            primary_predictions = model_io.predict_with_model(self.comparison_primary_model, comparison_data_transformed)
            results[primary_col_name] = primary_predictions

            # Run auxiliary model predictions
            aux_col_names = []
            for aux_model in self.comparison_auxiliary_models:
                aux_metadata = aux_model['metadata']
                aux_filename = aux_model.get('filename', 'Auxiliary_Model')
                aux_task = aux_metadata.get('task_type', 'unknown')

                # Add task type indicator to column name
                task_indicator = "(Class)" if aux_task == 'classification' else "(Reg)"
                # Use filename instead of target_model_preprocessing
                aux_col_name = f"{aux_filename} {task_indicator}"

                # Handle duplicate column names
                counter = 1
                original_col_name = aux_col_name
                while aux_col_name in results.columns:
                    aux_col_name = f"{original_col_name}_{counter}"
                    counter += 1

                aux_predictions = model_io.predict_with_model(aux_model, comparison_data_transformed)
                results[aux_col_name] = aux_predictions
                aux_col_names.append(aux_col_name)

            # Apply conditional flagging rules
            if self.comparison_rules:
                results['Flags'] = ""

                for rule in self.comparison_rules:
                    aux_model_target = rule['auxiliary_model']
                    condition = rule['condition']
                    value = rule['value']
                    flag_text = rule['flag_text']

                    # Find matching column
                    matching_cols = [col for col in aux_col_names if aux_model_target in col]

                    if matching_cols:
                        col = matching_cols[0]

                        # Apply condition based on type
                        try:
                            # Detect if column is classification or regression
                            is_classification = '(Class)' in col or results[col].dtype == 'object'

                            if condition == '==':
                                # Works for both text and numeric
                                mask = results[col].astype(str) == str(value)
                            elif condition == '!=':
                                # Works for both text and numeric
                                mask = results[col].astype(str) != str(value)
                            elif condition == 'contains':
                                # Text operation - works on both types
                                mask = results[col].astype(str).str.contains(str(value), case=False, na=False)
                            elif condition in ['>', '<', '>=', '<=']:
                                # Numeric operations - only for regression columns
                                if is_classification:
                                    print(f"Warning: Cannot apply numeric comparison '{condition}' to classification column '{col}'. Skipping rule.")
                                    continue

                                # Convert to numeric (coerce will make text -> NaN, which fails comparison)
                                numeric_col = pd.to_numeric(results[col], errors='coerce')

                                if condition == '>':
                                    mask = numeric_col > float(value)
                                elif condition == '<':
                                    mask = numeric_col < float(value)
                                elif condition == '>=':
                                    mask = numeric_col >= float(value)
                                elif condition == '<=':
                                    mask = numeric_col <= float(value)
                            else:
                                continue

                            # Add flags (only where mask is True and not NaN)
                            mask = mask.fillna(False)
                            results.loc[mask, 'Flags'] = results.loc[mask, 'Flags'].apply(
                                lambda x: f"{x}; {flag_text}" if x else flag_text
                            )
                        except Exception as e:
                            print(f"Warning: Failed to apply rule for {aux_model_target}: {e}")

            # Store results
            self.comparison_results = results

            # Display results in treeview
            self._display_comparison_results()

            self.comparison_status.config(
                text=f"✓ Comparison complete! {len(results)} samples analyzed.",
                foreground='green')

        except Exception as e:
            messagebox.showerror("Error", f"Comparison failed:\n{str(e)}")
            import traceback
            traceback.print_exc()
            self.comparison_status.config(text="Comparison failed", foreground='red')

    def _display_comparison_results(self):
        """Display comparison results in the treeview."""
        if self.comparison_results is None:
            return

        # Clear existing columns and data
        self.comparison_results_tree.delete(*self.comparison_results_tree.get_children())

        # Configure columns
        columns = list(self.comparison_results.columns)
        self.comparison_results_tree['columns'] = columns

        # Configure column headings and widths
        for col in columns:
            self.comparison_results_tree.heading(col, text=col)

            # Set column width based on content type
            if col == 'Sample':
                width = 100
            elif col == 'Flags':
                width = 200
            elif 'classification' in str(self.comparison_results[col].dtype).lower() or \
                 self.comparison_results[col].dtype == 'object':
                width = 150
            else:
                width = 120

            self.comparison_results_tree.column(col, width=width, anchor='center')

        # Insert data
        for idx, row in self.comparison_results.iterrows():
            values = []
            for col in columns:
                val = row[col]
                # Check if this is a classification column (by task indicator or dtype)
                is_classification = '(Class)' in col or self.comparison_results[col].dtype == 'object'

                # Format values based on type
                if pd.isna(val):
                    values.append("")
                elif col == 'Sample' or col == 'Flags':
                    values.append(str(val))
                elif is_classification:
                    # Classification predictions - display as text
                    values.append(str(val))
                elif isinstance(val, (int, float)):
                    # Regression predictions - format as float
                    values.append(f"{val:.4f}")
                else:
                    values.append(str(val))

            # Add tag for flagged rows
            tags = ()
            if 'Flags' in columns and row['Flags']:
                tags = ('flagged',)

            self.comparison_results_tree.insert('', 'end', values=values, tags=tags)

        # Configure tag colors for flagged rows
        self.comparison_results_tree.tag_configure('flagged', background='#FFF3CD', foreground='#856404')

    def _export_comparison_results(self):
        """Export comparison results to Excel with multiple sheets."""
        if self.comparison_results is None:
            messagebox.showinfo("Info", "No comparison results to export")
            return

        from tkinter import filedialog
        from pathlib import Path
        import pandas as pd

        filepath = filedialog.asksaveasfilename(
            title="Export Comparison Results",
            defaultextension=".xlsx",
            filetypes=[("Excel Files", "*.xlsx"), ("CSV Files", "*.csv")],
            initialdir=self.last_directory if hasattr(self, 'last_directory') else None
        )

        if not filepath:
            return

        try:
            filepath_obj = Path(filepath)

            if filepath_obj.suffix.lower() in ['.xlsx', '.xls']:
                # Multi-sheet Excel export
                with pd.ExcelWriter(filepath, engine='xlsxwriter') as writer:
                    # Sheet 1: Summary (Primary + Flags only)
                    primary_metadata = self.comparison_primary_model['metadata']
                    primary_target = primary_metadata.get('target_variable', 'Primary')

                    summary_cols = ['Sample']
                    primary_cols = [col for col in self.comparison_results.columns if primary_target in col]
                    summary_cols.extend(primary_cols)
                    if 'Flags' in self.comparison_results.columns:
                        summary_cols.append('Flags')

                    self.comparison_results[summary_cols].to_excel(writer, sheet_name='Summary', index=False)

                    # Sheet 2: All Predictions
                    self.comparison_results.to_excel(writer, sheet_name='All Predictions', index=False)

                    # Sheet 3: Model Metadata
                    metadata_rows = []

                    # Primary model
                    pm = self.comparison_primary_model['metadata']
                    metadata_rows.append({
                        'Role': 'Primary',
                        'Filename': self.comparison_primary_model['filename'],
                        'Target Variable': pm.get('target_variable', 'N/A'),
                        'Model Type': pm.get('model_name', 'N/A'),
                        'Task Type': pm.get('task_type', 'N/A'),
                        'Preprocessing': pm.get('preprocessing', 'N/A'),
                        'Performance': str(pm.get('performance', {}))
                    })

                    # Auxiliary models
                    for i, aux_model in enumerate(self.comparison_auxiliary_models):
                        am = aux_model['metadata']
                        metadata_rows.append({
                            'Role': f'Auxiliary {i+1}',
                            'Filename': aux_model['filename'],
                            'Target Variable': am.get('target_variable', 'N/A'),
                            'Model Type': am.get('model_name', 'N/A'),
                            'Task Type': am.get('task_type', 'N/A'),
                            'Preprocessing': am.get('preprocessing', 'N/A'),
                            'Performance': str(am.get('performance', {}))
                        })

                    pd.DataFrame(metadata_rows).to_excel(writer, sheet_name='Model Info', index=False)

                    # Sheet 4: Flagging Rules
                    if self.comparison_rules:
                        rules_df = pd.DataFrame(self.comparison_rules)
                        rules_df.to_excel(writer, sheet_name='Flagging Rules', index=False)

                messagebox.showinfo("Success", f"Results exported to Excel:\n{filepath}")
            else:
                # CSV export (all predictions only)
                self.comparison_results.to_csv(filepath, index=False)
                messagebox.showinfo("Success", f"Results exported to CSV:\n{filepath}")

        except Exception as e:
            messagebox.showerror("Error", f"Failed to export results:\n{str(e)}")
            import traceback
            traceback.print_exc()

    def _show_comparison_plots(self):
        """Show comparison plots in a new window."""
        if self.comparison_results is None:
            messagebox.showinfo("Info", "No comparison results to plot")
            return

        messagebox.showinfo("Coming Soon", "Comparison plots feature will be implemented in the next update!")

    def _create_tab10_calibration_transfer(self):
        """Tab 10: Calibration Transfer - Wizard interface with sections A-D.

        Section A: Get/Build Transfer Model
        - Load existing transfer model (.pkl file)
        - OR build new transfer model from master/slave data

        Section B: Choose Application Mode (Predict or Export)

        Section C: Apply Transfer & Predict (Mode A)

        Section D: Transform & Export Spectra (Mode B)
        """
        self.tab10 = ttk.Frame(self.notebook, style='TFrame')
        self.notebook.add(self.tab10, text='  🔄 Calibration Transfer  ')

        # Create scrollable content
        canvas = tk.Canvas(self.tab10, bg=self.colors['bg'], highlightthickness=0)
        scrollbar = ttk.Scrollbar(self.tab10, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas, style='TFrame')

        scrollable_frame.bind(
            "<Configure>",
            lambda e: self._debounced_configure_scrollregion("tab10", canvas)
        )

        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")

        main_frame = ttk.Frame(scrollable_frame, style='TFrame', padding="30")
        main_frame.pack(fill='both', expand=True)

        # ===================================================================
        # SECTION A: Radio Buttons - Load Existing vs Build New
        # ===================================================================
        mode_frame = ttk.LabelFrame(main_frame, text="A) Choose How to Get Transfer Model",
                                   style='Card.TFrame', padding=15)
        mode_frame.pack(fill='x', pady=(0, 15))

        self.ct_step1_mode_var = tk.StringVar(value='build')

        ttk.Radiobutton(mode_frame, text="Load Existing Transfer Model",
                       variable=self.ct_step1_mode_var, value='load',
                       command=self._on_step1_mode_changed).pack(anchor='w', pady=(0, 5))

        ttk.Radiobutton(mode_frame, text="Build New Transfer Model",
                       variable=self.ct_step1_mode_var, value='build',
                       command=self._on_step1_mode_changed).pack(anchor='w')

        # ===================================================================
        # SECTION B: Load Existing Transfer Model (initially hidden)
        # ===================================================================
        self.ct_load_existing_frame = ttk.LabelFrame(main_frame,
                                                      text="B) Load Existing Transfer Model",
                                                      style='Card.TFrame', padding=15)
        # Don't pack yet - controlled by radio button

        # Browse for .pkl file
        load_file_frame = ttk.Frame(self.ct_load_existing_frame)
        load_file_frame.pack(fill='x', pady=(0, 10))

        ttk.Label(load_file_frame, text="Transfer Model File (.pkl):",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        browse_frame = ttk.Frame(load_file_frame)
        browse_frame.pack(fill='x')

        self.ct_load_model_path_var = tk.StringVar()
        ttk.Entry(browse_frame, textvariable=self.ct_load_model_path_var,
                 width=60, state='readonly').pack(side='left', padx=(0, 10))

        ttk.Button(browse_frame, text="Browse...",
                  command=self._browse_existing_transfer_model,
                  style='Modern.TButton').pack(side='left', padx=(0, 10))

        self._create_accent_button(browse_frame, "Load Transfer Model",
                                   self._load_existing_transfer_model).pack(side='left')

        # Display loaded model info
        ttk.Label(self.ct_load_existing_frame, text="Loaded Model Information:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        self.ct_loaded_model_info_text = tk.Text(self.ct_load_existing_frame, height=5, width=80,
                                                 state='disabled', wrap='word', relief='flat',
                                                 borderwidth=0, bg=self.colors['panel'],
                                                 fg=self.colors['text'],
                                                 selectbackground=self.colors['accent'],
                                                 selectforeground=self.colors['text_inverse'])
        self.ct_loaded_model_info_text.pack(fill='x', pady=(0, 10))

        # Preview plots frame
        self.ct_loaded_model_plot_frame = ttk.Frame(self.ct_load_existing_frame, style='TFrame')
        self.ct_loaded_model_plot_frame.pack(fill='both', expand=True)

        # ===================================================================
        # SECTION C: Build New Transfer Model (initially shown)
        # ===================================================================
        self.ct_build_new_frame = ttk.LabelFrame(main_frame, text="C) Build New Transfer Model",
                                                style='Card.TFrame', padding=15)
        self.ct_build_new_frame.pack(fill='x', pady=(0, 15))

        # Sub-section A1: Load Data
        data_section = ttk.LabelFrame(self.ct_build_new_frame, text="A1) Load Master and Slave Data",
                                     style='Card.TFrame', padding=10)
        data_section.pack(fill='x', pady=(0, 10))

        # Load Master Spectra
        master_frame = ttk.Frame(data_section)
        master_frame.pack(fill='x', pady=(0, 10))

        ttk.Label(master_frame, text="Master Spectra:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        master_browse_frame = ttk.Frame(master_frame)
        master_browse_frame.pack(fill='x')

        self.ct_master_data_path_var = tk.StringVar()
        ttk.Entry(master_browse_frame, textvariable=self.ct_master_data_path_var,
                 width=50, state='readonly').pack(side='left', padx=(0, 10))

        ttk.Button(master_browse_frame, text="Browse Folder/File...",
                  command=self._browse_master_spectra,
                  style='Modern.TButton').pack(side='left', padx=(0, 10))

        self._create_accent_button(master_browse_frame, "Load Master Spectra",
                                   self._load_master_spectra).pack(side='left')

        # Load Slave Spectra
        slave_frame = ttk.Frame(data_section)
        slave_frame.pack(fill='x', pady=(0, 10))

        ttk.Label(slave_frame, text="Slave Spectra:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        slave_browse_frame = ttk.Frame(slave_frame)
        slave_browse_frame.pack(fill='x')

        self.ct_slave_data_path_var = tk.StringVar()
        ttk.Entry(slave_browse_frame, textvariable=self.ct_slave_data_path_var,
                 width=50, state='readonly').pack(side='left', padx=(0, 10))

        ttk.Button(slave_browse_frame, text="Browse Folder/File...",
                  command=self._browse_slave_spectra,
                  style='Modern.TButton').pack(side='left', padx=(0, 10))

        self._create_accent_button(slave_browse_frame, "Load Slave Spectra",
                                   self._load_slave_spectra).pack(side='left')

        # Data info display
        ttk.Label(data_section, text="Data Information:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        self.ct_data_info_text = tk.Text(data_section, height=4, width=80, state='disabled',
                                        wrap='word', relief='flat', borderwidth=0,
                                        bg=self.colors['panel'], fg=self.colors['text'],
                                        selectbackground=self.colors['accent'],
                                        selectforeground=self.colors['text_inverse'])
        self.ct_data_info_text.pack(fill='x', pady=(0, 10))

        # Preview button
        ttk.Button(data_section, text="Preview Master vs Slave Spectra",
                  command=self._preview_spectra,
                  style='Modern.TButton').pack(anchor='w')

        # Sub-section A2: Configure Method
        method_section = ttk.LabelFrame(self.ct_build_new_frame, text="A2) Configure Transfer Method",
                                       style='Card.TFrame', padding=10)
        method_section.pack(fill='x', pady=(0, 10))

        # Method selection (reuse existing UI pattern lines 15640-15790)
        ttk.Label(method_section, text="Transfer Method:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        method_buttons_frame = ttk.Frame(method_section)
        method_buttons_frame.pack(fill='x', pady=(0, 10))

        self.ct_method_var = tk.StringVar(value='ctai')

        # DS method
        ds_radio = ttk.Radiobutton(method_buttons_frame, text="DS",
                                   variable=self.ct_method_var, value='ds')
        ds_radio.pack(side='left', padx=(0, 10))
        CreateToolTip(ds_radio, text=TOOLTIP_CONTENT['calibration_transfer']['method_DS'], delay=500)

        # PDS method
        pds_radio = ttk.Radiobutton(method_buttons_frame, text="PDS",
                                    variable=self.ct_method_var, value='pds')
        pds_radio.pack(side='left', padx=(0, 10))
        CreateToolTip(pds_radio, text=TOOLTIP_CONTENT['calibration_transfer']['method_PDS'], delay=500)

        # TSR method
        tsr_radio = ttk.Radiobutton(method_buttons_frame, text="TSR",
                                    variable=self.ct_method_var, value='tsr')
        tsr_radio.pack(side='left', padx=(0, 10))
        CreateToolTip(tsr_radio, text=TOOLTIP_CONTENT['calibration_transfer']['method_TSR'], delay=500)

        # CTAI method
        ctai_radio = ttk.Radiobutton(method_buttons_frame, text="CTAI",
                                     variable=self.ct_method_var, value='ctai')
        ctai_radio.pack(side='left', padx=(0, 10))
        CreateToolTip(ctai_radio, text=TOOLTIP_CONTENT['calibration_transfer']['method_CTAI'], delay=500)

        # NS-PFCE method
        nspfce_radio = ttk.Radiobutton(method_buttons_frame, text="NS-PFCE",
                                       variable=self.ct_method_var, value='nspfce')
        nspfce_radio.pack(side='left', padx=(0, 10))
        CreateToolTip(nspfce_radio, text=TOOLTIP_CONTENT['calibration_transfer']['method_NSPFCE'], delay=500)

        # JYPLS-inv method
        jypls_radio = ttk.Radiobutton(method_buttons_frame, text="JYPLS-inv",
                                      variable=self.ct_method_var, value='jypls-inv', state='disabled')
        jypls_radio.pack(side='left')
        CreateToolTip(jypls_radio, text=TOOLTIP_CONTENT['calibration_transfer']['method_JYPLS'], delay=500)

        # Method parameters
        params_frame = ttk.LabelFrame(method_section, text="Method Parameters",
                                     style='Card.TFrame', padding=5)
        params_frame.pack(fill='x', pady=(0, 10))

        # Row 1
        row1 = ttk.Frame(params_frame)
        row1.pack(fill='x', pady=(0, 5))

        # DS Lambda
        ds_lambda_label = ttk.Label(row1, text="DS Lambda:", style='CardLabel.TLabel', width=20)
        ds_lambda_label.pack(side='left')
        CreateToolTip(ds_lambda_label, text=TOOLTIP_CONTENT['calibration_transfer']['param_ds_lambda'], delay=500)

        self.ct_ds_lambda_var = tk.StringVar(value='0.001')
        ds_lambda_entry = ttk.Entry(row1, textvariable=self.ct_ds_lambda_var, width=12)
        ds_lambda_entry.pack(side='left', padx=(0, 20))
        CreateToolTip(ds_lambda_entry, text=TOOLTIP_CONTENT['calibration_transfer']['param_ds_lambda'], delay=500)

        # PDS Window
        pds_window_label = ttk.Label(row1, text="PDS Window:", style='CardLabel.TLabel', width=20)
        pds_window_label.pack(side='left')
        CreateToolTip(pds_window_label, text=TOOLTIP_CONTENT['calibration_transfer']['param_pds_window'], delay=500)

        self.ct_pds_window_var = tk.StringVar(value='11')
        pds_window_entry = ttk.Entry(row1, textvariable=self.ct_pds_window_var, width=12)
        pds_window_entry.pack(side='left')
        CreateToolTip(pds_window_entry, text=TOOLTIP_CONTENT['calibration_transfer']['param_pds_window'], delay=500)

        # Row 2
        row2 = ttk.Frame(params_frame)
        row2.pack(fill='x', pady=(0, 5))

        # TSR Samples
        tsr_samples_label = ttk.Label(row2, text="TSR Samples:", style='CardLabel.TLabel', width=20)
        tsr_samples_label.pack(side='left')
        CreateToolTip(tsr_samples_label, text=TOOLTIP_CONTENT['calibration_transfer']['param_tsr_samples'], delay=500)

        self.ct_tsr_n_samples_var = tk.StringVar(value='12')
        tsr_samples_entry = ttk.Entry(row2, textvariable=self.ct_tsr_n_samples_var, width=12)
        tsr_samples_entry.pack(side='left', padx=(0, 20))
        CreateToolTip(tsr_samples_entry, text=TOOLTIP_CONTENT['calibration_transfer']['param_tsr_samples'], delay=500)

        # JYPLS Samples
        jypls_samples_label = ttk.Label(row2, text="JYPLS Samples:", style='CardLabel.TLabel', width=20)
        jypls_samples_label.pack(side='left')
        CreateToolTip(jypls_samples_label, text=TOOLTIP_CONTENT['calibration_transfer']['param_jypls_samples'], delay=500)

        self.ct_jypls_n_samples_var = tk.StringVar(value='12')
        jypls_samples_entry = ttk.Entry(row2, textvariable=self.ct_jypls_n_samples_var, width=12)
        jypls_samples_entry.pack(side='left')
        CreateToolTip(jypls_samples_entry, text=TOOLTIP_CONTENT['calibration_transfer']['param_jypls_samples'], delay=500)

        # Row 3
        row3 = ttk.Frame(params_frame)
        row3.pack(fill='x', pady=(0, 5))

        # JYPLS Components
        jypls_comp_label = ttk.Label(row3, text="JYPLS Components:", style='CardLabel.TLabel', width=20)
        jypls_comp_label.pack(side='left')
        CreateToolTip(jypls_comp_label, text=TOOLTIP_CONTENT['calibration_transfer']['param_jypls_components'], delay=500)

        self.ct_jypls_n_components_var = tk.StringVar(value='Auto')
        jypls_comp_combo = ttk.Combobox(row3, textvariable=self.ct_jypls_n_components_var,
                                        values=['Auto', '3', '5', '8', '10', '15', '20'],
                                        state='readonly', width=10)
        jypls_comp_combo.pack(side='left', padx=(0, 20))
        CreateToolTip(jypls_comp_combo, text=TOOLTIP_CONTENT['calibration_transfer']['param_jypls_components'], delay=500)

        # NS-PFCE Max Iter
        nspfce_maxiter_label = ttk.Label(row3, text="NS-PFCE Max Iter:", style='CardLabel.TLabel', width=20)
        nspfce_maxiter_label.pack(side='left')
        CreateToolTip(nspfce_maxiter_label, text=TOOLTIP_CONTENT['calibration_transfer']['param_nspfce_max_iter'], delay=500)

        self.ct_nspfce_max_iterations_var = tk.StringVar(value='100')
        nspfce_maxiter_entry = ttk.Entry(row3, textvariable=self.ct_nspfce_max_iterations_var, width=12)
        nspfce_maxiter_entry.pack(side='left')
        CreateToolTip(nspfce_maxiter_entry, text=TOOLTIP_CONTENT['calibration_transfer']['param_nspfce_max_iter'], delay=500)

        # Row 4
        row4 = ttk.Frame(params_frame)
        row4.pack(fill='x')

        # NS-PFCE Wavelength Selection
        self.ct_nspfce_use_wavelength_selection_var = tk.BooleanVar(value=True)
        nspfce_wavsel_check = ttk.Checkbutton(row4, text="NS-PFCE: Use Wavelength Selection",
                                              variable=self.ct_nspfce_use_wavelength_selection_var)
        nspfce_wavsel_check.pack(side='left', padx=(0, 20))
        CreateToolTip(nspfce_wavsel_check, text=TOOLTIP_CONTENT['calibration_transfer']['param_nspfce_wavelength_selection'], delay=500)

        # NS-PFCE Selector
        nspfce_selector_label = ttk.Label(row4, text="Selector:", style='CardLabel.TLabel')
        nspfce_selector_label.pack(side='left')
        CreateToolTip(nspfce_selector_label, text=TOOLTIP_CONTENT['calibration_transfer']['param_nspfce_selector'], delay=500)

        self.ct_nspfce_selector_var = tk.StringVar(value='vcpa-iriv')
        nspfce_selector_combo = ttk.Combobox(row4, textvariable=self.ct_nspfce_selector_var,
                                             values=['vcpa-iriv', 'cars', 'spa'], state='readonly',
                                             width=12)
        nspfce_selector_combo.pack(side='left')
        CreateToolTip(nspfce_selector_combo, text=TOOLTIP_CONTENT['calibration_transfer']['param_nspfce_selector'], delay=500)

        # Build button
        build_btn_frame = ttk.Frame(self.ct_build_new_frame)
        build_btn_frame.pack(fill='x', pady=(0, 10))

        self._create_accent_button(build_btn_frame, "Build Transfer Model",
                                   self._build_transfer_model_new).pack(side='left', padx=(0, 10))

        self.ct_save_tm_button = ttk.Button(build_btn_frame, text="Save Transfer Model...",
                                           command=self._save_transfer_model,
                                           style='Modern.TButton', state='disabled')
        self.ct_save_tm_button.pack(side='left')

        # Transfer model info and plots
        ttk.Label(self.ct_build_new_frame, text="Transfer Model Information:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        self.ct_transfer_info_text = tk.Text(self.ct_build_new_frame, height=5, width=80,
                                            state='disabled', wrap='word', relief='flat',
                                            borderwidth=0, bg=self.colors['panel'],
                                            fg=self.colors['text'],
                                            selectbackground=self.colors['accent'],
                                            selectforeground=self.colors['text_inverse'])
        self.ct_transfer_info_text.pack(fill='x', pady=(0, 10))

        # Plot frame for transfer quality
        self.ct_transfer_plot_frame = ttk.Frame(self.ct_build_new_frame, style='TFrame')
        self.ct_transfer_plot_frame.pack(fill='both', expand=True)

        # ===================================================================
        # STEP 2: Choose Application Mode
        # ===================================================================
        step2_title_label = ttk.Label(main_frame, text="STEP 2: Choose How to Use Transfer Model",
                                     style='Caption.TLabel')
        step2_title_label.pack(pady=(20, 10))

        mode_selection_frame = ttk.LabelFrame(main_frame, text="B) Application Mode",
                                             style='Card.TFrame', padding=15)
        mode_selection_frame.pack(fill='x', pady=(0, 15))

        self.ct_mode_var = tk.StringVar(value='')

        # Mode A: Predict Properties
        mode_a_frame = ttk.Frame(mode_selection_frame)
        mode_a_frame.pack(fill='x', pady=(0, 10))

        ttk.Radiobutton(mode_a_frame, text="Mode A: Predict Properties",
                       variable=self.ct_mode_var, value='predict',
                       command=self._on_mode_selected).pack(anchor='w', pady=(0, 5))

        ttk.Label(mode_a_frame,
                 text="Apply transfer to predict properties (e.g., % nitrogen) from new slave data.\nRequires: Master prediction model + new slave data",
                 style='CardLabel.TLabel', foreground='gray').pack(anchor='w', padx=(25, 0))

        # Mode B: Transform & Export
        mode_b_frame = ttk.Frame(mode_selection_frame)
        mode_b_frame.pack(fill='x', pady=(0, 10))

        ttk.Radiobutton(mode_b_frame, text="Mode B: Transform & Export",
                       variable=self.ct_mode_var, value='export',
                       command=self._on_mode_selected).pack(anchor='w', pady=(0, 5))

        ttk.Label(mode_b_frame,
                 text="Transform slave spectra to master domain and export files.\nRequires: Only new slave data (NO prediction model needed)",
                 style='CardLabel.TLabel', foreground='gray').pack(anchor='w', padx=(25, 0))

        # Status label
        self.ct_mode_status_label = ttk.Label(mode_selection_frame, text="",
                                             style='CardLabel.TLabel')
        self.ct_mode_status_label.pack(anchor='w', pady=(10, 0))

        # ===================================================================
        # STEP 3A: Prediction Workflow (Mode A) - Initially hidden
        # ===================================================================
        step3a_title_label = ttk.Label(main_frame, text="STEP 3A: Prediction Workflow",
                                      style='Caption.TLabel')
        step3a_title_label.pack(pady=(20, 10))

        self.ct_step3a_frame = ttk.LabelFrame(main_frame, text="C) Apply Transfer & Predict (Mode A)",
                                             style='Card.TFrame', padding=15)
        # Don't pack yet - controlled by mode selection

        # C1: Load Master Prediction Model
        c1_section = ttk.LabelFrame(self.ct_step3a_frame, text="C1) Load Master Prediction Model",
                                   style='Card.TFrame', padding=10)
        c1_section.pack(fill='x', pady=(0, 10))

        ttk.Label(c1_section, text="Load a trained prediction model (sklearn .pkl file):",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        model_load_frame = ttk.Frame(c1_section)
        model_load_frame.pack(fill='x', pady=(0, 10))

        self.ct_pred_model_path_var = tk.StringVar()
        ttk.Entry(model_load_frame, textvariable=self.ct_pred_model_path_var,
                 width=50, state='readonly').pack(side='left', padx=(0, 10))

        ttk.Button(model_load_frame, text="Browse...",
                  command=self._browse_prediction_model,
                  style='Modern.TButton').pack(side='left', padx=(0, 10))

        self._create_accent_button(model_load_frame, "Load Model",
                                   self._load_prediction_model).pack(side='left')

        # Display model info
        ttk.Label(c1_section, text="Model Information:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        self.ct_pred_model_info_text = tk.Text(c1_section, height=4, width=80,
                                              state='disabled', wrap='word', relief='flat',
                                              borderwidth=0, bg=self.colors['panel'],
                                              fg=self.colors['text'],
                                              selectbackground=self.colors['accent'],
                                              selectforeground=self.colors['text_inverse'])
        self.ct_pred_model_info_text.pack(fill='x', pady=(0, 10))

        # C2: Load New Slave Data
        c2_section = ttk.LabelFrame(self.ct_step3a_frame, text="C2) Load New Slave Data",
                                   style='Card.TFrame', padding=10)
        c2_section.pack(fill='x', pady=(0, 10))

        ttk.Label(c2_section, text="Load new slave spectra for prediction:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        slave_load_frame = ttk.Frame(c2_section)
        slave_load_frame.pack(fill='x', pady=(0, 10))

        self.ct_pred_slave_path_var = tk.StringVar()
        ttk.Entry(slave_load_frame, textvariable=self.ct_pred_slave_path_var,
                 width=50, state='readonly').pack(side='left', padx=(0, 10))

        ttk.Button(slave_load_frame, text="Browse...",
                  command=self._browse_new_slave_data_predict,
                  style='Modern.TButton').pack(side='left', padx=(0, 10))

        self._create_accent_button(slave_load_frame, "Load Slave Spectra",
                                   self._load_new_slave_data_predict).pack(side='left')

        # Display slave data info
        ttk.Label(c2_section, text="Slave Data Information:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        self.ct_pred_slave_info_text = tk.Text(c2_section, height=3, width=80,
                                              state='disabled', wrap='word', relief='flat',
                                              borderwidth=0, bg=self.colors['panel'],
                                              fg=self.colors['text'],
                                              selectbackground=self.colors['accent'],
                                              selectforeground=self.colors['text_inverse'])
        self.ct_pred_slave_info_text.pack(fill='x', pady=(0, 10))

        # C3: Run Prediction
        c3_section = ttk.LabelFrame(self.ct_step3a_frame, text="C3) Run Prediction",
                                   style='Card.TFrame', padding=10)
        c3_section.pack(fill='x', pady=(0, 10))

        prediction_btn_frame = ttk.Frame(c3_section)
        prediction_btn_frame.pack(fill='x', pady=(0, 10))

        self._create_accent_button(prediction_btn_frame, "Run Prediction Workflow",
                                   self._run_prediction_workflow).pack(side='left', padx=(0, 10))

        self.ct_export_predictions_button = ttk.Button(prediction_btn_frame,
                                                       text="Export Predictions to CSV...",
                                                       command=self._export_predictions,
                                                       style='Modern.TButton', state='disabled')
        self.ct_export_predictions_button.pack(side='left')

        # Results display
        ttk.Label(c3_section, text="Prediction Results:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        # Create treeview for results
        results_tree_frame = ttk.Frame(c3_section)
        results_tree_frame.pack(fill='both', expand=True, pady=(0, 10))

        # Scrollbars for treeview
        tree_scroll_y = ttk.Scrollbar(results_tree_frame, orient='vertical')
        tree_scroll_y.pack(side='right', fill='y')
        tree_scroll_x = ttk.Scrollbar(results_tree_frame, orient='horizontal')
        tree_scroll_x.pack(side='bottom', fill='x')

        self.ct_predictions_tree = ttk.Treeview(results_tree_frame,
                                               columns=('sample_id', 'predicted_value'),
                                               show='headings', height=8,
                                               yscrollcommand=tree_scroll_y.set,
                                               xscrollcommand=tree_scroll_x.set)

        tree_scroll_y.config(command=self.ct_predictions_tree.yview)
        tree_scroll_x.config(command=self.ct_predictions_tree.xview)

        self.ct_predictions_tree.heading('sample_id', text='Sample ID')
        self.ct_predictions_tree.heading('predicted_value', text='Predicted Value')
        self.ct_predictions_tree.column('sample_id', width=200, anchor='w')
        self.ct_predictions_tree.column('predicted_value', width=150, anchor='e')

        self.ct_predictions_tree.pack(fill='both', expand=True)

        # Plot frame for prediction results
        self.ct_predictions_plot_frame = ttk.Frame(c3_section, style='TFrame')
        self.ct_predictions_plot_frame.pack(fill='both', expand=True, pady=(10, 0))

        # ===================================================================
        # SECTION D: Transform & Export Workflow (Mode B)
        # ===================================================================
        self.ct_step3b_frame = ttk.LabelFrame(main_frame,
                                              text="D) Transform & Export Spectra (Mode B)",
                                              style='Card.TFrame', padding=15)
        # Don't pack yet - controlled by mode selection in Section B

        # D1: Load New Slave Data for Export
        export_load_section = ttk.LabelFrame(self.ct_step3b_frame,
                                            text="D1) Load New Slave Data to Transform",
                                            style='Card.TFrame', padding=10)
        export_load_section.pack(fill='x', pady=(0, 10))

        # Browse for new slave spectra
        export_browse_frame = ttk.Frame(export_load_section)
        export_browse_frame.pack(fill='x', pady=(0, 10))

        ttk.Label(export_browse_frame, text="Slave Spectra to Transform:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        browse_row = ttk.Frame(export_browse_frame)
        browse_row.pack(fill='x')

        self.ct_export_slave_path_var = tk.StringVar()
        ttk.Entry(browse_row, textvariable=self.ct_export_slave_path_var,
                 width=50, state='readonly').pack(side='left', padx=(0, 10))

        ttk.Button(browse_row, text="Browse Folder/File...",
                  command=self._browse_export_slave_spectra,
                  style='Modern.TButton').pack(side='left', padx=(0, 10))

        self._create_accent_button(browse_row, "Load Slave Spectra",
                                   self._load_new_slave_data_export).pack(side='left')

        # Display loaded data info
        ttk.Label(export_load_section, text="Loaded Data Information:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        self.ct_export_data_info_text = tk.Text(export_load_section, height=3, width=80,
                                               state='disabled', wrap='word', relief='flat',
                                               borderwidth=0, bg=self.colors['panel'],
                                               fg=self.colors['text'],
                                               selectbackground=self.colors['accent'],
                                               selectforeground=self.colors['text_inverse'])
        self.ct_export_data_info_text.pack(fill='x', pady=(0, 10))

        # D2: Transform Spectra
        transform_section = ttk.LabelFrame(self.ct_step3b_frame,
                                          text="D2) Transform Spectra",
                                          style='Card.TFrame', padding=10)
        transform_section.pack(fill='x', pady=(0, 10))

        transform_btn_frame = ttk.Frame(transform_section)
        transform_btn_frame.pack(fill='x', pady=(0, 10))

        self._create_accent_button(transform_btn_frame, "Transform Spectra",
                                   self._transform_spectra).pack(side='left')

        # Transform statistics display
        ttk.Label(transform_section, text="Transformation Statistics:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        self.ct_transform_stats_text = tk.Text(transform_section, height=3, width=80,
                                              state='disabled', wrap='word', relief='flat',
                                              borderwidth=0, bg=self.colors['panel'],
                                              fg=self.colors['text'],
                                              selectbackground=self.colors['accent'],
                                              selectforeground=self.colors['text_inverse'])
        self.ct_transform_stats_text.pack(fill='x', pady=(0, 10))

        # Preview plot frame
        self.ct_transform_preview_frame = ttk.Frame(transform_section, style='TFrame')
        self.ct_transform_preview_frame.pack(fill='both', expand=True)

        # D3: Export Transformed Spectra
        export_section = ttk.LabelFrame(self.ct_step3b_frame,
                                       text="D3) Export Transformed Spectra",
                                       style='Card.TFrame', padding=10)
        export_section.pack(fill='x', pady=(0, 10))

        export_btn_frame = ttk.Frame(export_section)
        export_btn_frame.pack(fill='x', pady=(0, 10))

        ttk.Label(export_btn_frame, text="Output Directory:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(0, 5))

        output_row = ttk.Frame(export_btn_frame)
        output_row.pack(fill='x', pady=(0, 10))

        self.ct_export_output_dir_var = tk.StringVar()
        ttk.Entry(output_row, textvariable=self.ct_export_output_dir_var,
                 width=50, state='readonly').pack(side='left', padx=(0, 10))

        ttk.Button(output_row, text="Browse...",
                  command=self._browse_export_output_dir,
                  style='Modern.TButton').pack(side='left')

        self._create_accent_button(export_btn_frame, "Export Transformed Spectra",
                                   self._export_transformed_spectra).pack(side='left')

        # Export status
        ttk.Label(export_section, text="Export Status:",
                 style='CardLabel.TLabel').pack(anchor='w', pady=(10, 5))

        self.ct_export_status_text = tk.Text(export_section, height=2, width=80,
                                            state='disabled', wrap='word', relief='flat',
                                            borderwidth=0, bg=self.colors['panel'],
                                            fg=self.colors['text'],
                                            selectbackground=self.colors['accent'],
                                            selectforeground=self.colors['text_inverse'])
        self.ct_export_status_text.pack(fill='x')

        # Initialize UI state
        self._on_step1_mode_changed()



def main():
    """Main entry point."""
    root = tk.Tk()
    app = SpectralPredictApp(root)

    # Initialize tier-based model selection (must be done after all UI is created)
    app._on_tier_changed()

    # Add menu bar
    menubar = tk.Menu(root)
    root.config(menu=menubar)

    help_menu = tk.Menu(menubar, tearoff=0)
    menubar.add_cascade(label="Help", menu=help_menu)
    help_menu.add_command(label="Quick Start", command=app._show_help)
    help_menu.add_separator()
    help_menu.add_command(label="Exit", command=root.quit)

    root.mainloop()


if __name__ == "__main__":
    main()
